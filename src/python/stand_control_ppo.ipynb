{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "simplified-testimony",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T01:06:00.629238Z",
     "start_time": "2021-03-16T01:06:00.613246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_device:0\n",
      "device_count:1\n",
      "Destructor called, FlexipodEnv deleted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# https://discuss.pytorch.org/t/how-to-change-the-default-device-of-gpu-device-ids-0/1041/24\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=f\"{1}\"\n",
    "\n",
    "import torch\n",
    "print(f\"current_device:{torch.cuda.current_device()}\")\n",
    "print(f\"device_count:{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from ddpg import DDPGagent\n",
    "from ppo.PPO_continuous import PPO,Memory\n",
    "# from ddpg.utils import NormalizedEnv\n",
    "from flexipod_env import FlexipodEnv\n",
    "\n",
    "env = FlexipodEnv(dof = 12)\n",
    "# env = NormalizedEnv(env)\n",
    "\n",
    "############## Hyperparameters ##############\n",
    "# env_name = \"BipedalWalker-v3\"\n",
    "env_name = \"flexipod\"\n",
    "render = True\n",
    "solved_reward = 1500        # stop training if avg_reward > solved_reward\n",
    "log_interval = 80           # print avg reward in the interval\n",
    "# log_interval = 2           # print avg reward in the interval\n",
    "\n",
    "max_episodes = 20000        # max training episodes\n",
    "max_timesteps = 1500        # max timesteps in one episode\n",
    "\n",
    "# update_timestep = 4000      # update policy every n timesteps\n",
    "update_timestep = 3000      # update policy every n timesteps\n",
    "\n",
    "\n",
    "action_std = 1.0            # constant std for action distribution (Multivariate Normal)\n",
    "# action_std = 1.0          # constant std for action distribution (Multivariate Normal)\n",
    "K_epochs = 80               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr = 0.0002                 # parameters for Adam optimizer\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "random_seed = None\n",
    "#############################################\n",
    "# creating environment\n",
    "# env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "centered-winning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T01:06:02.063217Z",
     "start_time": "2021-03-16T01:06:02.053224Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/soft12dof_experiment_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "excellent-engineering",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T02:03:03.247618Z",
     "start_time": "2021-03-08T02:03:03.235615Z"
    }
   },
   "outputs": [],
   "source": [
    "# env = FlexipodEnv(dof = 12)\n",
    "# self = env\n",
    "# msg_rec,_,_,_ = env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-classification",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-16T01:06:03.414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002 (0.9, 0.999)\n",
      "Episode 80 \t Avg length: 140 \t Avg reward: 175\n",
      "Episode 160 \t Avg length: 151 \t Avg reward: 190\n",
      "Episode 240 \t Avg length: 169 \t Avg reward: 212\n",
      "Episode 320 \t Avg length: 182 \t Avg reward: 229\n",
      "Episode 400 \t Avg length: 197 \t Avg reward: 248\n",
      "Episode 480 \t Avg length: 199 \t Avg reward: 251\n",
      "Episode 560 \t Avg length: 216 \t Avg reward: 274\n",
      "Episode 640 \t Avg length: 232 \t Avg reward: 295\n",
      "Episode 720 \t Avg length: 231 \t Avg reward: 293\n",
      "Episode 800 \t Avg length: 222 \t Avg reward: 282\n",
      "Episode 880 \t Avg length: 242 \t Avg reward: 308\n",
      "Episode 960 \t Avg length: 243 \t Avg reward: 309\n",
      "Episode 1040 \t Avg length: 218 \t Avg reward: 278\n",
      "Episode 1120 \t Avg length: 227 \t Avg reward: 288\n",
      "Episode 1200 \t Avg length: 226 \t Avg reward: 287\n",
      "Episode 1280 \t Avg length: 226 \t Avg reward: 288\n",
      "Episode 1360 \t Avg length: 247 \t Avg reward: 315\n",
      "Episode 1440 \t Avg length: 209 \t Avg reward: 265\n",
      "Episode 1520 \t Avg length: 221 \t Avg reward: 281\n",
      "load old best,avg_length=247.2375\n",
      "Episode 1600 \t Avg length: 203 \t Avg reward: 256\n",
      "Episode 1680 \t Avg length: 202 \t Avg reward: 256\n",
      "Episode 1760 \t Avg length: 213 \t Avg reward: 268\n",
      "Episode 1840 \t Avg length: 191 \t Avg reward: 242\n",
      "Episode 1920 \t Avg length: 202 \t Avg reward: 255\n",
      "Episode 2000 \t Avg length: 233 \t Avg reward: 295\n",
      "Episode 2080 \t Avg length: 211 \t Avg reward: 267\n"
     ]
    }
   ],
   "source": [
    "if random_seed:\n",
    "    print(\"Random Seed: {}\".format(random_seed))\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "memory = Memory()\n",
    "ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "print(lr,betas)\n",
    "\n",
    "max_avg_length = 0\n",
    "# checkpoint = ppo.load(f'./PPO_continuous_{env_name}_best.pth')\n",
    "# max_avg_length = checkpoint[\"avg_length\"]\n",
    "\n",
    "# logging variables\n",
    "running_reward = 0\n",
    "avg_length = 0\n",
    "# max_avg_length = 0\n",
    "time_step = 0\n",
    "\n",
    "# training loop\n",
    "for i_episode in range(1, max_episodes+1):\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        time_step +=1\n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(state, memory)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Saving reward and is_terminals:\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "\n",
    "        # update if its time\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo.update(memory)\n",
    "            memory.clear_memory()\n",
    "            time_step = 0\n",
    "        running_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    avg_length += t\n",
    "\n",
    "    # save every 500 episodes\n",
    "    if i_episode % 500 == 0:\n",
    "        ppo.save(f'./PPO_continuous_{env_name}.pth',avg_length=avg_length)\n",
    "\n",
    "    # logging\n",
    "    if i_episode % log_interval == 0:\n",
    "        avg_length = avg_length/log_interval\n",
    "        running_reward = running_reward/log_interval\n",
    "        writer.add_scalar(\"avg_length/train\", avg_length, i_episode)\n",
    "        writer.add_scalar(\"running_reward/train\", running_reward, i_episode)\n",
    "        \n",
    "        # stop training if avg_reward > solved_reward\n",
    "        if running_reward > (log_interval*solved_reward):\n",
    "            print(\"########## Solved! ##########\")\n",
    "            ppo.save(f'./PPO_continuous_solved_{env_name}.pth',avg_length=avg_length)\n",
    "            break\n",
    "            \n",
    "        if avg_length>max_avg_length:\n",
    "            max_avg_length = avg_length\n",
    "            ppo.save(f'./PPO_continuous_{env_name}_best.pth',avg_length=avg_length)\n",
    "        elif np.random.random()<0.1:# 50% chance \n",
    "            checkpoint = ppo.load(f'./PPO_continuous_{env_name}_best.pth')\n",
    "            print(f\"load old best,avg_length={checkpoint['avg_length']}\")# restart\n",
    "\n",
    "        print(f'Episode {i_episode} \\t Avg length: {avg_length:.0f} \\t Avg reward: {running_reward:.0f}')\n",
    "        running_reward = 0\n",
    "        avg_length = 0\n",
    "        \n",
    "env.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "homeless-vault",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T13:44:56.890561Z",
     "start_time": "2021-02-27T13:44:56.882987Z"
    }
   },
   "outputs": [],
   "source": [
    "# writer.add_scalar(\"baseline_length/train\", 200, 0)\n",
    "# writer.add_scalar(\"baseline_length/train\", 200, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tough-guatemala",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T01:15:21.518863Z",
     "start_time": "2021-03-15T01:14:59.437692Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time steps:96 ,duration: 1.43[s], fps:67\n",
      "time steps:93 ,duration: 1.38[s], fps:67\n",
      "time steps:123 ,duration: 1.82[s], fps:67\n",
      "time steps:92 ,duration: 1.37[s], fps:67\n",
      "time steps:104 ,duration: 1.54[s], fps:68\n",
      "time steps:102 ,duration: 1.50[s], fps:68\n",
      "time steps:91 ,duration: 1.35[s], fps:67\n",
      "time steps:94 ,duration: 1.39[s], fps:67\n",
      "time steps:89 ,duration: 1.32[s], fps:68\n",
      "time steps:96 ,duration: 1.42[s], fps:68\n",
      "time steps:97 ,duration: 1.46[s], fps:67\n",
      "time steps:109 ,duration: 1.65[s], fps:66\n",
      "time steps:90 ,duration: 1.33[s], fps:68\n",
      "time steps:103 ,duration: 1.51[s], fps:68\n",
      "time steps:91 ,duration: 1.34[s], fps:68\n",
      "time steps:91 ,duration: 1.33[s], fps:68\n",
      "time steps:87 ,duration: 1.28[s], fps:68\n",
      "time steps:98 ,duration: 1.44[s], fps:68\n",
      "time steps:91 ,duration: 1.34[s], fps:68\n",
      "time steps:99 ,duration: 1.44[s], fps:69\n",
      "mean time steps:97 ,duration: 1.43[s]\n"
     ]
    }
   ],
   "source": [
    "memory_2 = Memory()\n",
    "\n",
    "max_timesteps=3000\n",
    "action_std=.01\n",
    "ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "ppo.load(f'./PPO_continuous_{env_name}_best.pth')\n",
    "\n",
    "time_steps = []\n",
    "durations = []\n",
    "for k in range(20):\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(state, memory_2)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#         state, reward, done, _ = env.step()\n",
    "        if done or t==max_timesteps-1:\n",
    "            time_steps.append(t)\n",
    "            episode_duration= info['t'] - env.episode_start_time\n",
    "            durations.append(episode_duration)\n",
    "            print(f\"time steps:{t} ,duration: {episode_duration:.2f}[s], fps:{t/episode_duration:.0f}\")\n",
    "            break\n",
    "            \n",
    "print(f\"mean time steps:{np.mean(time_steps):.0f} ,duration: {np.mean(durations):.2f}[s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-domestic",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-arena",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
