{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "noticed-gospel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T01:18:09.944385Z",
     "start_time": "2021-03-21T01:18:06.633321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_device:0\n",
      "device_count:1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# https://discuss.pytorch.org/t/how-to-change-the-default-device-of-gpu-device-ids-0/1041/24\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=f\"{1}\"\n",
    "\n",
    "import torch\n",
    "print(f\"current_device:{torch.cuda.current_device()}\")\n",
    "print(f\"device_count:{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from ddpg import DDPGagent\n",
    "from ppo.PPO_continuous import PPO,Memory\n",
    "# from ddpg.utils import NormalizedEnv\n",
    "from flexipod_env import FlexipodEnv\n",
    "\n",
    "env = FlexipodEnv(dof = 12)\n",
    "# env = NormalizedEnv(env)\n",
    "\n",
    "############## Hyperparameters ##############\n",
    "# env_name = \"BipedalWalker-v3\"\n",
    "env_name = \"flexipod\"\n",
    "render = True\n",
    "solved_reward = 1500        # stop training if avg_reward > solved_reward\n",
    "log_interval = 80           # print avg reward in the interval\n",
    "# log_interval = 2           # print avg reward in the interval\n",
    "\n",
    "max_episodes = 20000        # max training episodes\n",
    "max_timesteps = 1500        # max timesteps in one episode\n",
    "\n",
    "# update_timestep = 4000      # update policy every n timesteps\n",
    "update_timestep = 3000      # update policy every n timesteps\n",
    "\n",
    "\n",
    "# action_std = 1.0            # constant std for action distribution (Multivariate Normal)\n",
    "action_std = 0.8            # constant std for action distribution (Multivariate Normal)\n",
    "K_epochs = 80               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr = 0.0002                 # parameters for Adam optimizer\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "random_seed = None\n",
    "#############################################\n",
    "# creating environment\n",
    "# env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "macro-script",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T01:18:10.281472Z",
     "start_time": "2021-03-21T01:18:09.952387Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/soft12dof_experiment_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unlimited-twist",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T00:58:00.056456Z",
     "start_time": "2021-03-21T00:58:00.044440Z"
    }
   },
   "outputs": [],
   "source": [
    "# env = FlexipodEnv(dof = 12)\n",
    "# self = env\n",
    "# msg_rec,_,_,_ = env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-trout",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-21T02:53:47.365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002 (0.9, 0.999)\n",
      "Episode 0 \t Avg length: 1 \t Avg reward: 1\n"
     ]
    }
   ],
   "source": [
    "if random_seed:\n",
    "    print(\"Random Seed: {}\".format(random_seed))\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "memory = Memory()\n",
    "ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "print(lr,betas)\n",
    "\n",
    "# logging variables\n",
    "running_reward = 0\n",
    "avg_length = 0\n",
    "max_avg_length = 0\n",
    "time_step = 0\n",
    "\n",
    "# checkpoint = ppo.load(f'./PPO_continuous_{env_name}_best.pth')\n",
    "# checkpoint = ppo.load(f'./PPO_continuous_{env_name}.pth')\n",
    "# max_avg_length = checkpoint[\"avg_length\"]\n",
    "\n",
    "# training loop\n",
    "for i_episode in range(0, max_episodes+1):\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        time_step +=1\n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(state, memory)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Saving reward and is_terminals:\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "\n",
    "        # update if its time\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo.update(memory)\n",
    "            memory.clear_memory()\n",
    "            time_step = 0\n",
    "        running_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    avg_length += t\n",
    "\n",
    "    # save every 500 episodes\n",
    "    if i_episode % 500 == 0:\n",
    "        ppo.save(f'./PPO_continuous_{env_name}.pth',avg_length=avg_length)\n",
    "\n",
    "    # logging\n",
    "    if i_episode % log_interval == 0:\n",
    "        avg_length = avg_length/log_interval\n",
    "        running_reward = running_reward/log_interval\n",
    "        writer.add_scalar(\"avg_length/train\", avg_length, i_episode)\n",
    "        writer.add_scalar(\"running_reward/train\", running_reward, i_episode)\n",
    "        \n",
    "        # stop training if avg_reward > solved_reward\n",
    "        if running_reward > (log_interval*solved_reward):\n",
    "            print(\"########## Solved! ##########\")\n",
    "            ppo.save(f'./PPO_continuous_solved_{env_name}.pth',avg_length=avg_length)\n",
    "            break\n",
    "            \n",
    "        if avg_length>max_avg_length:\n",
    "            max_avg_length = avg_length\n",
    "            ppo.save(f'./PPO_continuous_{env_name}_best.pth',avg_length=avg_length)\n",
    "        elif np.random.random()<0.1:# 50% chance \n",
    "            checkpoint = ppo.load(f'./PPO_continuous_{env_name}_best.pth')\n",
    "            print(f\"load old best,avg_length={checkpoint['avg_length']}\")# restart\n",
    "\n",
    "        print(f'Episode {i_episode} \\t Avg length: {avg_length:.0f} \\t Avg reward: {running_reward:.0f}')\n",
    "        running_reward = 0\n",
    "        avg_length = 0\n",
    "        \n",
    "env.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "flush-mediterranean",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T13:44:56.890561Z",
     "start_time": "2021-02-27T13:44:56.882987Z"
    }
   },
   "outputs": [],
   "source": [
    "# writer.add_scalar(\"baseline_length/train\", 200, 0)\n",
    "# writer.add_scalar(\"baseline_length/train\", 200, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "desirable-hammer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T14:03:33.336395Z",
     "start_time": "2021-03-18T14:02:34.463986Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time steps:606 ,duration: 6.43[s], fps:94\n",
      "time steps:756 ,duration: 7.22[s], fps:105\n",
      "time steps:435 ,duration: 4.03[s], fps:108\n",
      "time steps:637 ,duration: 5.82[s], fps:109\n",
      "time steps:396 ,duration: 3.53[s], fps:112\n",
      "time steps:384 ,duration: 3.39[s], fps:113\n",
      "time steps:608 ,duration: 5.31[s], fps:115\n",
      "time steps:518 ,duration: 4.53[s], fps:114\n",
      "mean time steps:542 ,duration: 5.03[s]\n"
     ]
    }
   ],
   "source": [
    "memory_2 = Memory()\n",
    "\n",
    "max_timesteps=3000\n",
    "action_std=.1\n",
    "ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "ppo.load(f'./PPO_continuous_{env_name}_best.pth')\n",
    "\n",
    "time_steps = []\n",
    "durations = []\n",
    "for k in range(8):\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(state, memory_2)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#         state, reward, done, info = env.step()\n",
    "        if done or t==max_timesteps-1:\n",
    "            time_steps.append(t)\n",
    "            episode_duration= info['t'] - env.episode_start_time\n",
    "            durations.append(episode_duration)\n",
    "            print(f\"time steps:{t} ,duration: {episode_duration:.2f}[s], fps:{t/episode_duration:.0f}\")\n",
    "            break\n",
    "            \n",
    "print(f\"mean time steps:{np.mean(time_steps):.0f} ,duration: {np.mean(durations):.2f}[s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-burke",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "desperate-surgery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T12:55:34.534102Z",
     "start_time": "2021-03-18T12:55:34.497134Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thousand-device",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T13:18:17.228600Z",
     "start_time": "2021-03-18T13:18:17.179589Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Could not find function 'ppo.policy_old.act'.\n",
      "NameError: name 'ppo' is not defined\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "state = env.reset()\n",
    "memory_2 = Memory()\n",
    "def test():\n",
    "    for k in range(100):\n",
    "        ppo.select_action(state, memory_2)\n",
    "    \n",
    "%lprun -f ppo.policy_old.act test()\n",
    "# %lprun -f ppo.select_action test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "failing-rebel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T12:58:35.484039Z",
     "start_time": "2021-03-18T12:58:35.466046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-wonder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
