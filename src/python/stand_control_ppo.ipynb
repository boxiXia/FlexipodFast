{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pregnant-czech",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T04:25:45.778459Z",
     "start_time": "2021-03-02T04:25:43.770442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_device:0\n",
      "device_count:1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# https://discuss.pytorch.org/t/how-to-change-the-default-device-of-gpu-device-ids-0/1041/24\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=f\"{1}\"\n",
    "\n",
    "import torch\n",
    "print(f\"current_device:{torch.cuda.current_device()}\")\n",
    "print(f\"device_count:{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from ddpg import DDPGagent\n",
    "from ppo.PPO_continuous import PPO,Memory\n",
    "# from ddpg.utils import NormalizedEnv\n",
    "from flexipod_env import FlexipodEnv\n",
    "\n",
    "env = FlexipodEnv(dof = 12)\n",
    "# env = NormalizedEnv(env)\n",
    "\n",
    "############## Hyperparameters ##############\n",
    "# env_name = \"BipedalWalker-v3\"\n",
    "env_name = \"flexipod\"\n",
    "render = True\n",
    "solved_reward = 1500        # stop training if avg_reward > solved_reward\n",
    "log_interval = 10           # print avg reward in the interval\n",
    "# log_interval = 2           # print avg reward in the interval\n",
    "\n",
    "max_episodes = 20000        # max training episodes\n",
    "max_timesteps = 1500        # max timesteps in one episode\n",
    "\n",
    "# update_timestep = 4000      # update policy every n timesteps\n",
    "update_timestep = 3000      # update policy every n timesteps\n",
    "\n",
    "\n",
    "action_std = 0.5            # constant std for action distribution (Multivariate Normal)\n",
    "# action_std = 1.0          # constant std for action distribution (Multivariate Normal)\n",
    "K_epochs = 80               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr = 0.0003                 # parameters for Adam optimizer\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "random_seed = None\n",
    "#############################################\n",
    "# creating environment\n",
    "# env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "local-miniature",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T04:26:03.297158Z",
     "start_time": "2021-03-02T04:26:03.285626Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/soft12dof_experiment_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "outside-myrtle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T04:26:05.761735Z",
     "start_time": "2021-03-02T04:26:05.756744Z"
    }
   },
   "outputs": [],
   "source": [
    "# env = FlexipodEnv(dof = 12)\n",
    "# self = env\n",
    "# msg_rec,_,_,_ = env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-psychology",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-02T04:26:14.538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003 (0.9, 0.999)\n",
      "Episode 10 \t Avg length: 127 \t Avg reward: 158\n",
      "Episode 20 \t Avg length: 123 \t Avg reward: 153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\repo\\experimental_flexipod\\src\\python\\flexipod_env.py:126: UserWarning: step(): try #0:timed out\n",
      "  warnings.warn(f\"step(): try #{k}:{e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 30 \t Avg length: 135 \t Avg reward: 167\n",
      "Episode 40 \t Avg length: 160 \t Avg reward: 199\n",
      "Episode 50 \t Avg length: 148 \t Avg reward: 185\n",
      "Episode 60 \t Avg length: 166 \t Avg reward: 206\n",
      "Episode 70 \t Avg length: 186 \t Avg reward: 233\n",
      "Episode 80 \t Avg length: 187 \t Avg reward: 234\n",
      "Episode 90 \t Avg length: 157 \t Avg reward: 196\n",
      "Episode 100 \t Avg length: 151 \t Avg reward: 188\n"
     ]
    }
   ],
   "source": [
    "if random_seed:\n",
    "    print(\"Random Seed: {}\".format(random_seed))\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "memory = Memory()\n",
    "ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "print(lr,betas)\n",
    "# ppo.policy_old.load_state_dict(torch.load(f'./PPO_continuous_{env_name}_best.pth'))\n",
    "\n",
    "# logging variables\n",
    "running_reward = 0\n",
    "avg_length = 0\n",
    "max_avg_length = 0\n",
    "time_step = 0\n",
    "\n",
    "# training loop\n",
    "for i_episode in range(1, max_episodes+1):\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        time_step +=1\n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(state, memory)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Saving reward and is_terminals:\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "\n",
    "        # update if its time\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo.update(memory)\n",
    "            memory.clear_memory()\n",
    "            time_step = 0\n",
    "        running_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    avg_length += t\n",
    "\n",
    "    # stop training if avg_reward > solved_reward\n",
    "    if running_reward > (log_interval*solved_reward):\n",
    "        print(\"########## Solved! ##########\")\n",
    "        torch.save(ppo.policy.state_dict(), './PPO_continuous_solved_{}.pth'.format(env_name))\n",
    "        break\n",
    "\n",
    "    # save every 500 episodes\n",
    "    if i_episode % 500 == 0:\n",
    "        torch.save(ppo.policy.state_dict(), './PPO_continuous_{}.pth'.format(env_name))\n",
    "\n",
    "    # logging\n",
    "    if i_episode % log_interval == 0:\n",
    "        avg_length = avg_length/log_interval\n",
    "        running_reward = running_reward/log_interval\n",
    "        writer.add_scalar(\"avg_length/train\", avg_length, i_episode)\n",
    "        writer.add_scalar(\"running_reward/train\", running_reward, i_episode)\n",
    "        if avg_length>max_avg_length:\n",
    "            max_avg_length = avg_length\n",
    "            torch.save(ppo.policy.state_dict(), f'./PPO_continuous_{env_name}_best.pth')\n",
    "\n",
    "        print(f'Episode {i_episode} \\t Avg length: {avg_length:.0f} \\t Avg reward: {running_reward:.0f}')\n",
    "        running_reward = 0\n",
    "        avg_length = 0\n",
    "        \n",
    "env.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "homeless-police",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T13:44:56.890561Z",
     "start_time": "2021-02-27T13:44:56.882987Z"
    }
   },
   "outputs": [],
   "source": [
    "# writer.add_scalar(\"baseline_length/train\", 200, 0)\n",
    "# writer.add_scalar(\"baseline_length/train\", 200, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "polished-tooth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T03:30:40.475833Z",
     "start_time": "2021-03-02T03:29:53.366133Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1092\n",
      "279\n",
      "563\n",
      "394\n",
      "363\n",
      "395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\repo\\experimental_flexipod\\src\\python\\flexipod_env.py:126: UserWarning: step(): try #0:timed out\n",
      "  warnings.warn(f\"step(): try #{k}:{e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "932\n",
      "313\n",
      "121\n",
      "121\n",
      "mean time steps:457.3\n"
     ]
    }
   ],
   "source": [
    "memory_2 = Memory()\n",
    "\n",
    "action_std=.1\n",
    "ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "ppo.policy_old.load_state_dict(torch.load(f'./PPO_continuous_{env_name}_best.pth'))\n",
    "# ppo.policy_old.load_state_dict(torch.load(f'./PPO_continuous_{env_name}.pth'))\n",
    "\n",
    "time_steps = []\n",
    "for k in range(10):\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(state, memory_2)\n",
    "#         state, reward, done, _ = env.step(action)\n",
    "        state, reward, done, _ = env.step()\n",
    "\n",
    "        if done:\n",
    "            time_steps.append(t)\n",
    "            print(t)\n",
    "            break\n",
    "print(f\"mean time steps:{np.mean(time_steps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-domain",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
