{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assisted-county",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T13:18:55.282109Z",
     "start_time": "2021-03-18T13:18:52.624929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_device:0\n",
      "device_count:1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# https://discuss.pytorch.org/t/how-to-change-the-default-device-of-gpu-device-ids-0/1041/24\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=f\"{0}\"\n",
    "\n",
    "import torch\n",
    "print(f\"current_device:{torch.cuda.current_device()}\")\n",
    "print(f\"device_count:{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from ddpg import DDPGagent\n",
    "from ppo.PPO_continuous import PPO,Memory\n",
    "# from ddpg.utils import NormalizedEnv\n",
    "from flexipod_env import FlexipodEnv\n",
    "\n",
    "env = FlexipodEnv(dof = 12)\n",
    "# env = NormalizedEnv(env)\n",
    "\n",
    "############## Hyperparameters ##############\n",
    "# env_name = \"BipedalWalker-v3\"\n",
    "env_name = \"flexipod\"\n",
    "render = True\n",
    "solved_reward = 1500        # stop training if avg_reward > solved_reward\n",
    "log_interval = 80           # print avg reward in the interval\n",
    "# log_interval = 2           # print avg reward in the interval\n",
    "\n",
    "max_episodes = 20000        # max training episodes\n",
    "max_timesteps = 1500        # max timesteps in one episode\n",
    "\n",
    "# update_timestep = 4000      # update policy every n timesteps\n",
    "update_timestep = 3000      # update policy every n timesteps\n",
    "\n",
    "\n",
    "# action_std = 1.0            # constant std for action distribution (Multivariate Normal)\n",
    "action_std = 0.8            # constant std for action distribution (Multivariate Normal)\n",
    "K_epochs = 80               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr = 0.0002                 # parameters for Adam optimizer\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "random_seed = None\n",
    "#############################################\n",
    "# creating environment\n",
    "# env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "finnish-stockholm",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T19:40:53.037125Z",
     "start_time": "2021-03-17T19:40:53.021934Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/soft12dof_experiment_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amino-architecture",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T19:40:53.260175Z",
     "start_time": "2021-03-17T19:40:53.245173Z"
    }
   },
   "outputs": [],
   "source": [
    "# env = FlexipodEnv(dof = 12)\n",
    "# self = env\n",
    "# msg_rec,_,_,_ = env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-teacher",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-17T19:40:53.749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002 (0.9, 0.999)\n",
      "Episode 0 \t Avg length: 1 \t Avg reward: 1\n",
      "Episode 80 \t Avg length: 86 \t Avg reward: 108\n",
      "Episode 160 \t Avg length: 121 \t Avg reward: 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\repo\\experimental_flexipod\\src\\python\\flexipod_env.py:125: UserWarning: step(): try #0:timed out\n",
      "  warnings.warn(f\"step(): try #{k}:{e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 240 \t Avg length: 133 \t Avg reward: 167\n",
      "Episode 320 \t Avg length: 148 \t Avg reward: 185\n",
      "Episode 400 \t Avg length: 151 \t Avg reward: 189\n",
      "Episode 480 \t Avg length: 161 \t Avg reward: 202\n",
      "Episode 560 \t Avg length: 167 \t Avg reward: 210\n",
      "Episode 640 \t Avg length: 183 \t Avg reward: 231\n",
      "Episode 720 \t Avg length: 177 \t Avg reward: 223\n",
      "Episode 800 \t Avg length: 186 \t Avg reward: 235\n",
      "Episode 880 \t Avg length: 194 \t Avg reward: 245\n",
      "Episode 960 \t Avg length: 195 \t Avg reward: 247\n",
      "Episode 1040 \t Avg length: 198 \t Avg reward: 251\n",
      "Episode 1120 \t Avg length: 209 \t Avg reward: 265\n",
      "Episode 1200 \t Avg length: 212 \t Avg reward: 269\n",
      "Episode 1280 \t Avg length: 201 \t Avg reward: 255\n",
      "Episode 1360 \t Avg length: 196 \t Avg reward: 249\n",
      "Episode 1440 \t Avg length: 216 \t Avg reward: 275\n",
      "Episode 1520 \t Avg length: 217 \t Avg reward: 276\n",
      "Episode 1600 \t Avg length: 217 \t Avg reward: 276\n",
      "Episode 1680 \t Avg length: 218 \t Avg reward: 278\n",
      "Episode 1760 \t Avg length: 211 \t Avg reward: 268\n",
      "Episode 1840 \t Avg length: 214 \t Avg reward: 273\n",
      "Episode 1920 \t Avg length: 229 \t Avg reward: 291\n",
      "Episode 2000 \t Avg length: 237 \t Avg reward: 302\n",
      "Episode 2080 \t Avg length: 242 \t Avg reward: 308\n",
      "Episode 2160 \t Avg length: 245 \t Avg reward: 312\n",
      "Episode 2240 \t Avg length: 236 \t Avg reward: 301\n",
      "Episode 2320 \t Avg length: 237 \t Avg reward: 303\n",
      "Episode 2400 \t Avg length: 245 \t Avg reward: 311\n",
      "Episode 2480 \t Avg length: 236 \t Avg reward: 301\n",
      "load old best,avg_length=244.5875\n",
      "Episode 2560 \t Avg length: 228 \t Avg reward: 290\n",
      "Episode 2640 \t Avg length: 239 \t Avg reward: 304\n",
      "Episode 2720 \t Avg length: 247 \t Avg reward: 316\n",
      "Episode 2800 \t Avg length: 244 \t Avg reward: 311\n",
      "Episode 2880 \t Avg length: 256 \t Avg reward: 327\n",
      "Episode 2960 \t Avg length: 272 \t Avg reward: 348\n",
      "Episode 3040 \t Avg length: 286 \t Avg reward: 367\n",
      "Episode 3120 \t Avg length: 274 \t Avg reward: 352\n",
      "Episode 3200 \t Avg length: 285 \t Avg reward: 365\n",
      "Episode 3280 \t Avg length: 289 \t Avg reward: 371\n",
      "Episode 3360 \t Avg length: 271 \t Avg reward: 348\n",
      "Episode 3440 \t Avg length: 282 \t Avg reward: 361\n",
      "Episode 3520 \t Avg length: 260 \t Avg reward: 334\n",
      "Episode 3600 \t Avg length: 275 \t Avg reward: 353\n",
      "Episode 3680 \t Avg length: 261 \t Avg reward: 334\n",
      "Episode 3760 \t Avg length: 261 \t Avg reward: 334\n",
      "Episode 3840 \t Avg length: 265 \t Avg reward: 340\n",
      "Episode 3920 \t Avg length: 268 \t Avg reward: 343\n",
      "Episode 4000 \t Avg length: 265 \t Avg reward: 339\n",
      "Episode 4080 \t Avg length: 267 \t Avg reward: 342\n",
      "Episode 4160 \t Avg length: 259 \t Avg reward: 332\n",
      "load old best,avg_length=288.8875\n",
      "Episode 4240 \t Avg length: 247 \t Avg reward: 316\n",
      "Episode 4320 \t Avg length: 276 \t Avg reward: 353\n",
      "Episode 4400 \t Avg length: 262 \t Avg reward: 335\n",
      "Episode 4480 \t Avg length: 281 \t Avg reward: 360\n",
      "Episode 4560 \t Avg length: 280 \t Avg reward: 358\n",
      "Episode 4640 \t Avg length: 252 \t Avg reward: 321\n",
      "Episode 4720 \t Avg length: 251 \t Avg reward: 320\n",
      "load old best,avg_length=288.8875\n",
      "Episode 4800 \t Avg length: 252 \t Avg reward: 322\n",
      "load old best,avg_length=288.8875\n",
      "Episode 4880 \t Avg length: 258 \t Avg reward: 327\n",
      "Episode 4960 \t Avg length: 245 \t Avg reward: 313\n",
      "Episode 5040 \t Avg length: 253 \t Avg reward: 324\n",
      "Episode 5120 \t Avg length: 246 \t Avg reward: 314\n",
      "Episode 5200 \t Avg length: 252 \t Avg reward: 323\n",
      "Episode 5280 \t Avg length: 246 \t Avg reward: 314\n"
     ]
    }
   ],
   "source": [
    "if random_seed:\n",
    "    print(\"Random Seed: {}\".format(random_seed))\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "memory = Memory()\n",
    "ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "print(lr,betas)\n",
    "\n",
    "# logging variables\n",
    "running_reward = 0\n",
    "avg_length = 0\n",
    "max_avg_length = 0\n",
    "time_step = 0\n",
    "\n",
    "# checkpoint = ppo.load(f'./PPO_continuous_{env_name}_best.pth')\n",
    "# checkpoint = ppo.load(f'./PPO_continuous_{env_name}.pth')\n",
    "# max_avg_length = checkpoint[\"avg_length\"]\n",
    "\n",
    "# training loop\n",
    "for i_episode in range(0, max_episodes+1):\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        time_step +=1\n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(state, memory)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Saving reward and is_terminals:\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "\n",
    "        # update if its time\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo.update(memory)\n",
    "            memory.clear_memory()\n",
    "            time_step = 0\n",
    "        running_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    avg_length += t\n",
    "\n",
    "    # save every 500 episodes\n",
    "    if i_episode % 500 == 0:\n",
    "        ppo.save(f'./PPO_continuous_{env_name}.pth',avg_length=avg_length)\n",
    "\n",
    "    # logging\n",
    "    if i_episode % log_interval == 0:\n",
    "        avg_length = avg_length/log_interval\n",
    "        running_reward = running_reward/log_interval\n",
    "        writer.add_scalar(\"avg_length/train\", avg_length, i_episode)\n",
    "        writer.add_scalar(\"running_reward/train\", running_reward, i_episode)\n",
    "        \n",
    "        # stop training if avg_reward > solved_reward\n",
    "        if running_reward > (log_interval*solved_reward):\n",
    "            print(\"########## Solved! ##########\")\n",
    "            ppo.save(f'./PPO_continuous_solved_{env_name}.pth',avg_length=avg_length)\n",
    "            break\n",
    "            \n",
    "        if avg_length>max_avg_length:\n",
    "            max_avg_length = avg_length\n",
    "            ppo.save(f'./PPO_continuous_{env_name}_best.pth',avg_length=avg_length)\n",
    "        elif np.random.random()<0.1:# 50% chance \n",
    "            checkpoint = ppo.load(f'./PPO_continuous_{env_name}_best.pth')\n",
    "            print(f\"load old best,avg_length={checkpoint['avg_length']}\")# restart\n",
    "\n",
    "        print(f'Episode {i_episode} \\t Avg length: {avg_length:.0f} \\t Avg reward: {running_reward:.0f}')\n",
    "        running_reward = 0\n",
    "        avg_length = 0\n",
    "        \n",
    "env.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "terminal-washer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T13:44:56.890561Z",
     "start_time": "2021-02-27T13:44:56.882987Z"
    }
   },
   "outputs": [],
   "source": [
    "# writer.add_scalar(\"baseline_length/train\", 200, 0)\n",
    "# writer.add_scalar(\"baseline_length/train\", 200, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "introductory-presentation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T14:03:33.336395Z",
     "start_time": "2021-03-18T14:02:34.463986Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time steps:606 ,duration: 6.43[s], fps:94\n",
      "time steps:756 ,duration: 7.22[s], fps:105\n",
      "time steps:435 ,duration: 4.03[s], fps:108\n",
      "time steps:637 ,duration: 5.82[s], fps:109\n",
      "time steps:396 ,duration: 3.53[s], fps:112\n",
      "time steps:384 ,duration: 3.39[s], fps:113\n",
      "time steps:608 ,duration: 5.31[s], fps:115\n",
      "time steps:518 ,duration: 4.53[s], fps:114\n",
      "mean time steps:542 ,duration: 5.03[s]\n"
     ]
    }
   ],
   "source": [
    "memory_2 = Memory()\n",
    "\n",
    "max_timesteps=3000\n",
    "action_std=.1\n",
    "ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "ppo.load(f'./PPO_continuous_{env_name}_best.pth')\n",
    "\n",
    "time_steps = []\n",
    "durations = []\n",
    "for k in range(8):\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(state, memory_2)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#         state, reward, done, info = env.step()\n",
    "        if done or t==max_timesteps-1:\n",
    "            time_steps.append(t)\n",
    "            episode_duration= info['t'] - env.episode_start_time\n",
    "            durations.append(episode_duration)\n",
    "            print(f\"time steps:{t} ,duration: {episode_duration:.2f}[s], fps:{t/episode_duration:.0f}\")\n",
    "            break\n",
    "            \n",
    "print(f\"mean time steps:{np.mean(time_steps):.0f} ,duration: {np.mean(durations):.2f}[s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-sharp",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "posted-trick",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T12:55:34.534102Z",
     "start_time": "2021-03-18T12:55:34.497134Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blind-daisy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T13:18:17.228600Z",
     "start_time": "2021-03-18T13:18:17.179589Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Could not find function 'ppo.policy_old.act'.\n",
      "NameError: name 'ppo' is not defined\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "state = env.reset()\n",
    "memory_2 = Memory()\n",
    "def test():\n",
    "    for k in range(100):\n",
    "        ppo.select_action(state, memory_2)\n",
    "    \n",
    "%lprun -f ppo.policy_old.act test()\n",
    "# %lprun -f ppo.select_action test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "growing-information",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T12:58:35.484039Z",
     "start_time": "2021-03-18T12:58:35.466046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-costs",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
