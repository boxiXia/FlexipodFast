{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intensive-earthquake",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T20:31:53.443316Z",
     "start_time": "2021-03-08T20:31:45.098829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_device:0\n",
      "device_count:1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# https://discuss.pytorch.org/t/how-to-change-the-default-device-of-gpu-device-ids-0/1041/24\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=f\"{1}\"\n",
    "\n",
    "import torch\n",
    "print(f\"current_device:{torch.cuda.current_device()}\")\n",
    "print(f\"device_count:{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from ddpg import DDPGagent\n",
    "from ppo.PPO_continuous import PPO,Memory\n",
    "# from ddpg.utils import NormalizedEnv\n",
    "from flexipod_env import FlexipodEnv\n",
    "\n",
    "env = FlexipodEnv(dof = 12)\n",
    "# env = NormalizedEnv(env)\n",
    "\n",
    "############## Hyperparameters ##############\n",
    "# env_name = \"BipedalWalker-v3\"\n",
    "env_name = \"flexipod\"\n",
    "render = True\n",
    "solved_reward = 1500        # stop training if avg_reward > solved_reward\n",
    "log_interval = 80           # print avg reward in the interval\n",
    "# log_interval = 2           # print avg reward in the interval\n",
    "\n",
    "max_episodes = 20000        # max training episodes\n",
    "max_timesteps = 1500        # max timesteps in one episode\n",
    "\n",
    "# update_timestep = 4000      # update policy every n timesteps\n",
    "update_timestep = 3000      # update policy every n timesteps\n",
    "\n",
    "\n",
    "action_std = 0.5            # constant std for action distribution (Multivariate Normal)\n",
    "# action_std = 1.0          # constant std for action distribution (Multivariate Normal)\n",
    "K_epochs = 80               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr = 0.0002                 # parameters for Adam optimizer\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "random_seed = None\n",
    "#############################################\n",
    "# creating environment\n",
    "# env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alleged-bouquet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T02:03:02.388480Z",
     "start_time": "2021-03-08T02:03:02.061403Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/soft12dof_experiment_9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alternative-devon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T02:03:03.247618Z",
     "start_time": "2021-03-08T02:03:03.235615Z"
    }
   },
   "outputs": [],
   "source": [
    "# env = FlexipodEnv(dof = 12)\n",
    "# self = env\n",
    "# msg_rec,_,_,_ = env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sunset-proportion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T20:00:58.563484Z",
     "start_time": "2021-03-08T02:03:04.611165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002 (0.9, 0.999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\repo\\experimental_flexipod\\src\\python\\flexipod_env.py:128: UserWarning: step(): try #0:timed out\n",
      "  warnings.warn(f\"step(): try #{k}:{e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 80 \t Avg length: 119 \t Avg reward: 150\n",
      "Episode 160 \t Avg length: 141 \t Avg reward: 178\n",
      "Episode 240 \t Avg length: 150 \t Avg reward: 189\n",
      "Episode 320 \t Avg length: 176 \t Avg reward: 224\n",
      "Episode 400 \t Avg length: 209 \t Avg reward: 266\n",
      "Episode 480 \t Avg length: 247 \t Avg reward: 316\n",
      "Episode 560 \t Avg length: 304 \t Avg reward: 390\n",
      "Episode 640 \t Avg length: 311 \t Avg reward: 399\n",
      "Episode 720 \t Avg length: 337 \t Avg reward: 434\n",
      "Episode 800 \t Avg length: 347 \t Avg reward: 447\n",
      "Episode 880 \t Avg length: 387 \t Avg reward: 500\n",
      "Episode 960 \t Avg length: 429 \t Avg reward: 554\n",
      "Episode 1040 \t Avg length: 365 \t Avg reward: 471\n",
      "Episode 1120 \t Avg length: 376 \t Avg reward: 484\n",
      "Episode 1200 \t Avg length: 434 \t Avg reward: 560\n",
      "Episode 1280 \t Avg length: 449 \t Avg reward: 580\n",
      "load old best,avg_length=448.5875\n",
      "Episode 1360 \t Avg length: 365 \t Avg reward: 472\n",
      "load old best,avg_length=448.5875\n",
      "Episode 1440 \t Avg length: 372 \t Avg reward: 481\n",
      "load old best,avg_length=448.5875\n",
      "Episode 1520 \t Avg length: 338 \t Avg reward: 435\n",
      "Episode 1600 \t Avg length: 380 \t Avg reward: 492\n",
      "Episode 1680 \t Avg length: 365 \t Avg reward: 472\n",
      "Episode 1760 \t Avg length: 372 \t Avg reward: 481\n",
      "Episode 1840 \t Avg length: 423 \t Avg reward: 547\n",
      "Episode 1920 \t Avg length: 407 \t Avg reward: 527\n",
      "Episode 2000 \t Avg length: 423 \t Avg reward: 548\n",
      "load old best,avg_length=448.5875\n",
      "Episode 2080 \t Avg length: 442 \t Avg reward: 572\n",
      "load old best,avg_length=448.5875\n",
      "Episode 2160 \t Avg length: 344 \t Avg reward: 444\n",
      "Episode 2240 \t Avg length: 403 \t Avg reward: 521\n",
      "Episode 2320 \t Avg length: 424 \t Avg reward: 548\n",
      "Episode 2400 \t Avg length: 386 \t Avg reward: 499\n",
      "Episode 2480 \t Avg length: 373 \t Avg reward: 482\n",
      "Episode 2560 \t Avg length: 407 \t Avg reward: 527\n",
      "Episode 2640 \t Avg length: 403 \t Avg reward: 521\n",
      "Episode 2720 \t Avg length: 448 \t Avg reward: 581\n",
      "Episode 2800 \t Avg length: 499 \t Avg reward: 645\n",
      "Episode 2880 \t Avg length: 536 \t Avg reward: 694\n",
      "Episode 2960 \t Avg length: 519 \t Avg reward: 672\n",
      "Episode 3040 \t Avg length: 518 \t Avg reward: 669\n",
      "Episode 3120 \t Avg length: 474 \t Avg reward: 611\n",
      "Episode 3200 \t Avg length: 477 \t Avg reward: 614\n",
      "Episode 3280 \t Avg length: 411 \t Avg reward: 527\n",
      "Episode 3360 \t Avg length: 433 \t Avg reward: 557\n",
      "Episode 3440 \t Avg length: 371 \t Avg reward: 477\n",
      "Episode 3520 \t Avg length: 471 \t Avg reward: 608\n",
      "Episode 3600 \t Avg length: 466 \t Avg reward: 601\n",
      "Episode 3680 \t Avg length: 429 \t Avg reward: 553\n",
      "Episode 3760 \t Avg length: 422 \t Avg reward: 542\n",
      "Episode 3840 \t Avg length: 486 \t Avg reward: 629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\repo\\experimental_flexipod\\src\\python\\flexipod_env.py:128: UserWarning: step(): try #1:timed out\n",
      "  warnings.warn(f\"step(): try #{k}:{e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3920 \t Avg length: 455 \t Avg reward: 589\n",
      "Episode 4000 \t Avg length: 481 \t Avg reward: 622\n",
      "Episode 4080 \t Avg length: 460 \t Avg reward: 595\n",
      "Episode 4160 \t Avg length: 438 \t Avg reward: 564\n",
      "Episode 4240 \t Avg length: 447 \t Avg reward: 573\n",
      "Episode 4320 \t Avg length: 437 \t Avg reward: 562\n",
      "load old best,avg_length=535.7625\n",
      "Episode 4400 \t Avg length: 437 \t Avg reward: 562\n",
      "load old best,avg_length=535.7625\n",
      "Episode 4480 \t Avg length: 454 \t Avg reward: 586\n",
      "Episode 4560 \t Avg length: 484 \t Avg reward: 626\n",
      "Episode 4640 \t Avg length: 532 \t Avg reward: 687\n",
      "Episode 4720 \t Avg length: 559 \t Avg reward: 721\n",
      "Episode 4800 \t Avg length: 616 \t Avg reward: 796\n",
      "Episode 4880 \t Avg length: 662 \t Avg reward: 858\n",
      "Episode 4960 \t Avg length: 716 \t Avg reward: 928\n",
      "Episode 5040 \t Avg length: 593 \t Avg reward: 768\n",
      "Episode 5120 \t Avg length: 564 \t Avg reward: 730\n",
      "Episode 5200 \t Avg length: 491 \t Avg reward: 636\n",
      "Episode 5280 \t Avg length: 448 \t Avg reward: 580\n",
      "Episode 5360 \t Avg length: 462 \t Avg reward: 597\n",
      "Episode 5440 \t Avg length: 401 \t Avg reward: 518\n",
      "Episode 5520 \t Avg length: 525 \t Avg reward: 677\n",
      "load old best,avg_length=716.4625\n",
      "Episode 5600 \t Avg length: 442 \t Avg reward: 570\n",
      "Episode 5680 \t Avg length: 455 \t Avg reward: 583\n",
      "load old best,avg_length=716.4625\n",
      "Episode 5760 \t Avg length: 489 \t Avg reward: 625\n",
      "Episode 5840 \t Avg length: 562 \t Avg reward: 724\n",
      "Episode 5920 \t Avg length: 524 \t Avg reward: 675\n",
      "load old best,avg_length=716.4625\n",
      "Episode 6000 \t Avg length: 509 \t Avg reward: 653\n",
      "Episode 6080 \t Avg length: 555 \t Avg reward: 716\n",
      "Episode 6160 \t Avg length: 521 \t Avg reward: 673\n",
      "Episode 6240 \t Avg length: 439 \t Avg reward: 566\n",
      "Episode 6320 \t Avg length: 558 \t Avg reward: 722\n",
      "Episode 6400 \t Avg length: 521 \t Avg reward: 675\n",
      "Episode 6480 \t Avg length: 506 \t Avg reward: 655\n",
      "Episode 6560 \t Avg length: 493 \t Avg reward: 637\n",
      "Episode 6640 \t Avg length: 510 \t Avg reward: 658\n",
      "Episode 6720 \t Avg length: 475 \t Avg reward: 613\n",
      "Episode 6800 \t Avg length: 571 \t Avg reward: 739\n",
      "Episode 6880 \t Avg length: 465 \t Avg reward: 601\n",
      "Episode 6960 \t Avg length: 437 \t Avg reward: 563\n",
      "Episode 7040 \t Avg length: 421 \t Avg reward: 542\n",
      "Episode 7120 \t Avg length: 404 \t Avg reward: 521\n",
      "Episode 7200 \t Avg length: 390 \t Avg reward: 503\n",
      "Episode 7280 \t Avg length: 458 \t Avg reward: 592\n",
      "Episode 7360 \t Avg length: 542 \t Avg reward: 702\n",
      "Episode 7440 \t Avg length: 581 \t Avg reward: 752\n",
      "Episode 7520 \t Avg length: 486 \t Avg reward: 630\n",
      "Episode 7600 \t Avg length: 829 \t Avg reward: 1077\n",
      "Episode 7680 \t Avg length: 838 \t Avg reward: 1090\n",
      "Episode 7760 \t Avg length: 650 \t Avg reward: 843\n",
      "Episode 7840 \t Avg length: 604 \t Avg reward: 782\n",
      "Episode 7920 \t Avg length: 469 \t Avg reward: 607\n",
      "Episode 8000 \t Avg length: 541 \t Avg reward: 702\n",
      "Episode 8080 \t Avg length: 629 \t Avg reward: 815\n",
      "Episode 8160 \t Avg length: 549 \t Avg reward: 711\n",
      "Episode 8240 \t Avg length: 604 \t Avg reward: 783\n",
      "Episode 8320 \t Avg length: 651 \t Avg reward: 845\n",
      "Episode 8400 \t Avg length: 668 \t Avg reward: 866\n",
      "Episode 8480 \t Avg length: 550 \t Avg reward: 712\n",
      "Episode 8560 \t Avg length: 595 \t Avg reward: 770\n",
      "Episode 8640 \t Avg length: 671 \t Avg reward: 870\n",
      "Episode 8720 \t Avg length: 558 \t Avg reward: 722\n",
      "Episode 8800 \t Avg length: 551 \t Avg reward: 713\n",
      "Episode 8880 \t Avg length: 579 \t Avg reward: 749\n",
      "Episode 8960 \t Avg length: 523 \t Avg reward: 676\n",
      "Episode 9040 \t Avg length: 545 \t Avg reward: 704\n",
      "Episode 9120 \t Avg length: 554 \t Avg reward: 716\n",
      "Episode 9200 \t Avg length: 606 \t Avg reward: 784\n",
      "load old best,avg_length=838.4375\n",
      "Episode 9280 \t Avg length: 607 \t Avg reward: 785\n",
      "Episode 9360 \t Avg length: 692 \t Avg reward: 897\n",
      "Episode 9440 \t Avg length: 611 \t Avg reward: 791\n",
      "Episode 9520 \t Avg length: 628 \t Avg reward: 812\n",
      "Episode 9600 \t Avg length: 519 \t Avg reward: 672\n",
      "Episode 9680 \t Avg length: 509 \t Avg reward: 657\n",
      "Episode 9760 \t Avg length: 470 \t Avg reward: 607\n",
      "Episode 9840 \t Avg length: 509 \t Avg reward: 658\n",
      "Episode 9920 \t Avg length: 656 \t Avg reward: 850\n",
      "Episode 10000 \t Avg length: 707 \t Avg reward: 916\n",
      "Episode 10080 \t Avg length: 736 \t Avg reward: 953\n",
      "Episode 10160 \t Avg length: 708 \t Avg reward: 916\n",
      "Episode 10240 \t Avg length: 789 \t Avg reward: 1023\n",
      "Episode 10320 \t Avg length: 642 \t Avg reward: 831\n",
      "Episode 10400 \t Avg length: 471 \t Avg reward: 608\n",
      "Episode 10480 \t Avg length: 413 \t Avg reward: 531\n",
      "Episode 10560 \t Avg length: 389 \t Avg reward: 500\n",
      "Episode 10640 \t Avg length: 485 \t Avg reward: 626\n",
      "Episode 10720 \t Avg length: 328 \t Avg reward: 423\n",
      "Episode 10800 \t Avg length: 352 \t Avg reward: 454\n",
      "load old best,avg_length=838.4375\n",
      "Episode 10880 \t Avg length: 299 \t Avg reward: 385\n",
      "load old best,avg_length=838.4375\n",
      "Episode 10960 \t Avg length: 397 \t Avg reward: 512\n",
      "Episode 11040 \t Avg length: 418 \t Avg reward: 541\n",
      "Episode 11120 \t Avg length: 474 \t Avg reward: 613\n",
      "Episode 11200 \t Avg length: 572 \t Avg reward: 738\n",
      "Episode 11280 \t Avg length: 466 \t Avg reward: 602\n",
      "Episode 11360 \t Avg length: 438 \t Avg reward: 566\n",
      "Episode 11440 \t Avg length: 439 \t Avg reward: 567\n",
      "Episode 11520 \t Avg length: 510 \t Avg reward: 659\n",
      "Episode 11600 \t Avg length: 491 \t Avg reward: 634\n",
      "Episode 11680 \t Avg length: 588 \t Avg reward: 761\n",
      "load old best,avg_length=838.4375\n",
      "Episode 11760 \t Avg length: 602 \t Avg reward: 779\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3db5f05cb834>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mtime_step\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Running policy_old:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\repo\\experimental_flexipod\\src\\python\\ppo\\PPO_continuous.py\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(self, state, memory)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_old\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\repo\\experimental_flexipod\\src\\python\\ppo\\PPO_continuous.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, memory)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[0mcov_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultivariateNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcov_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0maction_logprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\flexipod\\lib\\site-packages\\torch\\distributions\\multivariate_normal.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscale_tril\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mcovariance_matrix\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcovariance_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# precision_matrix is not None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_precision_to_scale_tril\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if random_seed:\n",
    "    print(\"Random Seed: {}\".format(random_seed))\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "memory = Memory()\n",
    "ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "print(lr,betas)\n",
    "\n",
    "max_avg_length = 0\n",
    "checkpoint = ppo.load(f'./PPO_continuous_{env_name}_best.pth')\n",
    "max_avg_length = checkpoint[\"avg_length\"]\n",
    "\n",
    "# logging variables\n",
    "running_reward = 0\n",
    "avg_length = 0\n",
    "# max_avg_length = 0\n",
    "time_step = 0\n",
    "\n",
    "# training loop\n",
    "for i_episode in range(1, max_episodes+1):\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        time_step +=1\n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(state, memory)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Saving reward and is_terminals:\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "\n",
    "        # update if its time\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo.update(memory)\n",
    "            memory.clear_memory()\n",
    "            time_step = 0\n",
    "        running_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    avg_length += t\n",
    "\n",
    "    # save every 500 episodes\n",
    "    if i_episode % 500 == 0:\n",
    "        ppo.save(f'./PPO_continuous_{env_name}.pth',avg_length=avg_length)\n",
    "\n",
    "    # logging\n",
    "    if i_episode % log_interval == 0:\n",
    "        avg_length = avg_length/log_interval\n",
    "        running_reward = running_reward/log_interval\n",
    "        writer.add_scalar(\"avg_length/train\", avg_length, i_episode)\n",
    "        writer.add_scalar(\"running_reward/train\", running_reward, i_episode)\n",
    "        \n",
    "        # stop training if avg_reward > solved_reward\n",
    "        if running_reward > (log_interval*solved_reward):\n",
    "            print(\"########## Solved! ##########\")\n",
    "            ppo.save(f'./PPO_continuous_solved_{env_name}.pth',avg_length=avg_length)\n",
    "            break\n",
    "            \n",
    "        if avg_length>max_avg_length:\n",
    "            max_avg_length = avg_length\n",
    "            ppo.save(f'./PPO_continuous_{env_name}_best.pth',avg_length=avg_length)\n",
    "        elif np.random.random()<0.1:# 50% chance \n",
    "            checkpoint = ppo.load(f'./PPO_continuous_{env_name}_best.pth')\n",
    "            print(f\"load old best,avg_length={checkpoint['avg_length']}\")# restart\n",
    "\n",
    "        print(f'Episode {i_episode} \\t Avg length: {avg_length:.0f} \\t Avg reward: {running_reward:.0f}')\n",
    "        running_reward = 0\n",
    "        avg_length = 0\n",
    "        \n",
    "env.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ongoing-preliminary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T13:44:56.890561Z",
     "start_time": "2021-02-27T13:44:56.882987Z"
    }
   },
   "outputs": [],
   "source": [
    "# writer.add_scalar(\"baseline_length/train\", 200, 0)\n",
    "# writer.add_scalar(\"baseline_length/train\", 200, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "confirmed-establishment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T20:38:07.632278Z",
     "start_time": "2021-03-08T20:31:55.451758Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time steps:1192 ,duration: 16.46[s], fps:72\n",
      "time steps:2999 ,duration: 39.21[s], fps:76\n",
      "time steps:2999 ,duration: 39.64[s], fps:76\n",
      "time steps:2999 ,duration: 40.30[s], fps:74\n",
      "time steps:1282 ,duration: 17.68[s], fps:73\n",
      "time steps:1695 ,duration: 22.17[s], fps:76\n",
      "time steps:2189 ,duration: 28.23[s], fps:78\n",
      "time steps:1148 ,duration: 14.67[s], fps:78\n",
      "time steps:1183 ,duration: 15.24[s], fps:78\n",
      "time steps:1767 ,duration: 22.91[s], fps:77\n",
      "time steps:1449 ,duration: 18.85[s], fps:77\n",
      "time steps:1689 ,duration: 22.20[s], fps:76\n",
      "time steps:231 ,duration: 3.27[s], fps:71\n",
      "time steps:977 ,duration: 12.64[s], fps:77\n",
      "time steps:2014 ,duration: 26.07[s], fps:77\n",
      "time steps:432 ,duration: 5.60[s], fps:77\n",
      "time steps:1237 ,duration: 16.18[s], fps:76\n",
      "time steps:1076 ,duration: 13.95[s], fps:77\n",
      "time steps:492 ,duration: 6.59[s], fps:75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\repo\\experimental_flexipod\\src\\python\\flexipod_env.py:125: UserWarning: step(): try #0:timed out\n",
      "  warnings.warn(f\"step(): try #{k}:{e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time steps:1781 ,duration: 23.10[s], fps:77\n",
      "mean time steps:1542 ,duration: 20.25[s]\n"
     ]
    }
   ],
   "source": [
    "memory_2 = Memory()\n",
    "\n",
    "max_timesteps=3000\n",
    "action_std=.01\n",
    "ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "ppo.load(f'./PPO_continuous_{env_name}_best.pth')\n",
    "\n",
    "time_steps = []\n",
    "durations = []\n",
    "for k in range(20):\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(state, memory_2)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#         state, reward, done, _ = env.step()\n",
    "        if done or t==max_timesteps-1:\n",
    "            time_steps.append(t)\n",
    "            episode_duration= info['t'] - env.episode_start_time\n",
    "            durations.append(episode_duration)\n",
    "            print(f\"time steps:{t} ,duration: {episode_duration:.2f}[s], fps:{t/episode_duration:.0f}\")\n",
    "            break\n",
    "            \n",
    "print(f\"mean time steps:{np.mean(time_steps):.0f} ,duration: {np.mean(durations):.2f}[s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-brunswick",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-drawing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
