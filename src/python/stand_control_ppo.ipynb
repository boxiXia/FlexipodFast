{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "excessive-attention",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T01:57:07.068831Z",
     "start_time": "2021-03-02T01:57:07.051821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_device:0\n",
      "device_count:1\n",
      "Destructor called, FlexipodEnv deleted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# https://discuss.pytorch.org/t/how-to-change-the-default-device-of-gpu-device-ids-0/1041/24\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=f\"{1}\"\n",
    "\n",
    "import torch\n",
    "print(f\"current_device:{torch.cuda.current_device()}\")\n",
    "print(f\"device_count:{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from ddpg import DDPGagent\n",
    "from ppo.PPO_continuous import PPO,Memory\n",
    "# from ddpg.utils import NormalizedEnv\n",
    "from flexipod_env import FlexipodEnv\n",
    "\n",
    "env = FlexipodEnv(dof = 12)\n",
    "# env = NormalizedEnv(env)\n",
    "\n",
    "############## Hyperparameters ##############\n",
    "# env_name = \"BipedalWalker-v3\"\n",
    "env_name = \"flexipod\"\n",
    "render = True\n",
    "solved_reward = 300         # stop training if avg_reward > solved_reward\n",
    "log_interval = 20           # print avg reward in the interval\n",
    "max_episodes = 20000        # max training episodes\n",
    "max_timesteps = 1500        # max timesteps in one episode\n",
    "\n",
    "update_timestep = 4000      # update policy every n timesteps\n",
    "action_std = 0.5            # constant std for action distribution (Multivariate Normal)\n",
    "# action_std = 1.0          # constant std for action distribution (Multivariate Normal)\n",
    "K_epochs = 80               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr = 0.0003                 # parameters for Adam optimizer\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "random_seed = None\n",
    "#############################################\n",
    "# creating environment\n",
    "# env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "convinced-camcorder",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T01:57:46.867426Z",
     "start_time": "2021-03-02T01:57:46.856414Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/soft12dof_experiment_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "selective-friday",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T01:57:09.674465Z",
     "start_time": "2021-03-02T01:57:09.670474Z"
    }
   },
   "outputs": [],
   "source": [
    "# env = FlexipodEnv(dof = 12)\n",
    "# self = env\n",
    "# msg_rec,_,_,_ = env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-technical",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-02T01:57:50.976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003 (0.9, 0.999)\n",
      "Episode 20 \t Avg length: 199 \t Avg reward: 263\n",
      "Episode 40 \t Avg length: 190 \t Avg reward: 252\n",
      "Episode 60 \t Avg length: 176 \t Avg reward: 233\n",
      "Episode 80 \t Avg length: 172 \t Avg reward: 228\n",
      "Episode 100 \t Avg length: 176 \t Avg reward: 231\n",
      "Episode 120 \t Avg length: 203 \t Avg reward: 269\n",
      "Episode 140 \t Avg length: 193 \t Avg reward: 255\n",
      "Episode 160 \t Avg length: 152 \t Avg reward: 200\n",
      "Episode 180 \t Avg length: 203 \t Avg reward: 269\n",
      "Episode 200 \t Avg length: 185 \t Avg reward: 245\n",
      "Episode 220 \t Avg length: 188 \t Avg reward: 248\n",
      "Episode 240 \t Avg length: 199 \t Avg reward: 263\n",
      "Episode 260 \t Avg length: 191 \t Avg reward: 251\n",
      "Episode 280 \t Avg length: 179 \t Avg reward: 236\n",
      "Episode 300 \t Avg length: 219 \t Avg reward: 289\n",
      "Episode 320 \t Avg length: 205 \t Avg reward: 271\n",
      "Episode 340 \t Avg length: 196 \t Avg reward: 259\n",
      "Episode 360 \t Avg length: 183 \t Avg reward: 241\n",
      "Episode 380 \t Avg length: 183 \t Avg reward: 242\n",
      "Episode 400 \t Avg length: 190 \t Avg reward: 250\n",
      "Episode 420 \t Avg length: 173 \t Avg reward: 227\n",
      "Episode 440 \t Avg length: 182 \t Avg reward: 239\n",
      "Episode 460 \t Avg length: 190 \t Avg reward: 251\n",
      "Episode 480 \t Avg length: 191 \t Avg reward: 253\n",
      "Episode 500 \t Avg length: 182 \t Avg reward: 241\n",
      "Episode 520 \t Avg length: 175 \t Avg reward: 230\n",
      "Episode 540 \t Avg length: 181 \t Avg reward: 238\n",
      "Episode 560 \t Avg length: 205 \t Avg reward: 271\n",
      "Episode 580 \t Avg length: 179 \t Avg reward: 236\n",
      "Episode 600 \t Avg length: 184 \t Avg reward: 244\n",
      "Episode 620 \t Avg length: 174 \t Avg reward: 230\n",
      "Episode 640 \t Avg length: 173 \t Avg reward: 228\n",
      "Episode 660 \t Avg length: 185 \t Avg reward: 244\n",
      "Episode 680 \t Avg length: 172 \t Avg reward: 226\n",
      "Episode 700 \t Avg length: 186 \t Avg reward: 247\n",
      "Episode 720 \t Avg length: 183 \t Avg reward: 241\n",
      "Episode 740 \t Avg length: 190 \t Avg reward: 252\n",
      "Episode 760 \t Avg length: 188 \t Avg reward: 249\n",
      "Episode 780 \t Avg length: 189 \t Avg reward: 249\n",
      "Episode 800 \t Avg length: 198 \t Avg reward: 262\n",
      "Episode 820 \t Avg length: 187 \t Avg reward: 248\n",
      "Episode 840 \t Avg length: 187 \t Avg reward: 247\n",
      "Episode 860 \t Avg length: 193 \t Avg reward: 256\n",
      "Episode 880 \t Avg length: 168 \t Avg reward: 222\n",
      "Episode 900 \t Avg length: 179 \t Avg reward: 237\n",
      "Episode 920 \t Avg length: 182 \t Avg reward: 241\n",
      "Episode 940 \t Avg length: 181 \t Avg reward: 239\n",
      "Episode 960 \t Avg length: 177 \t Avg reward: 233\n",
      "Episode 980 \t Avg length: 175 \t Avg reward: 231\n",
      "Episode 1000 \t Avg length: 184 \t Avg reward: 245\n",
      "Episode 1020 \t Avg length: 192 \t Avg reward: 254\n",
      "Episode 1040 \t Avg length: 168 \t Avg reward: 223\n",
      "Episode 1060 \t Avg length: 169 \t Avg reward: 223\n",
      "Episode 1080 \t Avg length: 174 \t Avg reward: 230\n",
      "Episode 1100 \t Avg length: 181 \t Avg reward: 240\n",
      "Episode 1120 \t Avg length: 162 \t Avg reward: 214\n",
      "Episode 1140 \t Avg length: 168 \t Avg reward: 222\n",
      "Episode 1160 \t Avg length: 173 \t Avg reward: 229\n",
      "Episode 1180 \t Avg length: 174 \t Avg reward: 231\n",
      "Episode 1200 \t Avg length: 184 \t Avg reward: 243\n",
      "Episode 1220 \t Avg length: 164 \t Avg reward: 217\n",
      "Episode 1240 \t Avg length: 179 \t Avg reward: 237\n",
      "Episode 1260 \t Avg length: 176 \t Avg reward: 232\n",
      "Episode 1280 \t Avg length: 181 \t Avg reward: 240\n",
      "Episode 1300 \t Avg length: 174 \t Avg reward: 231\n",
      "Episode 1320 \t Avg length: 166 \t Avg reward: 219\n",
      "Episode 1340 \t Avg length: 179 \t Avg reward: 237\n",
      "Episode 1360 \t Avg length: 166 \t Avg reward: 219\n",
      "Episode 1380 \t Avg length: 160 \t Avg reward: 212\n",
      "Episode 1400 \t Avg length: 169 \t Avg reward: 224\n",
      "Episode 1420 \t Avg length: 176 \t Avg reward: 233\n",
      "Episode 1440 \t Avg length: 170 \t Avg reward: 225\n",
      "Episode 1460 \t Avg length: 165 \t Avg reward: 218\n",
      "Episode 1480 \t Avg length: 169 \t Avg reward: 224\n",
      "Episode 1500 \t Avg length: 168 \t Avg reward: 223\n",
      "Episode 1520 \t Avg length: 165 \t Avg reward: 218\n",
      "Episode 1540 \t Avg length: 176 \t Avg reward: 233\n",
      "Episode 1560 \t Avg length: 168 \t Avg reward: 223\n",
      "Episode 1580 \t Avg length: 158 \t Avg reward: 209\n",
      "Episode 1600 \t Avg length: 180 \t Avg reward: 239\n",
      "Episode 1620 \t Avg length: 175 \t Avg reward: 232\n",
      "Episode 1640 \t Avg length: 170 \t Avg reward: 224\n",
      "Episode 1660 \t Avg length: 173 \t Avg reward: 228\n",
      "Episode 1680 \t Avg length: 181 \t Avg reward: 239\n",
      "Episode 1700 \t Avg length: 170 \t Avg reward: 225\n",
      "Episode 1720 \t Avg length: 170 \t Avg reward: 224\n",
      "Episode 1740 \t Avg length: 170 \t Avg reward: 225\n",
      "Episode 1760 \t Avg length: 183 \t Avg reward: 242\n",
      "Episode 1780 \t Avg length: 167 \t Avg reward: 221\n",
      "Episode 1800 \t Avg length: 183 \t Avg reward: 242\n",
      "Episode 1820 \t Avg length: 176 \t Avg reward: 233\n",
      "Episode 1840 \t Avg length: 173 \t Avg reward: 229\n",
      "Episode 1860 \t Avg length: 180 \t Avg reward: 239\n",
      "Episode 1880 \t Avg length: 164 \t Avg reward: 216\n",
      "Episode 1900 \t Avg length: 185 \t Avg reward: 245\n",
      "Episode 1920 \t Avg length: 179 \t Avg reward: 236\n",
      "Episode 1940 \t Avg length: 174 \t Avg reward: 230\n",
      "Episode 1960 \t Avg length: 184 \t Avg reward: 244\n",
      "Episode 1980 \t Avg length: 193 \t Avg reward: 256\n",
      "Episode 2000 \t Avg length: 177 \t Avg reward: 234\n",
      "Episode 2020 \t Avg length: 187 \t Avg reward: 247\n",
      "Episode 2040 \t Avg length: 179 \t Avg reward: 235\n",
      "Episode 2060 \t Avg length: 178 \t Avg reward: 236\n",
      "Episode 2080 \t Avg length: 184 \t Avg reward: 243\n",
      "Episode 2100 \t Avg length: 164 \t Avg reward: 217\n",
      "Episode 2120 \t Avg length: 173 \t Avg reward: 230\n",
      "Episode 2140 \t Avg length: 152 \t Avg reward: 201\n",
      "Episode 2160 \t Avg length: 170 \t Avg reward: 225\n",
      "Episode 2180 \t Avg length: 172 \t Avg reward: 228\n",
      "Episode 2200 \t Avg length: 178 \t Avg reward: 236\n",
      "Episode 2220 \t Avg length: 174 \t Avg reward: 230\n",
      "Episode 2240 \t Avg length: 162 \t Avg reward: 215\n",
      "Episode 2260 \t Avg length: 169 \t Avg reward: 225\n",
      "Episode 2280 \t Avg length: 165 \t Avg reward: 219\n",
      "Episode 2300 \t Avg length: 174 \t Avg reward: 229\n",
      "Episode 2320 \t Avg length: 166 \t Avg reward: 220\n",
      "Episode 2340 \t Avg length: 176 \t Avg reward: 234\n",
      "Episode 2360 \t Avg length: 180 \t Avg reward: 238\n",
      "Episode 2380 \t Avg length: 174 \t Avg reward: 231\n",
      "Episode 2400 \t Avg length: 183 \t Avg reward: 243\n"
     ]
    }
   ],
   "source": [
    "if random_seed:\n",
    "    print(\"Random Seed: {}\".format(random_seed))\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "memory = Memory()\n",
    "ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "print(lr,betas)\n",
    "# ppo.policy_old.load_state_dict(torch.load(f'./PPO_continuous_{env_name}_best.pth'))\n",
    "\n",
    "# logging variables\n",
    "running_reward = 0\n",
    "avg_length = 0\n",
    "max_avg_length = 0\n",
    "time_step = 0\n",
    "\n",
    "# training loop\n",
    "for i_episode in range(1, max_episodes+1):\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        time_step +=1\n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(state, memory)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Saving reward and is_terminals:\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "\n",
    "        # update if its time\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo.update(memory)\n",
    "            memory.clear_memory()\n",
    "            time_step = 0\n",
    "        running_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    avg_length += t\n",
    "\n",
    "    # stop training if avg_reward > solved_reward\n",
    "    if running_reward > (log_interval*solved_reward):\n",
    "        print(\"########## Solved! ##########\")\n",
    "        torch.save(ppo.policy.state_dict(), './PPO_continuous_solved_{}.pth'.format(env_name))\n",
    "        break\n",
    "\n",
    "    # save every 500 episodes\n",
    "    if i_episode % 500 == 0:\n",
    "        torch.save(ppo.policy.state_dict(), './PPO_continuous_{}.pth'.format(env_name))\n",
    "\n",
    "    # logging\n",
    "    if i_episode % log_interval == 0:\n",
    "        avg_length = avg_length/log_interval\n",
    "        running_reward = running_reward/log_interval\n",
    "        writer.add_scalar(\"avg_length/train\", avg_length, i_episode)\n",
    "        writer.add_scalar(\"running_reward/train\", running_reward, i_episode)\n",
    "        if avg_length>max_avg_length:\n",
    "            max_avg_length = avg_length\n",
    "            torch.save(ppo.policy.state_dict(), f'./PPO_continuous_{env_name}_best.pth')\n",
    "\n",
    "        print(f'Episode {i_episode} \\t Avg length: {avg_length:.0f} \\t Avg reward: {running_reward:.0f}')\n",
    "        running_reward = 0\n",
    "        avg_length = 0\n",
    "        \n",
    "env.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "complete-vision",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T13:44:56.890561Z",
     "start_time": "2021-02-27T13:44:56.882987Z"
    }
   },
   "outputs": [],
   "source": [
    "# writer.add_scalar(\"baseline_length/train\", 200, 0)\n",
    "# writer.add_scalar(\"baseline_length/train\", 200, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "searching-bumper",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T17:18:49.162260Z",
     "start_time": "2021-03-01T17:18:31.952228Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n",
      "193\n",
      "154\n",
      "200\n",
      "276\n",
      "222\n",
      "198\n",
      "190\n",
      "161\n",
      "177\n",
      "mean time steps:193.7\n"
     ]
    }
   ],
   "source": [
    "memory_2 = Memory()\n",
    "\n",
    "action_std=.1\n",
    "ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "ppo.policy_old.load_state_dict(torch.load(f'./PPO_continuous_{env_name}_best.pth'))\n",
    "# ppo.policy_old.load_state_dict(torch.load(f'./PPO_continuous_{env_name}.pth'))\n",
    "\n",
    "time_steps = []\n",
    "for k in range(10):\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(state, memory_2)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "#         state, reward, done, _ = env.step()\n",
    "\n",
    "        if done:\n",
    "            time_steps.append(t)\n",
    "            print(t)\n",
    "            break\n",
    "print(f\"mean time steps:{np.mean(time_steps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-grounds",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
