{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "complete-signature",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:35:45.787342Z",
     "start_time": "2021-03-07T04:35:43.097753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_device:0\n",
      "device_count:1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# https://discuss.pytorch.org/t/how-to-change-the-default-device-of-gpu-device-ids-0/1041/24\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=f\"{1}\"\n",
    "\n",
    "import torch\n",
    "print(f\"current_device:{torch.cuda.current_device()}\")\n",
    "print(f\"device_count:{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from ddpg import DDPGagent\n",
    "from ppo.PPO_continuous import PPO,Memory\n",
    "# from ddpg.utils import NormalizedEnv\n",
    "from flexipod_env import FlexipodEnv\n",
    "\n",
    "env = FlexipodEnv(dof = 12)\n",
    "# env = NormalizedEnv(env)\n",
    "\n",
    "############## Hyperparameters ##############\n",
    "# env_name = \"BipedalWalker-v3\"\n",
    "env_name = \"flexipod\"\n",
    "render = True\n",
    "solved_reward = 1500        # stop training if avg_reward > solved_reward\n",
    "log_interval = 80           # print avg reward in the interval\n",
    "# log_interval = 2           # print avg reward in the interval\n",
    "\n",
    "max_episodes = 20000        # max training episodes\n",
    "max_timesteps = 1500        # max timesteps in one episode\n",
    "\n",
    "# update_timestep = 4000      # update policy every n timesteps\n",
    "update_timestep = 3000      # update policy every n timesteps\n",
    "\n",
    "\n",
    "action_std = 0.5            # constant std for action distribution (Multivariate Normal)\n",
    "# action_std = 1.0          # constant std for action distribution (Multivariate Normal)\n",
    "K_epochs = 80               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr = 0.0002                 # parameters for Adam optimizer\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "random_seed = None\n",
    "#############################################\n",
    "# creating environment\n",
    "# env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "popular-princeton",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:35:46.185626Z",
     "start_time": "2021-03-07T04:35:45.856358Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/soft12dof_experiment_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chinese-officer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:25:42.621591Z",
     "start_time": "2021-03-07T04:25:42.609589Z"
    }
   },
   "outputs": [],
   "source": [
    "# env = FlexipodEnv(dof = 12)\n",
    "# self = env\n",
    "# msg_rec,_,_,_ = env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "addressed-profile",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T21:51:33.258963Z",
     "start_time": "2021-03-07T04:35:48.829906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002 (0.9, 0.999)\n",
      "Episode 80 \t Avg length: 110 \t Avg reward: 138\n",
      "Episode 160 \t Avg length: 139 \t Avg reward: 174\n",
      "Episode 240 \t Avg length: 157 \t Avg reward: 198\n",
      "Episode 320 \t Avg length: 167 \t Avg reward: 211\n",
      "Episode 400 \t Avg length: 187 \t Avg reward: 237\n",
      "Episode 480 \t Avg length: 207 \t Avg reward: 263\n",
      "Episode 560 \t Avg length: 250 \t Avg reward: 320\n",
      "Episode 640 \t Avg length: 272 \t Avg reward: 348\n",
      "Episode 720 \t Avg length: 319 \t Avg reward: 408\n",
      "Episode 800 \t Avg length: 305 \t Avg reward: 391\n",
      "Episode 880 \t Avg length: 313 \t Avg reward: 402\n",
      "Episode 960 \t Avg length: 339 \t Avg reward: 435\n",
      "Episode 1040 \t Avg length: 370 \t Avg reward: 474\n",
      "Episode 1120 \t Avg length: 353 \t Avg reward: 453\n",
      "Episode 1200 \t Avg length: 332 \t Avg reward: 426\n",
      "Episode 1280 \t Avg length: 321 \t Avg reward: 412\n",
      "Episode 1360 \t Avg length: 388 \t Avg reward: 499\n",
      "Episode 1440 \t Avg length: 415 \t Avg reward: 534\n",
      "Episode 1520 \t Avg length: 362 \t Avg reward: 465\n",
      "Episode 1600 \t Avg length: 421 \t Avg reward: 543\n",
      "Episode 1680 \t Avg length: 372 \t Avg reward: 480\n",
      "Episode 1760 \t Avg length: 393 \t Avg reward: 507\n",
      "Episode 1840 \t Avg length: 385 \t Avg reward: 497\n",
      "Episode 1920 \t Avg length: 353 \t Avg reward: 456\n",
      "Episode 2000 \t Avg length: 392 \t Avg reward: 507\n",
      "Episode 2080 \t Avg length: 380 \t Avg reward: 490\n",
      "Episode 2160 \t Avg length: 427 \t Avg reward: 547\n",
      "Episode 2240 \t Avg length: 474 \t Avg reward: 611\n",
      "Episode 2320 \t Avg length: 424 \t Avg reward: 546\n",
      "Episode 2400 \t Avg length: 514 \t Avg reward: 661\n",
      "Episode 2480 \t Avg length: 418 \t Avg reward: 538\n",
      "Episode 2560 \t Avg length: 421 \t Avg reward: 542\n",
      "Episode 2640 \t Avg length: 434 \t Avg reward: 559\n",
      "Episode 2720 \t Avg length: 405 \t Avg reward: 521\n",
      "Episode 2800 \t Avg length: 412 \t Avg reward: 530\n",
      "Episode 2880 \t Avg length: 304 \t Avg reward: 389\n",
      "Episode 2960 \t Avg length: 401 \t Avg reward: 515\n",
      "Episode 3040 \t Avg length: 438 \t Avg reward: 563\n",
      "Episode 3120 \t Avg length: 473 \t Avg reward: 611\n",
      "Episode 3200 \t Avg length: 425 \t Avg reward: 549\n",
      "Episode 3280 \t Avg length: 413 \t Avg reward: 534\n",
      "Episode 3360 \t Avg length: 400 \t Avg reward: 517\n",
      "Episode 3440 \t Avg length: 415 \t Avg reward: 537\n",
      "Episode 3520 \t Avg length: 405 \t Avg reward: 524\n",
      "Episode 3600 \t Avg length: 490 \t Avg reward: 635\n",
      "Episode 3680 \t Avg length: 554 \t Avg reward: 718\n",
      "Episode 3760 \t Avg length: 591 \t Avg reward: 766\n",
      "Episode 3840 \t Avg length: 449 \t Avg reward: 580\n",
      "Episode 3920 \t Avg length: 412 \t Avg reward: 532\n",
      "Episode 4000 \t Avg length: 341 \t Avg reward: 439\n",
      "Episode 4080 \t Avg length: 459 \t Avg reward: 594\n",
      "Episode 4160 \t Avg length: 430 \t Avg reward: 556\n",
      "load old best,avg_length=590.575\n",
      "Episode 4240 \t Avg length: 431 \t Avg reward: 557\n",
      "Episode 4320 \t Avg length: 489 \t Avg reward: 633\n",
      "Episode 4400 \t Avg length: 508 \t Avg reward: 658\n",
      "Episode 4480 \t Avg length: 556 \t Avg reward: 721\n",
      "load old best,avg_length=590.575\n",
      "Episode 4560 \t Avg length: 456 \t Avg reward: 589\n",
      "Episode 4640 \t Avg length: 543 \t Avg reward: 702\n",
      "Episode 4720 \t Avg length: 480 \t Avg reward: 620\n",
      "Episode 4800 \t Avg length: 481 \t Avg reward: 622\n",
      "Episode 4880 \t Avg length: 588 \t Avg reward: 763\n",
      "load old best,avg_length=590.575\n",
      "Episode 4960 \t Avg length: 546 \t Avg reward: 707\n",
      "Episode 5040 \t Avg length: 374 \t Avg reward: 483\n",
      "Episode 5120 \t Avg length: 465 \t Avg reward: 602\n",
      "Episode 5200 \t Avg length: 388 \t Avg reward: 503\n",
      "Episode 5280 \t Avg length: 483 \t Avg reward: 626\n",
      "Episode 5360 \t Avg length: 378 \t Avg reward: 489\n",
      "Episode 5440 \t Avg length: 405 \t Avg reward: 524\n",
      "Episode 5520 \t Avg length: 478 \t Avg reward: 619\n",
      "Episode 5600 \t Avg length: 419 \t Avg reward: 542\n",
      "load old best,avg_length=590.575\n",
      "Episode 5680 \t Avg length: 436 \t Avg reward: 563\n",
      "Episode 5760 \t Avg length: 481 \t Avg reward: 623\n",
      "Episode 5840 \t Avg length: 440 \t Avg reward: 569\n",
      "Episode 5920 \t Avg length: 555 \t Avg reward: 717\n",
      "Episode 6000 \t Avg length: 516 \t Avg reward: 666\n",
      "Episode 6080 \t Avg length: 531 \t Avg reward: 685\n",
      "Episode 6160 \t Avg length: 554 \t Avg reward: 717\n",
      "Episode 6240 \t Avg length: 428 \t Avg reward: 552\n",
      "Episode 6320 \t Avg length: 460 \t Avg reward: 594\n",
      "Episode 6400 \t Avg length: 398 \t Avg reward: 514\n",
      "Episode 6480 \t Avg length: 438 \t Avg reward: 567\n",
      "load old best,avg_length=590.575\n",
      "Episode 6560 \t Avg length: 395 \t Avg reward: 510\n",
      "Episode 6640 \t Avg length: 473 \t Avg reward: 612\n",
      "Episode 6720 \t Avg length: 504 \t Avg reward: 651\n",
      "load old best,avg_length=590.575\n",
      "Episode 6800 \t Avg length: 499 \t Avg reward: 644\n",
      "Episode 6880 \t Avg length: 654 \t Avg reward: 846\n",
      "Episode 6960 \t Avg length: 583 \t Avg reward: 754\n",
      "Episode 7040 \t Avg length: 620 \t Avg reward: 803\n",
      "Episode 7120 \t Avg length: 533 \t Avg reward: 691\n",
      "Episode 7200 \t Avg length: 515 \t Avg reward: 668\n",
      "Episode 7280 \t Avg length: 417 \t Avg reward: 539\n",
      "Episode 7360 \t Avg length: 523 \t Avg reward: 677\n",
      "Episode 7440 \t Avg length: 461 \t Avg reward: 594\n",
      "Episode 7520 \t Avg length: 425 \t Avg reward: 547\n",
      "Episode 7600 \t Avg length: 469 \t Avg reward: 605\n",
      "Episode 7680 \t Avg length: 528 \t Avg reward: 681\n",
      "Episode 7760 \t Avg length: 622 \t Avg reward: 804\n",
      "Episode 7840 \t Avg length: 476 \t Avg reward: 616\n",
      "load old best,avg_length=654.1125\n",
      "Episode 7920 \t Avg length: 448 \t Avg reward: 580\n",
      "Episode 8000 \t Avg length: 447 \t Avg reward: 578\n",
      "Episode 8080 \t Avg length: 582 \t Avg reward: 753\n",
      "Episode 8160 \t Avg length: 550 \t Avg reward: 712\n",
      "Episode 8240 \t Avg length: 573 \t Avg reward: 741\n",
      "Episode 8320 \t Avg length: 437 \t Avg reward: 566\n",
      "load old best,avg_length=654.1125\n",
      "Episode 8400 \t Avg length: 536 \t Avg reward: 694\n",
      "Episode 8480 \t Avg length: 534 \t Avg reward: 691\n",
      "Episode 8560 \t Avg length: 460 \t Avg reward: 593\n",
      "Episode 8640 \t Avg length: 470 \t Avg reward: 607\n",
      "Episode 8720 \t Avg length: 468 \t Avg reward: 604\n",
      "Episode 8800 \t Avg length: 500 \t Avg reward: 646\n",
      "Episode 8880 \t Avg length: 546 \t Avg reward: 706\n",
      "Episode 8960 \t Avg length: 384 \t Avg reward: 494\n",
      "Episode 9040 \t Avg length: 429 \t Avg reward: 552\n",
      "Episode 9120 \t Avg length: 461 \t Avg reward: 593\n",
      "Episode 9200 \t Avg length: 467 \t Avg reward: 601\n",
      "Episode 9280 \t Avg length: 499 \t Avg reward: 642\n",
      "Episode 9360 \t Avg length: 404 \t Avg reward: 520\n",
      "Episode 9440 \t Avg length: 403 \t Avg reward: 518\n",
      "Episode 9520 \t Avg length: 521 \t Avg reward: 671\n",
      "load old best,avg_length=654.1125\n",
      "Episode 9600 \t Avg length: 517 \t Avg reward: 665\n",
      "Episode 9680 \t Avg length: 466 \t Avg reward: 600\n",
      "Episode 9760 \t Avg length: 442 \t Avg reward: 567\n",
      "Episode 9840 \t Avg length: 473 \t Avg reward: 606\n",
      "Episode 9920 \t Avg length: 462 \t Avg reward: 590\n",
      "Episode 10000 \t Avg length: 461 \t Avg reward: 591\n",
      "Episode 10080 \t Avg length: 480 \t Avg reward: 616\n",
      "Episode 10160 \t Avg length: 444 \t Avg reward: 569\n",
      "Episode 10240 \t Avg length: 445 \t Avg reward: 571\n",
      "Episode 10320 \t Avg length: 475 \t Avg reward: 611\n",
      "Episode 10400 \t Avg length: 440 \t Avg reward: 565\n",
      "Episode 10480 \t Avg length: 460 \t Avg reward: 592\n",
      "Episode 10560 \t Avg length: 381 \t Avg reward: 491\n",
      "Episode 10640 \t Avg length: 394 \t Avg reward: 508\n",
      "Episode 10720 \t Avg length: 433 \t Avg reward: 559\n",
      "Episode 10800 \t Avg length: 441 \t Avg reward: 570\n",
      "Episode 10880 \t Avg length: 488 \t Avg reward: 630\n",
      "Episode 10960 \t Avg length: 445 \t Avg reward: 576\n",
      "Episode 11040 \t Avg length: 411 \t Avg reward: 531\n",
      "Episode 11120 \t Avg length: 413 \t Avg reward: 533\n",
      "Episode 11200 \t Avg length: 447 \t Avg reward: 577\n",
      "Episode 11280 \t Avg length: 389 \t Avg reward: 502\n",
      "load old best,avg_length=654.1125\n",
      "Episode 11360 \t Avg length: 401 \t Avg reward: 517\n",
      "Episode 11440 \t Avg length: 504 \t Avg reward: 651\n",
      "Episode 11520 \t Avg length: 447 \t Avg reward: 576\n",
      "load old best,avg_length=654.1125\n",
      "Episode 11600 \t Avg length: 513 \t Avg reward: 663\n",
      "Episode 11680 \t Avg length: 368 \t Avg reward: 475\n",
      "load old best,avg_length=654.1125\n",
      "Episode 11760 \t Avg length: 449 \t Avg reward: 582\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1c1f995176a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mtime_step\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Running policy_old:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\repo\\experimental_flexipod\\src\\python\\ppo\\PPO_continuous.py\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(self, state, memory)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_old\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\repo\\experimental_flexipod\\src\\python\\ppo\\PPO_continuous.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, memory)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[0mcov_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultivariateNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcov_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0maction_logprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\flexipod\\lib\\site-packages\\torch\\distributions\\multivariate_normal.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscale_tril\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mcovariance_matrix\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcovariance_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# precision_matrix is not None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_precision_to_scale_tril\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if random_seed:\n",
    "    print(\"Random Seed: {}\".format(random_seed))\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "memory = Memory()\n",
    "ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "print(lr,betas)\n",
    "\n",
    "max_avg_length = 0\n",
    "# checkpoint = ppo.load(f'./PPO_continuous_{env_name}_best.pth')\n",
    "# max_avg_length = checkpoint[\"avg_length\"]\n",
    "\n",
    "# logging variables\n",
    "running_reward = 0\n",
    "avg_length = 0\n",
    "# max_avg_length = 0\n",
    "time_step = 0\n",
    "\n",
    "# training loop\n",
    "for i_episode in range(1, max_episodes+1):\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        time_step +=1\n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(state, memory)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Saving reward and is_terminals:\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "\n",
    "        # update if its time\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo.update(memory)\n",
    "            memory.clear_memory()\n",
    "            time_step = 0\n",
    "        running_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    avg_length += t\n",
    "\n",
    "    # save every 500 episodes\n",
    "    if i_episode % 500 == 0:\n",
    "        ppo.save(f'./PPO_continuous_{env_name}.pth',avg_length=avg_length)\n",
    "\n",
    "    # logging\n",
    "    if i_episode % log_interval == 0:\n",
    "        avg_length = avg_length/log_interval\n",
    "        running_reward = running_reward/log_interval\n",
    "        writer.add_scalar(\"avg_length/train\", avg_length, i_episode)\n",
    "        writer.add_scalar(\"running_reward/train\", running_reward, i_episode)\n",
    "        \n",
    "        # stop training if avg_reward > solved_reward\n",
    "        if running_reward > (log_interval*solved_reward):\n",
    "            print(\"########## Solved! ##########\")\n",
    "            ppo.save(f'./PPO_continuous_solved_{env_name}.pth',avg_length=avg_length)\n",
    "            break\n",
    "            \n",
    "        if avg_length>max_avg_length:\n",
    "            max_avg_length = avg_length\n",
    "            ppo.save(f'./PPO_continuous_{env_name}_best.pth',avg_length=avg_length)\n",
    "        elif np.random.random()<0.1:# 50% chance \n",
    "            checkpoint = ppo.load(f'./PPO_continuous_{env_name}_best.pth')\n",
    "            print(f\"load old best,avg_length={checkpoint['avg_length']}\")# restart\n",
    "\n",
    "        print(f'Episode {i_episode} \\t Avg length: {avg_length:.0f} \\t Avg reward: {running_reward:.0f}')\n",
    "        running_reward = 0\n",
    "        avg_length = 0\n",
    "        \n",
    "env.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "external-block",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:35:20.267742Z",
     "start_time": "2021-03-07T04:35:20.252739Z"
    }
   },
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "self = env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "minimal-cemetery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:35:21.971519Z",
     "start_time": "2021-03-07T04:35:21.890500Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'com_pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5972f86d8c21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\repo\\experimental_flexipod\\src\\python\\flexipod_env.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"reset(): try #{k}:{e}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# failed at last time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"human\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\repo\\experimental_flexipod\\src\\python\\flexipod_env.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[0mmsg_rec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m                 \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_processRecMsg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_rec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrepeat_first\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m't'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\repo\\experimental_flexipod\\src\\python\\flexipod_env.py\u001b[0m in \u001b[0;36m_processRecMsg\u001b[1;34m(self, msg_rec, repeat_first)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0mactuation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmsg_rec_i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'actuation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;31m# com_vel = np.linalg.norm(msg_rec_i[self.ID['com_vel']])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m         \u001b[0mcom_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmsg_rec_i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'com_pos'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrepeat_first\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'com_pos'"
     ]
    }
   ],
   "source": [
    "self.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "genetic-appreciation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:30:21.380150Z",
     "start_time": "2021-03-07T04:30:21.362098Z"
    }
   },
   "outputs": [],
   "source": [
    "msg_rec = self.receive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "realistic-sperm",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T04:30:54.968932Z",
     "start_time": "2021-03-07T04:30:54.946937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14,\n",
       " 526.0750122070312,\n",
       " [1.5707980394363403,\n",
       "  -1.5707910060882568,\n",
       "  1.1583278179168701,\n",
       "  -1.1581804752349854,\n",
       "  1.5618208646774292,\n",
       "  0.00015313680341932923,\n",
       "  -1.5617340803146362,\n",
       "  0.00014471155009232461,\n",
       "  0.00017664824554231018,\n",
       "  -0.4020088016986847,\n",
       "  -0.00022034731227904558,\n",
       "  -0.4015836715698242],\n",
       " [1.2466489351936616e-07,\n",
       "  -6.653957029811863e-08,\n",
       "  -3.5676137486007065e-05,\n",
       "  3.467056012596004e-05,\n",
       "  7.091095994837815e-07,\n",
       "  1.28126725940092e-06,\n",
       "  -7.667101726838155e-07,\n",
       "  1.2046399433529587e-06,\n",
       "  -3.122644329778268e-07,\n",
       "  5.897355731576681e-05,\n",
       "  -1.964820768307618e-07,\n",
       "  6.117679731687531e-05],\n",
       " [-2.5745034690771718e-06,\n",
       "  -8.177499694284052e-06,\n",
       "  0.006205763202160597,\n",
       "  -0.006430943496525288,\n",
       "  0.013713488355278969,\n",
       "  -0.0002339880302315578,\n",
       "  -0.013846149668097496,\n",
       "  -0.00022111446014605463,\n",
       "  -0.0002698956523090601,\n",
       "  -0.009776351042091846,\n",
       "  0.0003366677847225219,\n",
       "  -0.01042591966688633],\n",
       " [0.1014338880777359,\n",
       "  -7.22313197911717e-05,\n",
       "  0.9948422908782959,\n",
       "  -0.005346452817320824,\n",
       "  0.9999855160713196,\n",
       "  0.0006177278119139373],\n",
       " [1.1195425940968562e-05, 0.0007120061782188714, -1.138506945608242e-06],\n",
       " [-0.002015327801927924, -4.237334906065371e-06, 0.00021220759663265198],\n",
       " [0.00024164457863662392, 1.0161375740835865e-08, -2.4794553610263392e-05],\n",
       " [0.02499666064977646, 0.00013498368207365274, 0.4568121135234833],\n",
       " [-5.110674101160839e-05,\n",
       "  0.0003894193796440959,\n",
       "  0.00018385893781669438,\n",
       "  0.00011342780635459349,\n",
       "  0.00024374559870921075,\n",
       "  0.00014285306679084897,\n",
       "  -6.338983803289011e-05,\n",
       "  -1.1889677807630505e-05,\n",
       "  -0.0001504153769928962,\n",
       "  0.00013766171468887478,\n",
       "  -4.438422547536902e-05,\n",
       "  1.1722572708094958e-06,\n",
       "  0.0008152617374435067,\n",
       "  0.00020514681818895042,\n",
       "  0.00026532638003118336,\n",
       "  -2.7685264285537414e-05,\n",
       "  -0.0003162870998494327,\n",
       "  9.533136471873149e-05,\n",
       "  0.0001269243803108111,\n",
       "  2.7029991542804055e-05,\n",
       "  0.0003927347424905747,\n",
       "  -9.066087659448385e-05,\n",
       "  7.033968722680584e-05,\n",
       "  -0.00026028163847513497,\n",
       "  -0.0001083435709006153,\n",
       "  -0.0002363819075981155,\n",
       "  0.0004682383150793612,\n",
       "  0.0003773694043047726,\n",
       "  0.00018726906273514032,\n",
       "  0.0002012171462411061,\n",
       "  -0.00011402182281017303,\n",
       "  8.575315587222576e-05,\n",
       "  -0.00030062979203648865,\n",
       "  0.0011087205493822694,\n",
       "  -0.0004825984942726791,\n",
       "  -7.45209472370334e-05,\n",
       "  0.00017933938943315297,\n",
       "  -0.0004326628113631159,\n",
       "  6.152425339678302e-05,\n",
       "  0.0009738068911246955,\n",
       "  0.002422471297904849,\n",
       "  0.0019482310162857175,\n",
       "  -0.0011731479316949844,\n",
       "  0.0002549861674197018,\n",
       "  0.00020601011055987328,\n",
       "  2.226120159320999e-05,\n",
       "  -3.778687823796645e-05,\n",
       "  -7.932916923891753e-05,\n",
       "  4.187467311567161e-06,\n",
       "  2.286445305799134e-05,\n",
       "  -0.00011207293573534116,\n",
       "  8.295042789541185e-06,\n",
       "  2.5465093131060712e-05,\n",
       "  -0.00010101532825501636,\n",
       "  -3.7530469398916466e-06,\n",
       "  -0.0005832367460243404,\n",
       "  -0.0009540427126921713,\n",
       "  8.729502587812021e-05,\n",
       "  -0.00015657406765967607,\n",
       "  -0.00017708187806420028,\n",
       "  -0.0006462167366407812,\n",
       "  -3.529757350406726e-06,\n",
       "  8.12728758319281e-05,\n",
       "  0.000176250483491458,\n",
       "  0.0003031438682228327,\n",
       "  -0.00020305559155531228,\n",
       "  -5.259845784166828e-05,\n",
       "  0.0007898516487330198,\n",
       "  0.00021704322716686875,\n",
       "  -4.875659942626953e-05,\n",
       "  -2.734431291173678e-05,\n",
       "  -5.1897419325541705e-05,\n",
       "  -0.00010921896318905056,\n",
       "  -5.790211434941739e-05,\n",
       "  7.25982608855702e-05,\n",
       "  -0.00023820159549359232,\n",
       "  -9.361968841403723e-05,\n",
       "  -0.0004375071730464697,\n",
       "  -0.0016788572538644075,\n",
       "  -0.00021320651285350323,\n",
       "  7.680762064410374e-05,\n",
       "  0.0005597592098638415,\n",
       "  0.0003109773388132453,\n",
       "  -6.166484672576189e-05,\n",
       "  0.0007504788227379322,\n",
       "  -0.0016150124138221145,\n",
       "  -0.00010930086864391342,\n",
       "  -0.000331420247675851,\n",
       "  -8.773548324825242e-05,\n",
       "  0.002398255979642272,\n",
       "  0.0012510230299085379,\n",
       "  -8.92986572580412e-05,\n",
       "  0.0021018655970692635,\n",
       "  -0.0015827787574380636,\n",
       "  -0.014808171428740025,\n",
       "  0.00019812595564872026,\n",
       "  -0.00032901347731240094,\n",
       "  -3.35279582941439e-05,\n",
       "  -0.0008624286274425685,\n",
       "  0.0005278017488308251,\n",
       "  5.253131348581519e-06,\n",
       "  -0.000356591132003814,\n",
       "  9.006700565805659e-05,\n",
       "  -0.00017414201283827424,\n",
       "  0.0010979792568832636,\n",
       "  0.0006506159552372992,\n",
       "  -0.00034720246912911534,\n",
       "  -0.010197927244007587,\n",
       "  0.0008807505946606398,\n",
       "  -0.0007761750603094697,\n",
       "  0.0012363855494186282,\n",
       "  -0.0008709784597158432,\n",
       "  -0.00020096359367016703,\n",
       "  0.00016549626889172941,\n",
       "  0.0017703104531392455,\n",
       "  0.0009132949053309858,\n",
       "  0.0011664527701213956,\n",
       "  0.004283309448510408,\n",
       "  -0.01113821193575859,\n",
       "  6.968130765017122e-05,\n",
       "  -0.00035570544423535466,\n",
       "  -0.000127240433357656,\n",
       "  -0.0002445351565256715,\n",
       "  -0.00026608092593960464,\n",
       "  0.00011969320621574298,\n",
       "  -0.0006519720191136003,\n",
       "  0.00012831177446059883,\n",
       "  0.0012859748676419258]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_rec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "civilian-spirituality",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T13:44:56.890561Z",
     "start_time": "2021-02-27T13:44:56.882987Z"
    }
   },
   "outputs": [],
   "source": [
    "# writer.add_scalar(\"baseline_length/train\", 200, 0)\n",
    "# writer.add_scalar(\"baseline_length/train\", 200, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adopted-notice",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-07T21:56:48.250444Z",
     "start_time": "2021-03-07T21:54:03.143091Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time steps:364 ,duration: 4.58[s], fps:80\n",
      "time steps:437 ,duration: 5.50[s], fps:79\n",
      "time steps:633 ,duration: 7.84[s], fps:81\n",
      "time steps:891 ,duration: 11.02[s], fps:81\n",
      "time steps:654 ,duration: 8.05[s], fps:81\n",
      "time steps:1111 ,duration: 13.87[s], fps:80\n",
      "time steps:1279 ,duration: 15.86[s], fps:81\n",
      "time steps:295 ,duration: 3.81[s], fps:77\n",
      "time steps:252 ,duration: 3.15[s], fps:80\n",
      "time steps:979 ,duration: 12.14[s], fps:81\n",
      "time steps:1423 ,duration: 17.66[s], fps:81\n",
      "time steps:487 ,duration: 6.20[s], fps:79\n",
      "time steps:416 ,duration: 5.21[s], fps:80\n",
      "time steps:310 ,duration: 3.84[s], fps:81\n",
      "time steps:687 ,duration: 8.54[s], fps:80\n",
      "time steps:439 ,duration: 5.46[s], fps:80\n",
      "time steps:629 ,duration: 7.86[s], fps:80\n",
      "time steps:282 ,duration: 3.49[s], fps:81\n",
      "time steps:197 ,duration: 2.47[s], fps:80\n",
      "mean time steps:619.2105263157895 ,duration: 7.713610197368421[s]\n"
     ]
    }
   ],
   "source": [
    "memory_2 = Memory()\n",
    "\n",
    "# action_std=.5\n",
    "ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "ppo.load(f'./PPO_continuous_{env_name}_best.pth')\n",
    "\n",
    "time_steps = []\n",
    "durations = []\n",
    "for k in range(20):\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(state, memory_2)\n",
    "#         action = ppo.select_action(state, memory)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#         state, reward, done, _ = env.step()\n",
    "\n",
    "        if done:\n",
    "            time_steps.append(t)\n",
    "            episode_duration= info['episode_duration']\n",
    "            durations.append(episode_duration)\n",
    "            print(f\"time steps:{t} ,duration: {episode_duration:.2f}[s], fps:{t/episode_duration:.0f}\")\n",
    "            break\n",
    "            \n",
    "print(f\"mean time steps:{np.mean(time_steps)} ,duration: {np.mean(durations)}[s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-reporter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
