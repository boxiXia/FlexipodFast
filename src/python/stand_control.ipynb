{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:50:10.588347Z",
     "start_time": "2021-02-10T15:50:09.071439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_device:0\n",
      "device_count:1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# https://discuss.pytorch.org/t/how-to-change-the-default-device-of-gpu-device-ids-0/1041/24\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=f\"{1}\"\n",
    "\n",
    "import torch\n",
    "print(f\"current_device:{torch.cuda.current_device()}\")\n",
    "print(f\"device_count:{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T05:42:21.189976Z",
     "start_time": "2021-02-01T05:42:11.552820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02573004 -0.01465614  0.03378908  0.01222349]\n",
      "[ 0.02543692  0.17996535  0.03403355 -0.26961   ]\n",
      "[ 0.02903623  0.37458551  0.02864135 -0.55136726]\n",
      "[ 0.03652794  0.56929374  0.017614   -0.83489041]\n",
      "[ 0.04791381  0.37393564  0.0009162  -0.53672047]\n",
      "[ 0.05539253  0.17880082 -0.00981821 -0.243749  ]\n",
      "[ 0.05896854 -0.01617953 -0.01469319  0.04582091]\n",
      "[ 0.05864495  0.17915    -0.01377678 -0.2514614 ]\n",
      "[ 0.06222795  0.37446594 -0.018806   -0.54845775]\n",
      "[ 0.06971727  0.17961315 -0.02977516 -0.2617589 ]\n",
      "[ 0.07330953 -0.01507139 -0.03501034  0.02138588]\n",
      "[ 0.07300811 -0.20967423 -0.03458262  0.30282032]\n",
      "[ 0.06881462 -0.40428668 -0.02852621  0.58439919]\n",
      "[ 0.06072889 -0.59899766 -0.01683823  0.86796126]\n",
      "[ 4.87489349e-02 -7.93886500e-01  5.20996220e-04  1.15530288e+00]\n",
      "[ 0.0328712  -0.59877135  0.02362705  0.86278336]\n",
      "[ 0.02089578 -0.79420688  0.04088272  1.16280055]\n",
      "[ 0.00501164 -0.98983667  0.06413873  1.46801621]\n",
      "[-0.01478509 -1.18568231  0.09349906  1.78002449]\n",
      "[-0.03849874 -0.99172857  0.12909955  1.51781331]\n",
      "[-0.05833331 -0.79838272  0.15945581  1.26805793]\n",
      "[-0.07430097 -0.60561555  0.18481697  1.02925698]\n",
      "[-0.08641328 -0.8026513   0.20540211  1.37380469]\n",
      "Episode finished after 23 timesteps\n",
      "[ 0.04664529 -0.04929314  0.02026515  0.01735493]\n",
      "[ 0.04565943  0.14553241  0.02061224 -0.26886585]\n",
      "[ 0.04857008 -0.04987754  0.01523493  0.03024638]\n",
      "[ 0.04757253 -0.24521462  0.01583986  0.32769689]\n",
      "[ 0.04266824 -0.05032172  0.02239379  0.04005093]\n",
      "[ 0.0416618  -0.24575752  0.02319481  0.33971435]\n",
      "[ 0.03674665 -0.44120169  0.0299891   0.63962042]\n",
      "[ 0.02792262 -0.24651041  0.04278151  0.35653028]\n",
      "[ 0.02299241 -0.44221367  0.04991211  0.66239049]\n",
      "[ 0.01414814 -0.24782039  0.06315992  0.38583178]\n",
      "[ 0.00919173 -0.05364927  0.07087656  0.11371252]\n",
      "[ 0.00811874  0.1403893   0.07315081 -0.15579431]\n",
      "[ 0.01092653  0.3343918   0.07003492 -0.42453267]\n",
      "[ 0.01761437  0.13835128  0.06154427 -0.11061931]\n",
      "[ 0.02038139 -0.05759611  0.05933188  0.2008279 ]\n",
      "[ 0.01922947 -0.25351423  0.06334844  0.51162131]\n",
      "[ 0.01415918 -0.44946863  0.07358087  0.82357532]\n",
      "[ 0.00516981 -0.25542622  0.09005237  0.55491254]\n",
      "[ 6.12870221e-05 -6.16764204e-02  1.01150624e-01  2.91905068e-01]\n",
      "[-0.00117224  0.13186875  0.10698873  0.03275883]\n",
      "[ 0.00146513 -0.06461185  0.1076439   0.35718927]\n",
      "[0.0001729  0.12882823 0.11478769 0.10029483]\n",
      "[ 0.00274946  0.32213395  0.11679358 -0.15408205]\n",
      "[0.00919214 0.12555022 0.11371194 0.17304397]\n",
      "[ 0.01170314 -0.07100003  0.11717282  0.4993246 ]\n",
      "[0.01028314 0.12229197 0.12715931 0.24574471]\n",
      "[ 0.01272898 -0.07439511  0.13207421  0.57567746]\n",
      "[0.01124108 0.11865227 0.14358776 0.32734534]\n",
      "[0.01361413 0.31146922 0.15013466 0.08316623]\n",
      "[ 0.01984351  0.50415582  0.15179799 -0.1586384 ]\n",
      "[ 0.02992663  0.69681542  0.14862522 -0.39984724]\n",
      "[ 0.04386294  0.4999322   0.14062828 -0.06424307]\n",
      "[ 0.05386158  0.69278715  0.13934342 -0.30946117]\n",
      "[0.06771732 0.49598336 0.13315419 0.02371774]\n",
      "[ 0.07763699  0.68896936  0.13362855 -0.22416706]\n",
      "[0.09141638 0.49221579 0.12914521 0.10750217]\n",
      "[ 0.10126069  0.68527329  0.13129525 -0.14180927]\n",
      "[0.11496616 0.48853924 0.12845906 0.18924192]\n",
      "[0.12473694 0.29183588 0.1322439  0.51952903]\n",
      "[0.13057366 0.09512437 0.14263448 0.8507864 ]\n",
      "[0.13247615 0.2880435  0.15965021 0.60613884]\n",
      "[0.13823702 0.48061542 0.17177299 0.36769241]\n",
      "[0.14784933 0.67293348 0.17912684 0.13371405]\n",
      "[0.161308   0.47575829 0.18180112 0.47712663]\n",
      "[0.17082316 0.66791057 0.19134365 0.2468039 ]\n",
      "[0.18418137 0.85985819 0.19627973 0.02004955]\n",
      "[ 0.20137854  1.05170313  0.19668072 -0.20485599]\n",
      "[0.2224126  0.85439199 0.1925836  0.14285671]\n",
      "[0.23950044 0.65710851 0.19544073 0.48958141]\n",
      "[0.25264261 0.84901423 0.20523236 0.2642975 ]\n",
      "Episode finished after 50 timesteps\n",
      "[-0.00810704  0.03836157 -0.02280035  0.03233448]\n",
      "[-0.00733981 -0.15642612 -0.02215366  0.31773735]\n",
      "[-0.01046833  0.03900424 -0.01579891  0.01815109]\n",
      "[-0.00968825  0.23434916 -0.01543589 -0.27947441]\n",
      "[-0.00500127  0.42968787 -0.02102538 -0.57698553]\n",
      "[ 0.00359249  0.23486684 -0.03256509 -0.29099958]\n",
      "[ 0.00828983  0.040224   -0.03838508 -0.00876256]\n",
      "[ 0.00909431 -0.15432704 -0.03856033  0.27156658]\n",
      "[ 0.00600777  0.04132333 -0.033129   -0.03302474]\n",
      "[ 0.00683423 -0.15330826 -0.03378949  0.24902431]\n",
      "[ 0.00376807  0.04227953 -0.02880901 -0.05412214]\n",
      "[ 0.00461366 -0.15241774 -0.02989145  0.22933392]\n",
      "[ 0.0015653   0.04311833 -0.02530477 -0.07262594]\n",
      "[ 0.00242767 -0.15163188 -0.02675729  0.21196703]\n",
      "[-0.00060497 -0.34636125 -0.02251795  0.49609083]\n",
      "[-0.00753219 -0.54115855 -0.01259613  0.78159298]\n",
      "[-0.01835536 -0.34586575  0.00303573  0.48497385]\n",
      "[-0.02527268 -0.5410304   0.0127352   0.77861199]\n",
      "[-0.03609329 -0.73632513  0.02830744  1.07527437]\n",
      "[-0.05081979 -0.54158842  0.04981293  0.79160761]\n",
      "[-0.06165156 -0.3471845   0.06564508  0.51500239]\n",
      "[-0.06859525 -0.54316652  0.07594513  0.82762803]\n",
      "[-0.07945858 -0.34916056  0.09249769  0.55976432]\n",
      "[-0.08644179 -0.54545074  0.10369298  0.88009648]\n",
      "[-0.0973508  -0.74181701  0.12129491  1.2034949 ]\n",
      "[-0.11218714 -0.93828005  0.1453648   1.53159761]\n",
      "[-0.13095274 -1.13482424  0.17599676  1.8658914 ]\n",
      "Episode finished after 27 timesteps\n",
      "[ 0.03164056 -0.02379998  0.01967341 -0.04273202]\n",
      "[ 0.03116456 -0.21919843  0.01881877  0.25609255]\n",
      "[ 0.0267806  -0.02435015  0.02394062 -0.03059573]\n",
      "[ 0.02629359  0.17042044  0.0233287  -0.31563003]\n",
      "[ 0.029702    0.36520246  0.0170161  -0.60086554]\n",
      "[ 0.03700605  0.56008229  0.00499879 -0.88814047]\n",
      "[ 0.0482077   0.36489285 -0.01276402 -0.59389033]\n",
      "[ 0.05550555  0.56019112 -0.02464182 -0.89056639]\n",
      "[ 0.06670938  0.36541201 -0.04245315 -0.60573039]\n",
      "[ 0.07401762  0.56110109 -0.05456776 -0.91147701]\n",
      "[ 0.08523964  0.36675826 -0.0727973  -0.63643163]\n",
      "[ 0.0925748   0.56281591 -0.08552593 -0.95112272]\n",
      "[ 0.10383112  0.75897833 -0.10454839 -1.26940484]\n",
      "[ 0.11901069  0.56533511 -0.12993648 -1.01120726]\n",
      "[ 0.13031739  0.37216377 -0.15016063 -0.76198679]\n",
      "[ 0.13776066  0.56899996 -0.16540037 -1.0979001 ]\n",
      "[ 0.14914066  0.37639548 -0.18735837 -0.86134253]\n",
      "[ 0.15666857  0.18425104 -0.20458522 -0.6329314 ]\n",
      "Episode finished after 18 timesteps\n",
      "[-0.01451382 -0.03013666 -0.04975721 -0.01953553]\n",
      "[-0.01511655  0.16566225 -0.05014792 -0.32749298]\n",
      "[-0.0118033   0.36146094 -0.05669778 -0.63555912]\n",
      "[-0.00457409  0.1671737  -0.06940896 -0.36125677]\n",
      "[-0.00123061  0.36321006 -0.0766341  -0.67499312]\n",
      "[ 0.00603359  0.55930863 -0.09013396 -0.99078604]\n",
      "[ 0.01721976  0.75551372 -0.10994968 -1.31036251]\n",
      "[ 0.03233004  0.95184305 -0.13615693 -1.63533836]\n",
      "[ 0.0513669   1.14827468 -0.1688637  -1.9671654 ]\n",
      "[ 0.07433239  1.34473209 -0.20820701 -2.30707033]\n",
      "Episode finished after 10 timesteps\n",
      "[-0.00370657 -0.00836537  0.0426698  -0.0053054 ]\n",
      "[-0.00387388 -0.20407247  0.04256369  0.30052916]\n",
      "[-0.00795533 -0.00958222  0.04857428  0.02156797]\n",
      "[-0.00814697 -0.20536591  0.04900564  0.32917233]\n",
      "[-0.01225429 -0.0109746   0.05558908  0.05233731]\n",
      "[-0.01247378  0.18330803  0.05663583 -0.2223017 ]\n",
      "[-0.00880762  0.37757666  0.0521898  -0.49659538]\n",
      "[-0.00125609  0.18175913  0.04225789 -0.18793218]\n",
      "[ 0.00237909  0.37625182  0.03849925 -0.46699065]\n",
      "[ 0.00990413  0.57080928  0.02915943 -0.7472946 ]\n",
      "[ 0.02132031  0.76551705  0.01421354 -1.03066045]\n",
      "[ 0.03663066  0.96044704 -0.00639967 -1.31884725]\n",
      "[ 0.0558396   0.76540659 -0.03277661 -1.02817406]\n",
      "[ 0.07114773  0.9609491  -0.0533401  -1.33096489]\n",
      "[ 0.09036671  1.15670172 -0.07995939 -1.63985076]\n",
      "[ 0.11350074  0.96260268 -0.11275641 -1.37311546]\n",
      "[ 0.1327528   0.76905647 -0.14021872 -1.11771993]\n",
      "[ 0.14813393  0.9657117  -0.16257312 -1.450897  ]\n",
      "[ 0.16744816  0.77291758 -0.19159106 -1.2131053 ]\n",
      "Episode finished after 19 timesteps\n",
      "[ 0.0338412  -0.03835922  0.04515787  0.01244359]\n",
      "[ 0.03307401  0.156087    0.04540674 -0.26565649]\n",
      "[ 0.03619575  0.35053246  0.04009361 -0.54367906]\n",
      "[ 0.0432064   0.15487068  0.02922003 -0.2386379 ]\n",
      "[ 0.04630382  0.34956328  0.02444727 -0.52196266]\n",
      "[ 0.05329508  0.1541059   0.01400802 -0.22167742]\n",
      "[ 0.0563772   0.34902485  0.00957447 -0.50990895]\n",
      "[ 0.0633577   0.15376934 -0.00062371 -0.21422425]\n",
      "[ 0.06643308  0.3489002  -0.0049082  -0.50710385]\n",
      "[ 0.07341109  0.54409097 -0.01505027 -0.80132948]\n",
      "[ 0.08429291  0.34917863 -0.03107686 -0.5134187 ]\n",
      "[ 0.09127648  0.15450784 -0.04134524 -0.23068863]\n",
      "[ 0.09436664 -0.03999967 -0.04595901  0.04867125]\n",
      "[ 0.09356664 -0.2344335  -0.04498558  0.32650685]\n",
      "[ 0.08887797 -0.03870091 -0.03845545  0.01998368]\n",
      "[ 0.08810396  0.15695083 -0.03805577 -0.28458007]\n",
      "[ 0.09124297 -0.03760827 -0.04374737 -0.00413842]\n",
      "[ 0.09049081  0.15811289 -0.04383014 -0.31029689]\n",
      "[ 0.09365306 -0.03635807 -0.05003608 -0.03175253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0929259  -0.2307281  -0.05067113  0.24473315]\n",
      "[ 0.08831134 -0.03492041 -0.04577647 -0.06349235]\n",
      "[ 0.08761293  0.16082695 -0.04704632 -0.37025939]\n",
      "[ 0.09082947  0.35658465 -0.0544515  -0.67739755]\n",
      "[ 0.09796117  0.16225983 -0.06799945 -0.40234316]\n",
      "[ 0.10120636  0.35827704 -0.07604632 -0.7156659 ]\n",
      "[ 0.1083719   0.16428547 -0.09035964 -0.44785561]\n",
      "[ 0.11165761  0.36056167 -0.09931675 -0.7675984 ]\n",
      "[ 0.11886885  0.55690038 -0.11466872 -1.08980632]\n",
      "[ 0.13000685  0.36346129 -0.13646484 -0.835192  ]\n",
      "[ 0.13727608  0.56015719 -0.15316868 -1.16748912]\n",
      "[ 0.14847922  0.36732326 -0.17651846 -0.92648033]\n",
      "[ 0.15582569  0.17496755 -0.19504807 -0.69405761]\n",
      "[ 0.15932504 -0.01699101 -0.20892922 -0.46856077]\n",
      "Episode finished after 33 timesteps\n",
      "[0.03715303 0.00133264 0.01379078 0.02388248]\n",
      "[ 0.03717969 -0.19398434  0.01426843  0.32088446]\n",
      "[0.0333     0.00093154 0.02068612 0.03273518]\n",
      "[ 0.03331863  0.19575083  0.02134083 -0.25334995]\n",
      "[ 0.03723365  0.39056166  0.01627383 -0.53922593]\n",
      "[ 0.04504488  0.19521476  0.00548931 -0.24146008]\n",
      "[ 0.04894918  0.39025787  0.00066011 -0.53240649]\n",
      "[ 0.05675433  0.19512664 -0.00998802 -0.23951564]\n",
      "[ 0.06065687  0.00014879 -0.01477834  0.05000012]\n",
      "[ 0.06065984 -0.19475817 -0.01377833  0.33798396]\n",
      "[ 0.05676468 -0.38968138 -0.00701865  0.62629031]\n",
      "[ 0.04897105 -0.58470466  0.00550715  0.91675456]\n",
      "[ 0.03727696 -0.77990064  0.02384224  1.21116319]\n",
      "[ 0.02167895 -0.97532215  0.04806551  1.5112211 ]\n",
      "[ 0.0021725  -1.17099226  0.07828993  1.81851311]\n",
      "[-0.02124734 -1.36689241  0.11466019  2.13445698]\n",
      "[-0.04858519 -1.56294808  0.15734933  2.46024573]\n",
      "[-0.07984415 -1.75901197  0.20655425  2.79677839]\n",
      "Episode finished after 18 timesteps\n",
      "[-0.04481234  0.01563693 -0.00238662  0.04653312]\n",
      "[-0.0444996   0.21079302 -0.00145596 -0.24690185]\n",
      "[-0.04028374  0.40593574 -0.00639399 -0.54004366]\n",
      "[-0.03216502  0.21090425 -0.01719487 -0.24938222]\n",
      "[-0.02794694  0.40626748 -0.02218251 -0.5474388 ]\n",
      "[-0.01982159  0.60169393 -0.03313129 -0.84702748]\n",
      "[-0.00778771  0.40703923 -0.05007184 -0.56494448]\n",
      "[ 3.53075721e-04  6.02826612e-01 -6.13707248e-02 -8.72972441e-01]\n",
      "[ 0.01240961  0.40859046 -0.07883017 -0.60019838]\n",
      "[ 0.02058142  0.6047216  -0.09083414 -0.91663355]\n",
      "[ 0.03267585  0.8009466  -0.10916681 -1.23642614]\n",
      "[ 0.04869478  0.60738347 -0.13389534 -0.97984114]\n",
      "[ 0.06084245  0.41428566 -0.15349216 -0.73203465]\n",
      "[ 0.06912816  0.22158027 -0.16813285 -0.49132467]\n",
      "[ 0.07355977  0.02917923 -0.17795934 -0.25599119]\n",
      "[ 0.07414335 -0.1630147  -0.18307917 -0.0242976 ]\n",
      "[ 0.07088306 -0.35510355 -0.18356512  0.20549522]\n",
      "[ 0.06378099 -0.15789606 -0.17945522 -0.13901171]\n",
      "[ 0.06062307  0.03928161 -0.18223545 -0.48250563]\n",
      "[ 0.0614087  -0.15286369 -0.19188556 -0.25234143]\n",
      "[ 0.05835143  0.04440629 -0.19693239 -0.59888428]\n",
      "[ 0.05923955 -0.14749472 -0.20891008 -0.37412113]\n",
      "Episode finished after 22 timesteps\n",
      "[ 0.03746189  0.02558001  0.04448156 -0.03954134]\n",
      "[ 0.03797349  0.22003681  0.04369073 -0.31786475]\n",
      "[ 0.04237422  0.02432069  0.03733343 -0.01172973]\n",
      "[ 0.04286064 -0.17131623  0.03709884  0.29249465]\n",
      "[0.03943431 0.02325766 0.04294873 0.01173918]\n",
      "[ 0.03989947  0.2177382   0.04318352 -0.26708956]\n",
      "[ 0.04425423  0.41221809  0.03784173 -0.54584543]\n",
      "[ 0.05249859  0.21658543  0.02692482 -0.24148372]\n",
      "[ 0.0568303   0.41131264  0.02209514 -0.52555371]\n",
      "[ 0.06505655  0.60611681  0.01158407 -0.81119321]\n",
      "[ 0.07717889  0.41083809 -0.0046398  -0.51488915]\n",
      "[ 0.08539565  0.60602508 -0.01493758 -0.80903057]\n",
      "[ 0.09751615  0.41111097 -0.03111819 -0.5210835 ]\n",
      "[ 0.10573837  0.60665682 -0.04153986 -0.82340755]\n",
      "[ 0.11787151  0.80232167 -0.05800801 -1.12886084]\n",
      "[ 0.13391794  0.99815343 -0.08058523 -1.4391592 ]\n",
      "[ 0.15388101  1.1941705  -0.10936841 -1.75589704]\n",
      "[ 0.17776442  1.00044513 -0.14448635 -1.49913543]\n",
      "[ 0.19777332  0.80734383 -0.17446906 -1.25483299]\n",
      "[ 0.2139202   1.00421673 -0.19956572 -1.59669329]\n",
      "Episode finished after 20 timesteps\n",
      "[ 0.00722703 -0.01233438  0.03743747  0.01438135]\n",
      "[ 0.00698034  0.18223123  0.03772509 -0.26625852]\n",
      "[ 0.01062496  0.37679502  0.03239992 -0.546808  ]\n",
      "[ 0.01816086  0.18123319  0.02146376 -0.24409532]\n",
      "[ 0.02178553 -0.01418865  0.01658186  0.05527979]\n",
      "[ 0.02150176  0.18069166  0.01768745 -0.23212565]\n",
      "[ 0.02511559  0.37555647  0.01304494 -0.51917731]\n",
      "[ 0.03262672  0.57049237  0.00266139 -0.80772116]\n",
      "[ 0.04403657  0.37533404 -0.01349303 -0.51420226]\n",
      "[ 0.05154325  0.5706434  -0.02377708 -0.81110646]\n",
      "[ 0.06295611  0.37585511 -0.03999921 -0.52599642]\n",
      "[ 0.07047322  0.18131816 -0.05051913 -0.24618104]\n",
      "[ 0.07409958 -0.01304722 -0.05544275  0.03014877]\n",
      "[ 0.07383864 -0.20733207 -0.05483978  0.30483642]\n",
      "[ 0.06969199 -0.01147323 -0.04874305 -0.00462498]\n",
      "[ 0.06946253  0.18431265 -0.04883555 -0.31227978]\n",
      "[ 0.07314878 -0.01008079 -0.05508115 -0.03538921]\n",
      "[ 0.07294717  0.185786   -0.05578893 -0.34492949]\n",
      "[ 0.07666289  0.38165536 -0.06268752 -0.65466986]\n",
      "[ 0.08429599  0.57759152 -0.07578092 -0.96641489]\n",
      "[ 0.09584782  0.38356462 -0.09510921 -0.69846808]\n",
      "[ 0.10351912  0.18988107 -0.10907858 -0.43717575]\n",
      "[ 0.10731674 -0.00354149 -0.11782209 -0.18077178]\n",
      "[ 0.10724591  0.19305209 -0.12143753 -0.50817728]\n",
      "[ 1.11106950e-01 -1.68385143e-04 -1.31601073e-01 -2.56095963e-01]\n",
      "[ 0.11110358 -0.1931899  -0.13672299 -0.00764638]\n",
      "[ 0.10723978 -0.38611316 -0.13687592  0.23896651]\n",
      "[ 0.09951752 -0.57904131 -0.13209659  0.48553609]\n",
      "[ 0.08793669 -0.38232686 -0.12238587  0.15431156]\n",
      "[ 0.08029016 -0.18568447 -0.11929964 -0.17433745]\n",
      "[ 0.07657647 -0.37891493 -0.12278639  0.07845713]\n",
      "[ 0.06899817 -0.5720823  -0.12121724  0.33001816]\n",
      "[ 0.05755652 -0.76528894 -0.11461688  0.58215088]\n",
      "[ 0.04225075 -0.56876347 -0.10297386  0.25567325]\n",
      "[ 0.03087548 -0.76227598 -0.0978604   0.51418359]\n",
      "[ 0.01562996 -0.95589338 -0.08757673  0.77449508]\n",
      "[-0.00348791 -1.14970846 -0.07208682  1.03838888]\n",
      "[-0.02648208 -1.34380225 -0.05131905  1.30759826]\n",
      "[-0.05335813 -1.53823763 -0.02516708  1.58378619]\n",
      "[-0.08412288 -1.73305136  0.00650864  1.8685159 ]\n",
      "[-0.11878391 -1.92824388  0.04387896  2.163212  ]\n",
      "[-0.15734878 -1.73357755  0.0871432   1.88438985]\n",
      "[-0.19202033 -1.92953283  0.124831    2.2027951 ]\n",
      "[-0.23061099 -1.73581292  0.1688869   1.9510814 ]\n",
      "[-0.26532725 -1.54284194  0.20790853  1.71516022]\n",
      "Episode finished after 45 timesteps\n",
      "[-0.0330163  -0.0448521  -0.04026901 -0.00457747]\n",
      "[-0.03391334  0.15082354 -0.04036056 -0.30968887]\n",
      "[-0.03089687 -0.04370081 -0.04655434 -0.03000275]\n",
      "[-0.03177088 -0.2381253  -0.0471544   0.24763598]\n",
      "[-0.03653339 -0.04236273 -0.04220168 -0.05953972]\n",
      "[-0.03738064 -0.23685498 -0.04339247  0.21953529]\n",
      "[-0.04211774 -0.43133067 -0.03900176  0.49822084]\n",
      "[-0.05074436 -0.62588164 -0.02903735  0.77836177]\n",
      "[-0.06326199 -0.43037271 -0.01347011  0.47668621]\n",
      "[-0.07186945 -0.23506318 -0.00393639  0.1797884 ]\n",
      "[-7.65707086e-02 -4.30128579e-01 -3.40620271e-04  4.71226937e-01]\n",
      "[-0.08517328 -0.23500182  0.00908392  0.17843667]\n",
      "[-0.08987332 -0.43025258  0.01265265  0.47397136]\n",
      "[-0.09847837 -0.62555091  0.02213208  0.77061519]\n",
      "[-0.11098939 -0.43074042  0.03754438  0.48497732]\n",
      "[-0.11960419 -0.23616785  0.04724393  0.20435959]\n",
      "[-0.12432755 -0.04175224  0.05133112 -0.07305388]\n",
      "[-0.1251626  -0.23757107  0.04987004  0.23537221]\n",
      "[-0.12991402 -0.0431958   0.05457749 -0.0411725 ]\n",
      "[-0.13077793  0.15110277  0.05375404 -0.31614858]\n",
      "[-0.12775588 -0.044742    0.04743107 -0.00700966]\n",
      "[-0.12865072 -0.24051096  0.04729087  0.30025303]\n",
      "[-0.13346094 -0.43627395  0.05329593  0.60746755]\n",
      "[-0.14218642 -0.24193607  0.06544528  0.33203623]\n",
      "[-0.14702514 -0.43792555  0.07208601  0.64461828]\n",
      "[-0.15578365 -0.63397413  0.08497837  0.95910236]\n",
      "[-0.16846313 -0.44009107  0.10416042  0.69428078]\n",
      "[-0.17726495 -0.24655634  0.11804604  0.43611988]\n",
      "[-0.18219608 -0.05328595  0.12676844  0.18285685]\n",
      "[-0.1832618   0.13981565  0.13042557 -0.06730109]\n",
      "[-0.18046549 -0.0569116   0.12907955  0.26351996]\n",
      "[-0.18160372  0.1361543   0.13434995  0.01417443]\n",
      "[-0.17888063 -0.06061298  0.13463344  0.3460458 ]\n",
      "[-0.18009289 -0.25736746  0.14155435  0.67796951]\n",
      "[-0.18524024 -0.45414265  0.15511374  1.01165818]\n",
      "[-0.19432309 -0.65095566  0.17534691  1.34875408]\n",
      "[-0.20734221 -0.84779319  0.20232199  1.69077116]\n",
      "Episode finished after 37 timesteps\n",
      "[ 0.03410478  0.01759938 -0.00805774 -0.01534685]\n",
      "[ 0.03445676  0.21283596 -0.00836467 -0.31056115]\n",
      "[ 0.03871348  0.01783418 -0.0145759  -0.0205279 ]\n",
      "[ 0.03907017  0.2131621  -0.01498646 -0.31777381]\n",
      "[ 0.04333341  0.01825677 -0.02134193 -0.0298545 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04369854  0.21367817 -0.02193902 -0.3291939 ]\n",
      "[ 0.04797211  0.40910546 -0.0285229  -0.62871385]\n",
      "[ 0.05615422  0.21439293 -0.04109718 -0.34514846]\n",
      "[ 0.06044207  0.01987896 -0.04800015 -0.06570303]\n",
      "[ 0.06083965 -0.17452311 -0.04931421  0.21145758]\n",
      "[ 0.05734919  0.02126797 -0.04508506 -0.0963645 ]\n",
      "[ 0.05777455  0.21700613 -0.04701235 -0.40292391]\n",
      "[ 0.06211467  0.02258139 -0.05507082 -0.12542556]\n",
      "[ 0.0625663  -0.17171013 -0.05757933  0.14938727]\n",
      "[ 0.0591321  -0.36596232 -0.05459159  0.42336369]\n",
      "[ 0.05181285 -0.56027011 -0.04612432  0.69834922]\n",
      "[ 0.04060745 -0.36453999 -0.03215733  0.39151055]\n",
      "[ 0.03331665 -0.55919116 -0.02432712  0.67388373]\n",
      "[ 0.02213283 -0.75396671 -0.01084945  0.95880913]\n",
      "[ 0.00705349 -0.94894113  0.00832674  1.24806387]\n",
      "[-0.01192533 -1.14416885  0.03328801  1.54334333]\n",
      "[-0.03480871 -1.33967466  0.06415488  1.84622445]\n",
      "[-0.0616022  -1.53544209  0.10107937  2.15812009]\n",
      "[-0.09231104 -1.73139931  0.14424177  2.48022237]\n",
      "[-0.12693903 -1.92740233  0.19384622  2.81343391]\n",
      "Episode finished after 25 timesteps\n",
      "[ 0.02540409 -0.03107803 -0.00699356  0.00847377]\n",
      "[ 0.02478253 -0.22609899 -0.00682409  0.29894197]\n",
      "[ 0.02026055 -0.03088043 -0.00084525  0.00411469]\n",
      "[ 0.01964294  0.16425363 -0.00076295 -0.2888348 ]\n",
      "[ 0.02292801 -0.03085743 -0.00653965  0.0036074 ]\n",
      "[ 0.02231086  0.16435769 -0.0064675  -0.29113167]\n",
      "[ 0.02559802 -0.03067144 -0.01229013 -0.00049552]\n",
      "[ 0.02498459  0.16462459 -0.01230004 -0.29703065]\n",
      "[ 0.02827708  0.3599197  -0.01824066 -0.59356728]\n",
      "[ 0.03547547  0.55529217 -0.030112   -0.89193968]\n",
      "[ 0.04658132  0.75080937 -0.0479508  -1.19393413]\n",
      "[ 0.0615975   0.55634008 -0.07182948 -0.91665762]\n",
      "[ 0.07272431  0.36225904 -0.09016263 -0.64738647]\n",
      "[ 0.07996949  0.16850128 -0.10311036 -0.38440228]\n",
      "[ 0.08333951  0.36492443 -0.11079841 -0.7077329 ]\n",
      "[ 0.090638    0.56139263 -0.12495306 -1.03313624]\n",
      "[ 0.10186585  0.36813395 -0.14561579 -0.78214901]\n",
      "[ 0.10922853  0.56492507 -0.16125877 -1.11687157]\n",
      "[ 0.12052703  0.37224399 -0.1835962  -0.87880459]\n",
      "[ 0.12797191  0.56932201 -0.20117229 -1.22312789]\n",
      "Episode finished after 20 timesteps\n",
      "[0.04995774 0.01531235 0.02242322 0.00913071]\n",
      "[ 0.05026399 -0.18012389  0.02260583  0.30880324]\n",
      "[0.04666151 0.01466879 0.02878189 0.02333442]\n",
      "[ 0.04695489 -0.18085384  0.02924858  0.32495761]\n",
      "[ 0.04333781 -0.37637977  0.03574774  0.62671892]\n",
      "[ 0.03581022 -0.18177455  0.04828211  0.34550518]\n",
      "[0.03217473 0.01262853 0.05519222 0.06842982]\n",
      "[ 0.0324273  -0.18323949  0.05656081  0.37800274]\n",
      "[ 0.02876251 -0.37911719  0.06412087  0.68796946]\n",
      "[ 0.02118016 -0.18494106  0.07788026  0.41614244]\n",
      "[0.01748134 0.00899573 0.08620311 0.14899268]\n",
      "[ 0.01766126 -0.18724808  0.08918296  0.46757769]\n",
      "[0.01391629 0.00650817 0.09853451 0.2042834 ]\n",
      "[ 0.01404646 -0.18987472  0.10262018  0.52635116]\n",
      "[0.01024896 0.0036649  0.11314721 0.26768641]\n",
      "[0.01032226 0.19700552 0.11850093 0.01272425]\n",
      "[ 0.01426237  0.39024625  0.11875542 -0.24034625]\n",
      "[ 0.0220673   0.58348925  0.11394849 -0.4933371 ]\n",
      "[ 0.03373708  0.77683522  0.10408175 -0.74804685]\n",
      "[ 0.04927379  0.9703791   0.08912081 -1.00624678]\n",
      "[ 0.06868137  1.16420526  0.06899588 -1.26966533]\n",
      "[ 0.09196547  1.35838174  0.04360257 -1.53996836]\n",
      "[ 0.11913311  1.1627633   0.0128032  -1.23400449]\n",
      "[ 0.14238837  1.35771834 -0.01187688 -1.52264904]\n",
      "[ 0.16954274  1.55298173 -0.04232987 -1.81901519]\n",
      "[ 0.20060238  1.7485478  -0.07871017 -2.12454278]\n",
      "[ 0.23557333  1.55429164 -0.12120103 -1.85717758]\n",
      "[ 0.26665916  1.75051809 -0.15834458 -2.18490395]\n",
      "[ 0.30166953  1.94678171 -0.20204266 -2.5219754 ]\n",
      "Episode finished after 29 timesteps\n",
      "[ 0.01418252  0.00515301 -0.0055178  -0.04301217]\n",
      "[ 0.01428558 -0.18988939 -0.00637804  0.24792474]\n",
      "[ 0.01048779  0.00532307 -0.00141955 -0.04676312]\n",
      "[ 0.01059425 -0.1897785  -0.00235481  0.24547159]\n",
      "[ 0.00679868  0.00537701  0.00255462 -0.04795317]\n",
      "[ 0.00690622  0.20046223  0.00159556 -0.33982899]\n",
      "[ 0.01091546  0.00531762 -0.00520102 -0.04664334]\n",
      "[ 0.01102182 -0.18972937 -0.00613389  0.24439409]\n",
      "[ 0.00722723  0.00547965 -0.00124601 -0.05021729]\n",
      "[ 0.00733682  0.20061945 -0.00225035 -0.34329308]\n",
      "[ 0.01134921  0.00552958 -0.00911621 -0.05132063]\n",
      "[ 0.0114598   0.20078106 -0.01014263 -0.3468658 ]\n",
      "[ 0.01547542  0.00580483 -0.01707994 -0.05739839]\n",
      "[ 0.01559152 -0.18906811 -0.01822791  0.22984713]\n",
      "[ 0.01181016 -0.38392491 -0.01363097  0.51672506]\n",
      "[ 0.00413166 -0.5788523  -0.00329647  0.8050816 ]\n",
      "[-0.00744539 -0.7739289   0.01280517  1.09672576]\n",
      "[-0.02292396 -0.96921711  0.03473968  1.39339867]\n",
      "[-0.04230831 -0.77454437  0.06260765  1.11177716]\n",
      "[-0.05779919 -0.9704303   0.0848432   1.42342501]\n",
      "[-0.0772078  -1.16649267  0.1133117   1.74137468]\n",
      "[-0.10053765 -1.36270774  0.14813919  2.06705223]\n",
      "[-0.12779181 -1.55899513  0.18948024  2.40165233]\n",
      "Episode finished after 23 timesteps\n",
      "[ 0.03349513 -0.00761818  0.03192814  0.03308794]\n",
      "[ 0.03334276  0.1870317   0.0325899  -0.2493528 ]\n",
      "[ 0.0370834   0.38167346  0.02760284 -0.53158066]\n",
      "[ 0.04471687  0.5763965   0.01697123 -0.81543976]\n",
      "[ 0.0562448   0.38104633  0.00066243 -0.5174674 ]\n",
      "[ 0.06386572  0.57615894 -0.00968692 -0.8099415 ]\n",
      "[ 0.0753889   0.77141227 -0.02588575 -1.10565566]\n",
      "[ 0.09081715  0.57664007 -0.04799886 -0.8212048 ]\n",
      "[ 0.10234995  0.7723848  -0.06442296 -1.12858997]\n",
      "[ 0.11779765  0.57816311 -0.08699476 -0.85678905]\n",
      "[ 0.12936091  0.38432735 -0.10413054 -0.59267914]\n",
      "[ 0.13704745  0.58074102 -0.11598412 -0.91626286]\n",
      "[ 0.14866227  0.38736228 -0.13430938 -0.66216654]\n",
      "[ 0.15640952  0.58407198 -0.14755271 -0.9939421 ]\n",
      "[ 0.16809096  0.7808266  -0.16743155 -1.32909033]\n",
      "[ 0.18370749  0.58816554 -0.19401336 -1.0931352 ]\n",
      "Episode finished after 16 timesteps\n",
      "[ 0.01034231  0.04383896 -0.03701539  0.00114807]\n",
      "[ 0.01121909 -0.15073312 -0.03699243  0.28192623]\n",
      "[ 0.00820443 -0.34530843 -0.03135391  0.56271622]\n",
      "[ 0.00129826 -0.14976085 -0.02009958  0.26032249]\n",
      "[-0.00169696 -0.3445902  -0.01489313  0.54659859]\n",
      "[-0.00858876 -0.53949977 -0.00396116  0.8345521 ]\n",
      "[-0.01937875 -0.34432392  0.01272988  0.54062606]\n",
      "[-0.02626523 -0.1493832   0.0235424   0.25198115]\n",
      "[-0.0292529   0.04539481  0.02858203 -0.03318407]\n",
      "[-0.028345   -0.15012512  0.02791834  0.268378  ]\n",
      "[-0.0313475   0.04458751  0.0332859  -0.0153702 ]\n",
      "[-0.03045575 -0.1509956   0.0329785   0.2876262 ]\n",
      "[-0.03347567  0.04364092  0.03873102  0.00552411]\n",
      "[-0.03260285  0.23818662  0.03884151 -0.27469153]\n",
      "[-0.02783911  0.43273348  0.03334768 -0.55487518]\n",
      "[-0.01918444  0.23715955  0.02225017 -0.251875  ]\n",
      "[-0.01444125  0.43195683  0.01721267 -0.53745758]\n",
      "[-0.00580212  0.23659716  0.00646352 -0.2394012 ]\n",
      "[-0.00107017  0.43162618  0.0016755  -0.53003836]\n",
      "[ 0.00756235  0.2364807  -0.00892527 -0.23682796]\n",
      "[ 0.01229196  0.04148739 -0.01366183  0.05302636]\n",
      "[ 0.01312171 -0.15343603 -0.0126013   0.34136775]\n",
      "[ 0.01005299 -0.34837645 -0.00577395  0.63005048]\n",
      "[ 0.00308546 -0.1531744   0.00682706  0.33555476]\n",
      "[2.19740503e-05 4.18497279e-02 1.35381578e-02 4.50325267e-02]\n",
      "[ 0.00085897 -0.15346371  0.01443881  0.34195593]\n",
      "[-0.00221031  0.04144987  0.02127793  0.0538609 ]\n",
      "[-0.00138131  0.23626036  0.02235515 -0.23203345]\n",
      "[0.0033439  0.04082623 0.01771448 0.06761636]\n",
      "[ 0.00416042 -0.15454515  0.0190668   0.36583524]\n",
      "[ 0.00106952 -0.3499328   0.02638351  0.66446874]\n",
      "[-0.00592914 -0.1551876   0.03967288  0.38020846]\n",
      "[-0.00903289  0.03934918  0.04727705  0.10029366]\n",
      "[-0.0082459  -0.15641733  0.04928293  0.40750959]\n",
      "[-0.01137425 -0.35220217  0.05743312  0.71531361]\n",
      "[-0.01841829 -0.15792027  0.07173939  0.44124732]\n",
      "[-0.0215767  -0.35398023  0.08056434  0.75565409]\n",
      "[-0.0286563  -0.16005576  0.09567742  0.4893712 ]\n",
      "[-0.03185742 -0.35638808  0.10546484  0.81060903]\n",
      "[-0.03898518 -0.1628569   0.12167702  0.55287444]\n",
      "[-0.04224232 -0.35945858  0.13273451  0.88128142]\n",
      "[-0.04943149 -0.16636514  0.15036014  0.63309849]\n",
      "[-0.05275879  0.02637491  0.16302211  0.39128988]\n",
      "[-0.05223129 -0.17063984  0.17084791  0.73061025]\n",
      "[-0.05564409 -0.36765934  0.18546011  1.07182217]\n",
      "[-0.06299728 -0.56468362  0.20689655  1.41650383]\n",
      "Episode finished after 46 timesteps\n",
      "[0.01165438 0.04622607 0.03569483 0.03578738]\n",
      "[ 0.0125789  -0.14938909  0.03641057  0.33951526]\n",
      "[ 0.00959112 -0.3450097   0.04320088  0.64345403]\n",
      "[ 0.00269092 -0.15051564  0.05606996  0.36468225]\n",
      "[-3.19388313e-04 -3.46387749e-01  6.33636034e-02  6.74504622e-01]\n",
      "[-0.00724714 -0.15220096  0.0768537   0.40242542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01029116  0.0417516   0.0849022   0.13492808]\n",
      "[-0.00945613  0.23556131  0.08760077 -0.12980804]\n",
      "[-0.0047449   0.03930081  0.08500461  0.18917557]\n",
      "[-0.00395889 -0.15692791  0.08878812  0.50741712]\n",
      "[-0.00709745  0.03683817  0.09893646  0.24398231]\n",
      "[-0.00636068 -0.1595475   0.10381611  0.56616015]\n",
      "[-0.00955163 -0.3559609   0.11513931  0.88966115]\n",
      "[-0.01667085 -0.16257371  0.13293253  0.63527726]\n",
      "[-0.01992233 -0.3592747   0.14563808  0.96669281]\n",
      "[-0.02710782 -0.55602041  0.16497193  1.30135348]\n",
      "[-0.03822823 -0.75280593  0.190999    1.64080617]\n",
      "Episode finished after 17 timesteps\n",
      "[ 0.0023049  -0.0293956   0.02101001  0.02690352]\n",
      "[ 0.00171698  0.16541885  0.02154809 -0.25907717]\n",
      "[ 0.00502536  0.36022666  0.01636654 -0.5448864 ]\n",
      "[ 0.0122299   0.55511486  0.00546881 -0.83236799]\n",
      "[ 0.02333219  0.3599186  -0.01117855 -0.53797016]\n",
      "[ 0.03053056  0.16495558 -0.02193795 -0.24883032]\n",
      "[ 0.03382968 -0.02984632 -0.02691456  0.03685297]\n",
      "[ 0.03323275 -0.22457218 -0.0261775   0.32092405]\n",
      "[ 0.02874131 -0.41931175 -0.01975902  0.60523801]\n",
      "[ 0.02035507 -0.61415189 -0.00765426  0.89163241]\n",
      "[ 0.00807203 -0.41892695  0.01017839  0.59655324]\n",
      "[-3.06505693e-04 -2.23948902e-01  2.21094579e-02  3.07093720e-01]\n",
      "[-0.00478548 -0.02914886  0.02825133  0.02146473]\n",
      "[-0.00536846  0.16555678  0.02868063 -0.26217241]\n",
      "[-0.00205733 -0.02996258  0.02343718  0.03941696]\n",
      "[-0.00265658 -0.22541264  0.02422552  0.33940144]\n",
      "[-0.00716483 -0.03064362  0.03101355  0.05445524]\n",
      "[-0.0077777  -0.22619622  0.03210265  0.3567596 ]\n",
      "[-0.01230163 -0.42175952  0.03923784  0.65938997]\n",
      "[-0.02073682 -0.22720499  0.05242564  0.37931576]\n",
      "[-0.02528092 -0.03286521  0.06001196  0.10361269]\n",
      "[-0.02593822  0.16134765  0.06208421 -0.16954926]\n",
      "[-0.02271127 -0.03460545  0.05869323  0.14205515]\n",
      "[-0.02340338  0.15962893  0.06153433 -0.13154883]\n",
      "[-0.0202108   0.35381792  0.05890335 -0.40420135]\n",
      "[-0.01313444  0.15791222  0.05081933 -0.09354487]\n",
      "[-0.0099762   0.35227034  0.04894843 -0.36977121]\n",
      "[-0.00293079  0.5466639   0.041553   -0.6466272 ]\n",
      "[ 0.00800249  0.35098835  0.02862046 -0.34115418]\n",
      "[ 0.01502226  0.15547113  0.02179738 -0.03958519]\n",
      "[ 0.01813168  0.35027384  0.02100567 -0.32531191]\n",
      "[ 0.02513716  0.1548592   0.01449943 -0.02607941]\n",
      "[ 0.02823434  0.34977025  0.01397785 -0.31415256]\n",
      "[ 0.03522975  0.154452    0.0076948  -0.01709446]\n",
      "[ 0.03831879 -0.04077945  0.00735291  0.2780063 ]\n",
      "[ 0.0375032   0.15423683  0.01291303 -0.01234849]\n",
      "[ 0.04058793  0.34917124  0.01266606 -0.30092939]\n",
      "[ 0.04757136  0.54411039  0.00664747 -0.58959094]\n",
      "[ 0.05845357  0.34889599 -0.00514434 -0.29482147]\n",
      "[ 0.06543148  0.15384776 -0.01104077 -0.00376542]\n",
      "[ 0.06850844  0.34912629 -0.01111608 -0.29991129]\n",
      "[ 0.07549097  0.54440491 -0.01711431 -0.59607918]\n",
      "[ 0.08637906  0.73976214 -0.02903589 -0.89410347]\n",
      "[ 0.10117431  0.54504574 -0.04691796 -0.61068734]\n",
      "[ 0.11207522  0.3506099  -0.05913171 -0.33314335]\n",
      "[ 0.11908742  0.15637725 -0.06579457 -0.05967817]\n",
      "[ 0.12221496 -0.03774266 -0.06698814  0.21154202]\n",
      "[ 0.12146011 -0.23184606 -0.0627573   0.48236431]\n",
      "[ 0.11682319 -0.03589707 -0.05311001  0.1705809 ]\n",
      "[ 0.11610525  0.15994324 -0.04969839 -0.13837236]\n",
      "[ 0.11930411 -0.03443297 -0.05246584  0.13822664]\n",
      "[ 0.11861545  0.16139965 -0.04970131 -0.17053597]\n",
      "[ 0.12184345 -0.03297699 -0.05311203  0.10606278]\n",
      "[ 0.12118391  0.16286427 -0.05099077 -0.20289247]\n",
      "[ 0.12444119 -0.03149277 -0.05504862  0.07327938]\n",
      "[ 0.12381134  0.16437339 -0.05358303 -0.23625094]\n",
      "[ 0.12709881  0.36021831 -0.05830805 -0.54534257]\n",
      "[ 0.13430317  0.55610901 -0.0692149  -0.85581211]\n",
      "[ 0.14542535  0.75210242 -0.08633115 -1.16943125]\n",
      "[ 0.1604674   0.94823461 -0.10971977 -1.48788371]\n",
      "[ 0.17943209  1.14450881 -0.13947745 -1.81271759]\n",
      "[ 0.20232227  0.95118904 -0.1757318  -1.56642754]\n",
      "[ 0.22134605  1.14792118 -0.20706035 -1.90838054]\n",
      "Episode finished after 63 timesteps\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T05:38:43.213862Z",
     "start_time": "2021-02-01T05:38:43.200241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ddpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T16:38:14.879393Z",
     "start_time": "2021-02-10T16:38:14.018617Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b\n",
    "# utils.py\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Ornstein-Ulhenbeck Process\n",
    "# Taken from #https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py\n",
    "class OUNoise(object):\n",
    "    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_space.shape[0]\n",
    "        self.low          = action_space.low\n",
    "        self.high         = action_space.high\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def evolve_state(self):\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "    def get_action(self, action, t=0):\n",
    "        ou_state = self.evolve_state()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "        return np.clip(action + ou_state, self.low, self.high)\n",
    "\n",
    "\n",
    "# https://github.com/openai/gym/blob/master/gym/core.py\n",
    "class NormalizedEnv(gym.ActionWrapper):\n",
    "    \"\"\" Wrap action \"\"\"\n",
    "\n",
    "    def action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k_inv * (action - act_b)\n",
    "        \n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, np.array([reward]), next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        for experience in batch:\n",
    "            state, action, reward, next_state, done = experience\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            done_batch.append(done)\n",
    "        \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "class Memory:\n",
    "    def __init__(self, max_size,state_size,action_size):\n",
    "        self.max_size = max_size\n",
    "        self.state_buf = np.zeros((max_size,state_size))\n",
    "        self.action_buf = np.zeros((max_size,action_size))\n",
    "        self.reward_buf = np.zeros((max_size,1))\n",
    "        self.next_state_buf = np.zeros((max_size,state_size))\n",
    "        self.done_buf = np.zeros(max_size,dtype=bool)\n",
    "        self.index = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        k = self.index%self.max_size\n",
    "        self.state_buf[k] = state\n",
    "        self.action_buf[k] = action\n",
    "        self.reward_buf[k] = reward\n",
    "        self.next_state_buf[k] = next_state\n",
    "        self.done_buf[k] = done\n",
    "        self.index+=1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ids = np.random.choice(len(self),batch_size)\n",
    "        state_batch = self.state_buf[ids]\n",
    "        action_batch = self.action_buf[ids]\n",
    "        reward_batch = self.reward_buf[ids]\n",
    "        next_state_batch = self.next_state_buf[ids]\n",
    "        done_batch = self.done_buf[ids]\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.index if self.index<self.max_size else self.max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T16:38:15.607520Z",
     "start_time": "2021-02-10T16:38:14.881393Z"
    }
   },
   "outputs": [],
   "source": [
    "# models.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        Params state and actions are torch tensors\n",
    "        \"\"\"\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate = 3e-4):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Param state is a torch tensor\n",
    "        \"\"\"\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = torch.tanh(self.linear3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T16:38:15.639541Z",
     "start_time": "2021-02-10T16:38:15.609521Z"
    }
   },
   "outputs": [],
   "source": [
    "# ddpg.py\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "# from model import *\n",
    "# from utils import *\n",
    "\n",
    "class DDPGagent:\n",
    "    def __init__(self, env, hidden_size=512, \n",
    "                 actor_learning_rate=1e-4, critic_learning_rate=1e-3, \n",
    "                 gamma=0.99, tau=1e-2, max_memory_size=50000,device=\"cuda\"):\n",
    "        # Params\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.shape[0]\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "\n",
    "        # Networks\n",
    "        self.actor = Actor(self.num_states, hidden_size, self.num_actions).to(device)\n",
    "        self.actor_target = Actor(self.num_states, hidden_size, self.num_actions).to(device)\n",
    "        self.critic = Critic(self.num_states + self.num_actions, hidden_size, self.num_actions).to(device)\n",
    "        self.critic_target = Critic(self.num_states + self.num_actions, hidden_size, self.num_actions).to(device)\n",
    "\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        # Training\n",
    "        self.memory = Memory(max_memory_size,self.num_states, self.num_actions)    \n",
    "#         self.memory = Memory(max_memory_size)\n",
    "        self.critic_criterion  = nn.MSELoss()\n",
    "        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=actor_learning_rate)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_learning_rate)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = Variable(torch.from_numpy(state).float().unsqueeze(0)).to(self.device)\n",
    "        action = self.actor.forward(state)\n",
    "        action = action.detach().cpu().numpy()[0,0]\n",
    "        return action\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        states, actions, rewards, next_states, _ = self.memory.sample(batch_size)\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.FloatTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "    \n",
    "        # Critic loss        \n",
    "        Qvals = self.critic.forward(states, actions)\n",
    "        next_actions = self.actor_target.forward(next_states)\n",
    "        next_Q = self.critic_target.forward(next_states, next_actions.detach())\n",
    "        Qprime = rewards + self.gamma * next_Q\n",
    "        critic_loss = self.critic_criterion(Qvals, Qprime)\n",
    "\n",
    "        # Actor loss\n",
    "        policy_loss = -self.critic.forward(states, self.actor.forward(states)).mean()\n",
    "        \n",
    "        # update networks\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward() \n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # update target networks \n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "       \n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:56:58.662174Z",
     "start_time": "2021-02-10T15:56:58.646013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.98062095, -0.19591467, -0.87179786]),\n",
       " -0.08320917045968758,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = gym.make(\"Pendulum-v0\")\n",
    "# env.reset()\n",
    "# env.step([0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T16:38:21.633120Z",
     "start_time": "2021-02-10T16:38:17.433941Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9d39cad63799>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-087828ce65c8>\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-72d9cb62b2be>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mParam\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \"\"\"\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\flexipod\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1136\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1137\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from DDPG import DDPGagent\n",
    "# from utils import *\n",
    "\n",
    "# env = NormalizedEnv(gym.make(\"Pendulum-v0\"))\n",
    "\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "\n",
    "agent = DDPGagent(env)\n",
    "noise = OUNoise(env.action_space)\n",
    "batch_size = 256\n",
    "rewards = []\n",
    "avg_rewards = []\n",
    "\n",
    "for episode in range(50):\n",
    "    state = env.reset()\n",
    "    noise.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(500):\n",
    "        action = agent.get_action(state)\n",
    "        action = noise.get_action(action, step)\n",
    "        new_state, reward, done, _ = env.step(action) \n",
    "        agent.memory.push(state, action, reward, new_state, done)\n",
    "        \n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.update(batch_size)        \n",
    "        \n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    rewards.append(episode_reward)\n",
    "    avg_rewards.append(np.mean(rewards[-10:]))\n",
    "    sys.stdout.write(\"episode: {}, reward: {}, average _reward: {} \\n\".format(episode, np.round(episode_reward, decimals=2), np.mean(rewards[-10:])))\n",
    "\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.plot(avg_rewards)\n",
    "plt.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T16:19:12.893617Z",
     "start_time": "2021-02-08T16:19:04.701127Z"
    }
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "noise.reset()\n",
    "episode_reward = 0\n",
    "\n",
    "for step in range(500):\n",
    "    env.render()\n",
    "    action = agent.get_action(state)\n",
    "    action = noise.get_action(action, step)\n",
    "    new_state, reward, done, _ = env.step(action) \n",
    "    agent.memory.push(state, action, reward, new_state, done)\n",
    "\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.update(batch_size)        \n",
    "\n",
    "    state = new_state\n",
    "    episode_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T01:07:22.990859Z",
     "start_time": "2021-02-11T01:07:22.960716Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import subprocess\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import msgpack\n",
    "import socket\n",
    "\n",
    "class FlexipodEnv(gym.Env):\n",
    "    \n",
    "    # name of command message\n",
    "    CMD_NAME = (\"header\", \"t\", \"cmd\")\n",
    "\n",
    "    # name of the returned message\n",
    "    REC_NAME = (\"header\",\n",
    "                \"t\",           # simulation time [s]\n",
    "                \"joint_pos\",   # joint angle [rad]\n",
    "                \"joint_vel\",   # joint velocity [rad/s]\n",
    "                \"actuation\",   # joint actuation [-1,1]\n",
    "                \"orientation\", # base link (body) orientation\n",
    "                \"ang_vel\",     # base link (body) angular velocity [rad/s]\n",
    "                \"com_acc\",     # base link (body) acceleration\n",
    "                \"com_vel\",     # base link (body) velocity\n",
    "                \"com_pos\"      # base link (body) position\n",
    "                )\n",
    "\n",
    "    ID =defaultdict(None,{name: k  for k,name in enumerate(REC_NAME)})\n",
    "    CMD_ID = defaultdict(None,{name: k  for k,name in enumerate(CMD_NAME)})\n",
    "    COMBINED_NAME = tuple(list(REC_NAME)[1:]+[\"cmd\"])\n",
    "    \n",
    "    UDP_TERMINATE = -1\n",
    "    UDP_PAUSE = 17\n",
    "    UDP_RESUME = 16\n",
    "    UDP_RESET = 15\n",
    "    UDP_ROBOT_STATE_REPORT = 14\n",
    "    UDP_MOTOR_VEL_COMMEND = 13\n",
    "    UDP_STEP_MOTOR_VEL_COMMEND = 12\n",
    "    UDP_MOTOR_POS_COMMEND = 11\n",
    "    UDP_STEP_MOTOR_POS_COMMEND = 10\n",
    "    \n",
    "    def __init__(self, dof = 12,\n",
    "           ip_local = \"127.0.0.1\", port_local = 32000,\n",
    "           ip_remote = \"127.0.0.1\",port_remote = 32001):\n",
    "        \n",
    "        super(FlexipodEnv,self).__init__()\n",
    "        self.local_address = (ip_local, port_local)\n",
    "        self.remote_address = (ip_remote, port_remote)\n",
    "        self.packer = msgpack.Packer(use_single_float=True, use_bin_type=True)\n",
    "        \n",
    "        self.dof = dof # num joints\n",
    "        \n",
    "        # joint_pos,joint_vel,actuation,orientation,ang_vel,com_acc,com_vel,com_pos.z\n",
    "        state_size = self.dof * 3 + 6 + 3*3+1\n",
    "        # state = np.empty(state_size,dtype=np.float32)\n",
    "        action_size = self.dof\n",
    "        self.max_action = 1.\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low= -self.max_action*np.ones(action_size,dtype=np.float32),\n",
    "            high = self.max_action*np.ones(action_size,dtype=np.float32),\n",
    "            dtype=np.float32)\n",
    "        \n",
    "        self.max_observation = np.hstack([\n",
    "            np.ones(dof)*np.pi,  # joint_pos,\n",
    "            np.ones(dof)*10.,   # joint_vel\n",
    "            np.ones(dof),       # actuation\n",
    "            np.ones(6),         # orientation\n",
    "            np.ones(3)*30,      # ang_vel\n",
    "            np.ones(3)*30,      # com_acc\n",
    "            np.ones(3)*2,       # com_vel\n",
    "            np.ones(1),         # com_pos_z\n",
    "        ]).astype(np.float32)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low =-self.max_observation,\n",
    "            high=self.max_observation,\n",
    "#             shape = state_size,\n",
    "            dtype = np.float32)\n",
    "\n",
    "        # reset data packed # https://msgpack-python.readthedocs.io/en/latest/api.html#msgpack.Packer\n",
    "        self.reset_cmd_b = self.packer.pack([self.UDP_RESET,0,[0,0,0,0]])\n",
    "        self.pause_cmd_b = self.packer.pack([self.UDP_PAUSE,0,[0,0,0,0]])\n",
    "        self.resume_cmd_b = self.packer.pack([self.UDP_RESUME,0,[0,0,0,0]])\n",
    "        \n",
    "        self.BUFFER_LEN = 512  # in bytes\n",
    "        self.TIMEOUT = 0.5 #timeout duration\n",
    "        \n",
    "        self._startSocket()\n",
    "#         gc.collect()\n",
    "\n",
    "        \n",
    "    def __del__(self): # Deleting (Calling destructor) \n",
    "        print('Destructor called, FlexipodEnv deleted.')\n",
    "        self.close()\n",
    "        \n",
    "        \n",
    "    def send(cmd):\n",
    "        sock = socket.socket(socket.AF_INET,socket.SOCK_DGRAM) # UDP\n",
    "        #Enable/disable immediate reuse of IP address\n",
    "        sock.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)\n",
    "        #Set a timeout so the socket does not block indefinitely when trying to receive data\n",
    "        self.sock.settimeout(self.TIMEOUT)\n",
    "#         self.sock.setblocking(0)\n",
    "        self.sock.bind(self.local_address) #Bind the socket to the port\n",
    "    \n",
    "    \n",
    "    def step(self,action:list):\n",
    "#         step_cmd_b = self.packer.pack([self.UDP_STEP_MOTOR_VEL_COMMEND,time.time(),action])\n",
    "        step_cmd_b = self.packer.pack([self.UDP_MOTOR_VEL_COMMEND,time.time(),action if type(action) is list else action.tolist()])\n",
    "        num_bytes_send = self.sock.sendto(step_cmd_b,self.remote_address)\n",
    "        self._closeSocket()\n",
    "        self._startSocket()\n",
    "        data = self.sock.recv(self.BUFFER_LEN)\n",
    "        msg_rec = msgpack.unpackb(data)\n",
    "        return self._processRecMsg(msg_rec)\n",
    "\n",
    "    def _processRecMsg(self,msg_rec):\n",
    "        \"\"\"processed received message to state action pair\"\"\"\n",
    "        # joint_pos,joint_vel,actuation,orientation,ang_vel,com_acc,com_vel,com_pos.z\n",
    "        observation = np.hstack(msg_rec[2:-1]+[msg_rec[-1][-1]]).astype(np.float32)\n",
    "        reward = msg_rec[self.ID['orientation']][2]+msg_rec[self.ID['com_pos']][2]\n",
    "        done = True if reward<0.8 else False\n",
    "        return observation,reward,done, None\n",
    "    \n",
    "    def reset(self):\n",
    "        time.sleep(1e-8)\n",
    "        self._closeSocket()\n",
    "        self._startSocket()\n",
    "        num_bytes_send = self.sock.sendto(self.reset_cmd_b,self.remote_address)\n",
    "        time.sleep(1e-8)\n",
    "        num_bytes_send = self.sock.sendto(self.pause_cmd_b,self.remote_address)\n",
    "        self._closeSocket()\n",
    "        self._startSocket()\n",
    "        data = self.sock.recv(self.BUFFER_LEN)\n",
    "        msg_rec = msgpack.unpackb(data)\n",
    "        return self._processRecMsg(msg_rec)[0]\n",
    "    \n",
    "    def render(self,mode=\"human\"):\n",
    "        pass\n",
    "    def close(self):\n",
    "        self._closeSocket()\n",
    "            \n",
    "\n",
    "    def _startSocket(self):\n",
    "        # socket\n",
    "        self.sock = socket.socket(socket.AF_INET,socket.SOCK_DGRAM) # UDP\n",
    "        #Enable/disable immediate reuse of IP address\n",
    "        self.sock.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)\n",
    "        #Set a timeout so the socket does not block indefinitely when trying to receive data\n",
    "        self.sock.settimeout(self.TIMEOUT)\n",
    "#         self.sock.setblocking(0)\n",
    "        self.sock.bind(self.local_address) #Bind the socket to the port\n",
    "        # self.sock.connect(remote_address)\n",
    "        \n",
    "    def _closeSocket(self):\n",
    "        if self.sock and self.sock._closed:\n",
    "            # for closing connection:\n",
    "            self.sock.shutdown(socket.SHUT_RDWR)\n",
    "            self.sock.close()\n",
    "        \n",
    "    def checkSimulation(self):\n",
    "        try: # check if the simulation is opened\n",
    "            data = self.sock.recv(self.BUFFER_LEN)\n",
    "        except socket.timeout: # program not opened\n",
    "#             task = subprocess.Popen([\"../../build/flexipod.exe\"])\n",
    "#             time.sleep(self.TIMEOUT)\n",
    "            raise OSError(\"felxipod program not opened\")\n",
    "\n",
    "    def receive(self):\n",
    "#         if self.sock._closed:\n",
    "#             self.sock.\n",
    "        try:\n",
    "            data = self.sock.recv(self.BUFFER_LEN)\n",
    "            data_unpacked = msgpack.unpackb(data)\n",
    "#             str_data = \"\".join([FlexipodEnv.item_to_String(n,d) for n,d in zip(self.REC_NAME,data_unpacked)])\n",
    "#             print(str_data)\n",
    "            return data_unpacked\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        \n",
    "    def item_to_String(key,value):\n",
    "        try:\n",
    "            return f\"{key:<12s}:{','.join(f'{k:>+6.2f}' for k in value)}\\n\"\n",
    "        except:\n",
    "            return f\"{key:<12s}:{value:>+6.2f}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T01:07:26.771046Z",
     "start_time": "2021-02-11T01:07:26.751977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Destructor called, FlexipodEnv deleted.\n"
     ]
    }
   ],
   "source": [
    "env = FlexipodEnv(dof = 12)\n",
    "# env.sock.sendto(env.resume_cmd_b,env.remote_address)\n",
    "# env.sock.sendto(env.pause_cmd_b,env.remote_address)\n",
    "# env.step((np.random.random(12)*1).tolist())\n",
    "# for k in range(100):\n",
    "#     env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T01:07:31.691672Z",
     "start_time": "2021-02-11T01:07:27.826935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in range(3):\n",
    "    env.reset()\n",
    "    env.sock.sendto(env.resume_cmd_b,env.remote_address)\n",
    "    for t in range(1000):\n",
    "        observation,reward,done,_=env.step(((-0.5+np.random.random(12))*5).tolist())\n",
    "        if done:\n",
    "            break\n",
    "#     env.sock.sendto(env.pause_cmd_b,env.remote_address)\n",
    "env.sock.sendto(env.pause_cmd_b,env.remote_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T01:07:35.116739Z",
     "start_time": "2021-02-11T01:07:35.089749Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from DDPG import DDPGagent\n",
    "# from utils import *\n",
    "\n",
    "\n",
    "env = NormalizedEnv(env)\n",
    "\n",
    "agent = DDPGagent(env)\n",
    "noise = OUNoise(env.action_space)\n",
    "batch_size = 256\n",
    "rewards = []\n",
    "avg_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T01:21:46.312175Z",
     "start_time": "2021-02-11T01:07:45.414397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, reward: 202.67, average _reward: 202.67361030320598 \n",
      "episode: 1, reward: 167.74, average _reward: 185.2091343478519 \n",
      "episode: 2, reward: 83.66, average _reward: 151.36038721882144 \n",
      "episode: 3, reward: 57.83, average _reward: 127.97661216811673 \n",
      "episode: 4, reward: 74.04, average _reward: 117.19014074435844 \n",
      "episode: 5, reward: 54.13, average _reward: 106.68005508583553 \n",
      "episode: 6, reward: 60.19, average _reward: 100.0392780378291 \n",
      "episode: 7, reward: 69.92, average _reward: 96.27397284966068 \n",
      "episode: 8, reward: 66.36, average _reward: 92.95074549190774 \n",
      "episode: 9, reward: 66.75, average _reward: 90.33071142265577 \n",
      "episode: 10, reward: 73.47, average _reward: 77.41056722146722 \n",
      "episode: 11, reward: 65.94, average _reward: 67.23022027194516 \n",
      "episode: 12, reward: 71.76, average _reward: 66.03975139701011 \n",
      "episode: 13, reward: 116.91, average _reward: 71.94785869095554 \n",
      "episode: 14, reward: 105.07, average _reward: 75.05051466188416 \n",
      "episode: 15, reward: 155.45, average _reward: 85.18256478966528 \n",
      "episode: 16, reward: 131.01, average _reward: 92.26422705032496 \n",
      "episode: 17, reward: 123.26, average _reward: 97.59894573660654 \n",
      "episode: 18, reward: 87.93, average _reward: 99.75527438660359 \n",
      "episode: 19, reward: 69.51, average _reward: 100.03075042916984 \n",
      "episode: 20, reward: 71.05, average _reward: 99.78877657888358 \n",
      "episode: 21, reward: 61.72, average _reward: 99.36681933910236 \n",
      "episode: 22, reward: 61.4, average _reward: 98.33115962001538 \n",
      "episode: 23, reward: 62.0, average _reward: 92.84031344809182 \n",
      "episode: 24, reward: 66.55, average _reward: 88.98844585939115 \n",
      "episode: 25, reward: 101.39, average _reward: 83.58254402085417 \n",
      "episode: 26, reward: 66.36, average _reward: 77.11748815732788 \n",
      "episode: 27, reward: 72.36, average _reward: 72.0266640666529 \n",
      "episode: 28, reward: 69.09, average _reward: 70.14301369273957 \n",
      "episode: 29, reward: 75.48, average _reward: 70.74019171588182 \n",
      "episode: 30, reward: 60.87, average _reward: 69.72155124801505 \n",
      "episode: 31, reward: 63.81, average _reward: 69.93044170799196 \n",
      "episode: 32, reward: 63.28, average _reward: 70.11856497004275 \n",
      "episode: 33, reward: 64.98, average _reward: 70.41701169027397 \n",
      "episode: 34, reward: 69.94, average _reward: 70.75609108861036 \n",
      "episode: 35, reward: 85.27, average _reward: 69.14371800730683 \n",
      "episode: 36, reward: 80.56, average _reward: 70.56332850615289 \n",
      "episode: 37, reward: 104.12, average _reward: 73.74013678829493 \n",
      "episode: 38, reward: 99.5, average _reward: 76.78055817132051 \n",
      "episode: 39, reward: 80.76, average _reward: 77.30875121210849 \n",
      "episode: 40, reward: 77.22, average _reward: 78.94447676836808 \n",
      "episode: 41, reward: 80.82, average _reward: 80.64566360494682 \n",
      "episode: 42, reward: 84.28, average _reward: 82.74498430345862 \n",
      "episode: 43, reward: 77.91, average _reward: 84.037949662676 \n",
      "episode: 44, reward: 76.98, average _reward: 84.74122048616812 \n",
      "episode: 45, reward: 80.93, average _reward: 84.30743470699927 \n",
      "episode: 46, reward: 67.85, average _reward: 83.03680435529296 \n",
      "episode: 47, reward: 129.02, average _reward: 85.52654261892084 \n",
      "episode: 48, reward: 86.66, average _reward: 84.24289705018262 \n",
      "episode: 49, reward: 87.42, average _reward: 84.90928483290274 \n",
      "episode: 50, reward: 79.0, average _reward: 85.08728962354725 \n",
      "episode: 51, reward: 88.57, average _reward: 85.8620678609976 \n",
      "episode: 52, reward: 88.47, average _reward: 86.28166882980247 \n",
      "episode: 53, reward: 100.77, average _reward: 88.56760234782567 \n",
      "episode: 54, reward: 63.27, average _reward: 87.19707136095184 \n",
      "episode: 55, reward: 144.29, average _reward: 93.53281048133185 \n",
      "episode: 56, reward: 96.66, average _reward: 96.41404531978337 \n",
      "episode: 57, reward: 79.52, average _reward: 91.4641136878417 \n",
      "episode: 58, reward: 93.84, average _reward: 92.18180387790952 \n",
      "episode: 59, reward: 76.84, average _reward: 91.12334839855966 \n",
      "episode: 60, reward: 59.68, average _reward: 89.19109477349173 \n",
      "episode: 61, reward: 60.61, average _reward: 86.39553806892293 \n",
      "episode: 62, reward: 83.14, average _reward: 85.8622090985169 \n",
      "episode: 63, reward: 87.32, average _reward: 84.51659706510574 \n",
      "episode: 64, reward: 91.83, average _reward: 87.37302483478939 \n",
      "episode: 65, reward: 82.26, average _reward: 81.1701915230213 \n",
      "episode: 66, reward: 102.23, average _reward: 81.72697152895624 \n",
      "episode: 67, reward: 107.43, average _reward: 84.51789124378915 \n",
      "episode: 68, reward: 143.66, average _reward: 89.50070267961527 \n",
      "episode: 69, reward: 76.68, average _reward: 89.48457975999035 \n",
      "episode: 70, reward: 79.75, average _reward: 91.49199469689295 \n",
      "episode: 71, reward: 65.12, average _reward: 91.94204489700489 \n",
      "episode: 72, reward: 72.8, average _reward: 90.90831880166672 \n",
      "episode: 73, reward: 80.13, average _reward: 90.19006835988628 \n",
      "episode: 74, reward: 116.67, average _reward: 92.67371058824172 \n",
      "episode: 75, reward: 111.62, average _reward: 95.61016569615998 \n",
      "episode: 76, reward: 100.9, average _reward: 95.47698775328067 \n",
      "episode: 77, reward: 74.3, average _reward: 92.16432849530807 \n",
      "episode: 78, reward: 78.08, average _reward: 85.60619915190867 \n",
      "episode: 79, reward: 79.64, average _reward: 85.90228919534157 \n",
      "episode: 80, reward: 69.62, average _reward: 84.88927134779314 \n",
      "episode: 81, reward: 65.37, average _reward: 84.91462023979243 \n",
      "episode: 82, reward: 62.44, average _reward: 83.8784874664961 \n",
      "episode: 83, reward: 87.22, average _reward: 84.58723734593642 \n",
      "episode: 84, reward: 79.95, average _reward: 80.91471341370472 \n",
      "episode: 85, reward: 61.02, average _reward: 75.8541819253478 \n",
      "episode: 86, reward: 74.03, average _reward: 73.16690816429033 \n",
      "episode: 87, reward: 121.01, average _reward: 77.83780314181355 \n",
      "episode: 88, reward: 60.78, average _reward: 76.10705650891924 \n",
      "episode: 89, reward: 51.99, average _reward: 73.34253683996238 \n",
      "episode: 90, reward: 64.06, average _reward: 72.78565825332566 \n",
      "episode: 91, reward: 60.56, average _reward: 72.30486609033967 \n",
      "episode: 92, reward: 57.56, average _reward: 71.81684120314239 \n",
      "episode: 93, reward: 63.29, average _reward: 69.42351769404998 \n",
      "episode: 94, reward: 59.44, average _reward: 67.37254361184094 \n",
      "episode: 95, reward: 80.47, average _reward: 69.31755456289292 \n",
      "episode: 96, reward: 83.73, average _reward: 70.28787809244709 \n",
      "episode: 97, reward: 67.47, average _reward: 64.9333607316249 \n",
      "episode: 98, reward: 71.86, average _reward: 66.0416079377699 \n",
      "episode: 99, reward: 70.77, average _reward: 67.91968508822382 \n",
      "episode: 100, reward: 74.05, average _reward: 68.91864570955326 \n",
      "episode: 101, reward: 70.71, average _reward: 69.93391688254621 \n",
      "episode: 102, reward: 62.14, average _reward: 70.39234500327571 \n",
      "episode: 103, reward: 76.2, average _reward: 71.68413047716122 \n",
      "episode: 104, reward: 63.94, average _reward: 72.13488008459713 \n",
      "episode: 105, reward: 62.44, average _reward: 70.3317398461779 \n",
      "episode: 106, reward: 57.5, average _reward: 67.7087083380067 \n",
      "episode: 107, reward: 63.35, average _reward: 67.29700261318985 \n",
      "episode: 108, reward: 62.55, average _reward: 66.36643897758569 \n",
      "episode: 109, reward: 66.53, average _reward: 65.94195159673733 \n",
      "episode: 110, reward: 58.33, average _reward: 64.37055363255229 \n",
      "episode: 111, reward: 64.8, average _reward: 63.77956917847539 \n",
      "episode: 112, reward: 62.34, average _reward: 63.79874753645182 \n",
      "episode: 113, reward: 81.02, average _reward: 64.28026854250035 \n",
      "episode: 114, reward: 59.25, average _reward: 63.81081605071925 \n",
      "episode: 115, reward: 86.29, average _reward: 66.19595681427978 \n",
      "episode: 116, reward: 64.65, average _reward: 66.91122319968021 \n",
      "episode: 117, reward: 126.6, average _reward: 73.23602557019407 \n",
      "episode: 118, reward: 58.81, average _reward: 72.86183864235196 \n",
      "episode: 119, reward: 95.19, average _reward: 75.7279502801944 \n",
      "episode: 120, reward: 77.77, average _reward: 77.67211283892452 \n",
      "episode: 121, reward: 82.06, average _reward: 79.39769441944375 \n",
      "episode: 122, reward: 107.21, average _reward: 83.8850815065605 \n",
      "episode: 123, reward: 100.53, average _reward: 85.83615538571073 \n",
      "episode: 124, reward: 76.09, average _reward: 87.52043678825399 \n",
      "episode: 125, reward: 90.85, average _reward: 87.9768478908695 \n",
      "episode: 126, reward: 99.72, average _reward: 91.48337920250093 \n",
      "episode: 127, reward: 61.21, average _reward: 84.94476255889569 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 128, reward: 69.93, average _reward: 86.05693280513294 \n",
      "episode: 129, reward: 74.32, average _reward: 83.97045893051094 \n",
      "episode: 130, reward: 55.71, average _reward: 81.76392891246809 \n",
      "episode: 131, reward: 65.0, average _reward: 80.05775519206054 \n",
      "episode: 132, reward: 92.22, average _reward: 78.55889979522371 \n",
      "episode: 133, reward: 75.92, average _reward: 76.09830444428849 \n",
      "episode: 134, reward: 75.15, average _reward: 76.00420678950636 \n",
      "episode: 135, reward: 83.09, average _reward: 75.22760236656084 \n",
      "episode: 136, reward: 66.77, average _reward: 71.93264353955996 \n",
      "episode: 137, reward: 81.38, average _reward: 73.94904828318901 \n",
      "episode: 138, reward: 80.54, average _reward: 75.01004647164322 \n",
      "episode: 139, reward: 59.76, average _reward: 73.55366533494643 \n",
      "episode: 140, reward: 64.4, average _reward: 74.42261475328628 \n",
      "episode: 141, reward: 57.3, average _reward: 73.65244521024036 \n",
      "episode: 142, reward: 89.64, average _reward: 73.39400990391876 \n",
      "episode: 143, reward: 57.5, average _reward: 71.55189197479811 \n",
      "episode: 144, reward: 86.28, average _reward: 72.6646974211786 \n",
      "episode: 145, reward: 57.28, average _reward: 70.08366324507182 \n",
      "episode: 146, reward: 61.94, average _reward: 69.60133767178813 \n",
      "episode: 147, reward: 58.74, average _reward: 67.33764943804577 \n",
      "episode: 148, reward: 72.08, average _reward: 66.49177082996196 \n",
      "episode: 149, reward: 57.4, average _reward: 66.2556177498613 \n",
      "episode: 150, reward: 67.15, average _reward: 66.53135461394443 \n",
      "episode: 151, reward: 80.46, average _reward: 68.84782770824324 \n",
      "episode: 152, reward: 57.12, average _reward: 65.59597180288907 \n",
      "episode: 153, reward: 60.28, average _reward: 65.87331531593564 \n",
      "episode: 154, reward: 59.44, average _reward: 63.189105028296616 \n",
      "episode: 155, reward: 58.99, average _reward: 63.36004609538429 \n",
      "episode: 156, reward: 58.05, average _reward: 62.970310934604335 \n",
      "episode: 157, reward: 65.29, average _reward: 63.624797320843015 \n",
      "episode: 158, reward: 85.62, average _reward: 64.97845244269715 \n",
      "episode: 159, reward: 82.48, average _reward: 67.48627198413675 \n",
      "episode: 160, reward: 53.59, average _reward: 66.13025574391862 \n",
      "episode: 161, reward: 66.4, average _reward: 64.72396229125188 \n",
      "episode: 162, reward: 58.03, average _reward: 64.81502216896564 \n",
      "episode: 163, reward: 56.77, average _reward: 64.46454457050464 \n",
      "episode: 164, reward: 66.51, average _reward: 65.17156755034503 \n",
      "episode: 165, reward: 59.61, average _reward: 65.23357894627358 \n",
      "episode: 166, reward: 71.39, average _reward: 66.5677996574516 \n",
      "episode: 167, reward: 74.45, average _reward: 67.48447765159474 \n",
      "episode: 168, reward: 75.61, average _reward: 66.48346171512163 \n",
      "episode: 169, reward: 58.74, average _reward: 64.11016656003633 \n",
      "episode: 170, reward: 60.04, average _reward: 64.75495904996166 \n",
      "episode: 171, reward: 56.36, average _reward: 63.75163523443414 \n",
      "episode: 172, reward: 81.16, average _reward: 66.06476026242687 \n",
      "episode: 173, reward: 56.05, average _reward: 65.99250464739325 \n",
      "episode: 174, reward: 74.44, average _reward: 66.78579513316114 \n",
      "episode: 175, reward: 62.55, average _reward: 67.0806989369072 \n",
      "episode: 176, reward: 52.88, average _reward: 65.22933526462337 \n",
      "episode: 177, reward: 73.89, average _reward: 65.1735863326255 \n",
      "episode: 178, reward: 54.99, average _reward: 63.11146170236685 \n",
      "episode: 179, reward: 55.24, average _reward: 62.76110822047011 \n",
      "episode: 180, reward: 55.54, average _reward: 62.31076908549166 \n",
      "episode: 181, reward: 52.0, average _reward: 61.87469767713695 \n",
      "episode: 182, reward: 56.31, average _reward: 59.389845544339195 \n",
      "episode: 183, reward: 53.57, average _reward: 59.141555289598614 \n",
      "episode: 184, reward: 57.63, average _reward: 57.46060028908512 \n",
      "episode: 185, reward: 63.23, average _reward: 57.52775653363371 \n",
      "episode: 186, reward: 55.06, average _reward: 57.74592038159265 \n",
      "episode: 187, reward: 71.3, average _reward: 57.486911613734605 \n",
      "episode: 188, reward: 82.91, average _reward: 60.27905563136378 \n",
      "episode: 189, reward: 53.73, average _reward: 60.12779512337089 \n",
      "episode: 190, reward: 62.84, average _reward: 60.85792958524708 \n",
      "episode: 191, reward: 76.6, average _reward: 63.31720567936791 \n",
      "episode: 192, reward: 63.4, average _reward: 64.02649959197576 \n",
      "episode: 193, reward: 66.93, average _reward: 65.36267097195243 \n",
      "episode: 194, reward: 67.59, average _reward: 66.35830361492907 \n",
      "episode: 195, reward: 72.15, average _reward: 67.25049481012283 \n",
      "episode: 196, reward: 75.38, average _reward: 69.28305489941347 \n",
      "episode: 197, reward: 92.38, average _reward: 71.39093557350043 \n",
      "episode: 198, reward: 51.21, average _reward: 68.22126410458957 \n",
      "episode: 199, reward: 67.34, average _reward: 69.58240967850293 \n",
      "episode: 200, reward: 67.65, average _reward: 70.06360315424645 \n",
      "episode: 201, reward: 74.02, average _reward: 69.80581999695957 \n",
      "episode: 202, reward: 66.21, average _reward: 70.08617969227427 \n",
      "episode: 203, reward: 77.85, average _reward: 71.17792243002482 \n",
      "episode: 204, reward: 63.38, average _reward: 70.75693936554245 \n",
      "episode: 205, reward: 69.98, average _reward: 70.53989401150803 \n",
      "episode: 206, reward: 68.31, average _reward: 69.8324022194103 \n",
      "episode: 207, reward: 57.31, average _reward: 66.32528407708575 \n",
      "episode: 208, reward: 69.24, average _reward: 68.12811110832085 \n",
      "episode: 209, reward: 63.2, average _reward: 67.71395282231148 \n",
      "episode: 210, reward: 118.04, average _reward: 72.75299876675254 \n",
      "episode: 211, reward: 71.01, average _reward: 72.45262200544056 \n",
      "episode: 212, reward: 109.16, average _reward: 76.74820189577048 \n",
      "episode: 213, reward: 114.08, average _reward: 80.37182361552736 \n",
      "episode: 214, reward: 65.98, average _reward: 80.63181996950159 \n",
      "episode: 215, reward: 68.79, average _reward: 80.51321063150542 \n",
      "episode: 216, reward: 75.88, average _reward: 81.27041891317272 \n",
      "episode: 217, reward: 66.73, average _reward: 82.21210729855267 \n",
      "episode: 218, reward: 78.56, average _reward: 83.1435698492393 \n",
      "episode: 219, reward: 69.78, average _reward: 83.80203259801058 \n",
      "episode: 220, reward: 77.14, average _reward: 79.71199544000822 \n",
      "episode: 221, reward: 69.25, average _reward: 79.535162984707 \n",
      "episode: 222, reward: 92.79, average _reward: 77.89748241515393 \n",
      "episode: 223, reward: 67.14, average _reward: 73.20354280609881 \n",
      "episode: 224, reward: 102.03, average _reward: 76.80874277076988 \n",
      "episode: 225, reward: 90.8, average _reward: 79.00953626493178 \n",
      "episode: 226, reward: 71.5, average _reward: 78.57135833083036 \n",
      "episode: 227, reward: 90.58, average _reward: 80.95672558011975 \n",
      "episode: 228, reward: 107.89, average _reward: 83.89014170754783 \n",
      "episode: 229, reward: 83.8, average _reward: 85.2917810996414 \n",
      "episode: 230, reward: 64.18, average _reward: 83.99526326706606 \n",
      "episode: 231, reward: 100.1, average _reward: 87.08081889432142 \n",
      "episode: 232, reward: 77.82, average _reward: 85.58434924678866 \n",
      "episode: 233, reward: 87.3, average _reward: 87.59984089380853 \n",
      "episode: 234, reward: 93.15, average _reward: 86.711791980914 \n",
      "episode: 235, reward: 83.67, average _reward: 85.99857670187372 \n",
      "episode: 236, reward: 91.21, average _reward: 87.96950431661844 \n",
      "episode: 237, reward: 91.73, average _reward: 88.08412522817679 \n",
      "episode: 238, reward: 79.32, average _reward: 85.2275707145062 \n",
      "episode: 239, reward: 74.46, average _reward: 84.29331943567563 \n",
      "episode: 240, reward: 92.96, average _reward: 87.1714878903794 \n",
      "episode: 241, reward: 90.5, average _reward: 86.21113709604278 \n",
      "episode: 242, reward: 73.79, average _reward: 85.80796041728605 \n",
      "episode: 243, reward: 69.9, average _reward: 84.06842861271508 \n",
      "episode: 244, reward: 68.43, average _reward: 81.5966226886183 \n",
      "episode: 245, reward: 68.65, average _reward: 80.09541579678202 \n",
      "episode: 246, reward: 70.26, average _reward: 78.00047900153916 \n",
      "episode: 247, reward: 78.18, average _reward: 76.64609320530582 \n",
      "episode: 248, reward: 66.94, average _reward: 75.40726586287258 \n",
      "episode: 249, reward: 75.46, average _reward: 75.50743312715944 \n",
      "episode: 250, reward: 69.44, average _reward: 73.15530671908058 \n",
      "episode: 251, reward: 70.6, average _reward: 71.16527192862553 \n",
      "episode: 252, reward: 109.45, average _reward: 74.73089439392255 \n",
      "episode: 253, reward: 83.21, average _reward: 76.06201180768798 \n",
      "episode: 254, reward: 95.42, average _reward: 78.7605896161493 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 255, reward: 79.54, average _reward: 79.8495046546723 \n",
      "episode: 256, reward: 74.45, average _reward: 80.26828453036003 \n",
      "episode: 257, reward: 73.06, average _reward: 79.75608985366003 \n",
      "episode: 258, reward: 57.27, average _reward: 78.78939943068528 \n",
      "episode: 259, reward: 61.92, average _reward: 77.43598227121294 \n",
      "episode: 260, reward: 61.65, average _reward: 76.65728793889797 \n",
      "episode: 261, reward: 59.9, average _reward: 75.5878479779291 \n",
      "episode: 262, reward: 61.38, average _reward: 70.78108583790905 \n",
      "episode: 263, reward: 58.5, average _reward: 68.30974458872237 \n",
      "episode: 264, reward: 56.03, average _reward: 64.37131556658147 \n",
      "episode: 265, reward: 56.46, average _reward: 62.06308040021221 \n",
      "episode: 266, reward: 67.83, average _reward: 61.401709598721425 \n",
      "episode: 267, reward: 66.95, average _reward: 60.79081785420908 \n",
      "episode: 268, reward: 53.21, average _reward: 60.384347560880954 \n",
      "episode: 269, reward: 54.75, average _reward: 59.66703467961165 \n",
      "episode: 270, reward: 57.48, average _reward: 59.25001045523898 \n",
      "episode: 271, reward: 53.91, average _reward: 58.65030808849715 \n",
      "episode: 272, reward: 54.08, average _reward: 57.92077068770063 \n",
      "episode: 273, reward: 67.48, average _reward: 58.81896062334241 \n",
      "episode: 274, reward: 33.52, average _reward: 56.567421343949036 \n",
      "episode: 275, reward: 11.07, average _reward: 52.02865442558194 \n",
      "episode: 276, reward: 12.88, average _reward: 46.53355120237958 \n",
      "episode: 277, reward: 13.0, average _reward: 41.13860771929283 \n",
      "episode: 278, reward: 10.63, average _reward: 36.880743612232536 \n",
      "episode: 279, reward: 9.93, average _reward: 32.398793805390866 \n",
      "episode: 280, reward: 11.0, average _reward: 27.75056766679188 \n",
      "episode: 281, reward: 14.22, average _reward: 23.78171756550376 \n",
      "episode: 282, reward: 17.36, average _reward: 20.109623505534373 \n",
      "episode: 283, reward: 15.15, average _reward: 14.876416476316113 \n",
      "episode: 284, reward: 10.42, average _reward: 12.566432982056721 \n",
      "episode: 285, reward: 12.7, average _reward: 12.728853940108456 \n",
      "episode: 286, reward: 10.54, average _reward: 12.494734872928586 \n",
      "episode: 287, reward: 13.23, average _reward: 12.517695664822588 \n",
      "episode: 288, reward: 11.65, average _reward: 12.619614794739086 \n",
      "episode: 289, reward: 10.43, average _reward: 12.669361386320205 \n",
      "episode: 290, reward: 12.16, average _reward: 12.785441366674945 \n",
      "episode: 291, reward: 13.54, average _reward: 12.718030665993 \n",
      "episode: 292, reward: 11.21, average _reward: 12.103158102453872 \n",
      "episode: 293, reward: 12.88, average _reward: 11.87626118394068 \n",
      "episode: 294, reward: 9.46, average _reward: 11.780420560342227 \n",
      "episode: 295, reward: 12.15, average _reward: 11.725644214112938 \n",
      "episode: 296, reward: 11.69, average _reward: 11.841032816553865 \n",
      "episode: 297, reward: 10.82, average _reward: 11.599193412093214 \n",
      "episode: 298, reward: 12.01, average _reward: 11.635882038349617 \n",
      "episode: 299, reward: 11.12, average _reward: 11.704605767165793 \n",
      "episode: 300, reward: 12.62, average _reward: 11.75039032339885 \n",
      "episode: 301, reward: 9.57, average _reward: 11.353404313220008 \n",
      "episode: 302, reward: 9.75, average _reward: 11.206839549490203 \n",
      "episode: 303, reward: 7.92, average _reward: 10.711105513983833 \n",
      "episode: 304, reward: 11.69, average _reward: 10.934121189669895 \n",
      "episode: 305, reward: 9.88, average _reward: 10.707242788744583 \n",
      "episode: 306, reward: 9.3, average _reward: 10.468274087711718 \n",
      "episode: 307, reward: 8.38, average _reward: 10.224826109173675 \n",
      "episode: 308, reward: 11.08, average _reward: 10.131967338692712 \n",
      "episode: 309, reward: 10.69, average _reward: 10.089373019263837 \n",
      "episode: 310, reward: 13.46, average _reward: 10.173917917372766 \n",
      "episode: 311, reward: 9.39, average _reward: 10.155547235695959 \n",
      "episode: 312, reward: 10.88, average _reward: 10.26894673620008 \n",
      "episode: 313, reward: 8.48, average _reward: 10.324734200132038 \n",
      "episode: 314, reward: 13.48, average _reward: 10.50349929050152 \n",
      "episode: 315, reward: 10.28, average _reward: 10.543334177494186 \n",
      "episode: 316, reward: 10.32, average _reward: 10.645078947882258 \n",
      "episode: 317, reward: 8.64, average _reward: 10.670541195786226 \n",
      "episode: 318, reward: 10.65, average _reward: 10.627059968761605 \n",
      "episode: 319, reward: 11.08, average _reward: 10.66589156682496 \n",
      "episode: 320, reward: 11.11, average _reward: 10.430523078052056 \n",
      "episode: 321, reward: 11.16, average _reward: 10.607772177221833 \n",
      "episode: 322, reward: 13.34, average _reward: 10.853999067315101 \n",
      "episode: 323, reward: 13.18, average _reward: 11.323471032930598 \n",
      "episode: 324, reward: 10.85, average _reward: 11.061058652940236 \n",
      "episode: 325, reward: 10.94, average _reward: 11.126948427046832 \n",
      "episode: 326, reward: 14.89, average _reward: 11.583323710066816 \n",
      "episode: 327, reward: 9.8, average _reward: 11.699865218065039 \n",
      "episode: 328, reward: 13.03, average _reward: 11.938105490946224 \n",
      "episode: 329, reward: 9.33, average _reward: 11.762804619377984 \n",
      "episode: 330, reward: 13.41, average _reward: 11.99258638189317 \n",
      "episode: 331, reward: 13.29, average _reward: 12.204978026704055 \n",
      "episode: 332, reward: 14.04, average _reward: 12.27455013908035 \n",
      "episode: 333, reward: 15.82, average _reward: 12.538665795326832 \n",
      "episode: 334, reward: 10.4, average _reward: 12.493313719816388 \n",
      "episode: 335, reward: 11.19, average _reward: 12.518555356039736 \n",
      "episode: 336, reward: 11.16, average _reward: 12.146336423662671 \n",
      "episode: 337, reward: 9.85, average _reward: 12.151668031202906 \n",
      "episode: 338, reward: 10.46, average _reward: 11.894207575931256 \n",
      "episode: 339, reward: 14.06, average _reward: 12.367986393217427 \n",
      "episode: 340, reward: 11.46, average _reward: 12.173722560911182 \n",
      "episode: 341, reward: 9.59, average _reward: 11.804430579890916 \n",
      "episode: 342, reward: 12.3, average _reward: 11.629898593233971 \n",
      "episode: 343, reward: 11.92, average _reward: 11.239731239172823 \n",
      "episode: 344, reward: 12.54, average _reward: 11.454160190927583 \n",
      "episode: 345, reward: 12.91, average _reward: 11.62575379521262 \n",
      "episode: 346, reward: 13.31, average _reward: 11.840491917563982 \n",
      "episode: 347, reward: 13.56, average _reward: 12.210657356045054 \n",
      "episode: 348, reward: 9.87, average _reward: 12.152215951685003 \n",
      "episode: 349, reward: 11.03, average _reward: 11.849071436336121 \n",
      "episode: 350, reward: 11.03, average _reward: 11.80587206336074 \n",
      "episode: 351, reward: 10.46, average _reward: 11.891954530306414 \n",
      "episode: 352, reward: 12.22, average _reward: 11.884607480779346 \n",
      "episode: 353, reward: 12.09, average _reward: 11.901603080702667 \n",
      "episode: 354, reward: 12.23, average _reward: 11.870640682294702 \n",
      "episode: 355, reward: 12.36, average _reward: 11.815418869702476 \n",
      "episode: 356, reward: 12.96, average _reward: 11.780320218720894 \n",
      "episode: 357, reward: 11.58, average _reward: 11.582566901783519 \n",
      "episode: 358, reward: 12.1, average _reward: 11.805150612928339 \n",
      "episode: 359, reward: 9.22, average _reward: 11.6243958326361 \n",
      "episode: 360, reward: 10.71, average _reward: 11.592681533480917 \n",
      "episode: 361, reward: 14.01, average _reward: 11.948461464522541 \n",
      "episode: 362, reward: 12.78, average _reward: 12.004646807217126 \n",
      "episode: 363, reward: 10.95, average _reward: 11.890618996113238 \n",
      "episode: 364, reward: 9.76, average _reward: 11.643623797311037 \n",
      "episode: 365, reward: 12.27, average _reward: 11.634817479046069 \n",
      "episode: 366, reward: 11.03, average _reward: 11.442196104334077 \n",
      "episode: 367, reward: 13.32, average _reward: 11.615932058547836 \n",
      "episode: 368, reward: 11.04, average _reward: 11.50978929361734 \n",
      "episode: 369, reward: 12.46, average _reward: 11.832853876884258 \n",
      "episode: 370, reward: 13.37, average _reward: 12.09846224017279 \n",
      "episode: 371, reward: 15.04, average _reward: 12.200746121537023 \n",
      "episode: 372, reward: 12.95, average _reward: 12.217328321135287 \n",
      "episode: 373, reward: 12.46, average _reward: 12.368601107046334 \n",
      "episode: 374, reward: 11.67, average _reward: 12.559198352969458 \n",
      "episode: 375, reward: 12.83, average _reward: 12.615142766095286 \n",
      "episode: 376, reward: 12.02, average _reward: 12.713919016393346 \n",
      "episode: 377, reward: 10.68, average _reward: 12.45006081738374 \n",
      "episode: 378, reward: 13.8, average _reward: 12.726541306247986 \n",
      "episode: 379, reward: 10.99, average _reward: 12.580093915192137 \n",
      "episode: 380, reward: 15.51, average _reward: 12.793959479482371 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 381, reward: 10.45, average _reward: 12.334979717183653 \n",
      "episode: 382, reward: 12.33, average _reward: 12.27306536584925 \n",
      "episode: 383, reward: 11.99, average _reward: 12.226400849966828 \n",
      "episode: 384, reward: 11.92, average _reward: 12.25169482194747 \n",
      "episode: 385, reward: 10.7, average _reward: 12.03887641058879 \n",
      "episode: 386, reward: 11.18, average _reward: 11.95454478888136 \n",
      "episode: 387, reward: 10.95, average _reward: 11.98159683843593 \n",
      "episode: 388, reward: 10.62, average _reward: 11.663785382244466 \n",
      "episode: 389, reward: 14.14, average _reward: 11.978982327127607 \n",
      "episode: 390, reward: 13.94, average _reward: 11.822302655069466 \n",
      "episode: 391, reward: 12.58, average _reward: 12.03532599971101 \n",
      "episode: 392, reward: 11.59, average _reward: 11.961514821341058 \n",
      "episode: 393, reward: 9.31, average _reward: 11.692945735357366 \n",
      "episode: 394, reward: 11.23, average _reward: 11.623868017555125 \n",
      "episode: 395, reward: 9.27, average _reward: 11.481194799056677 \n",
      "episode: 396, reward: 12.42, average _reward: 11.605682965709976 \n",
      "episode: 397, reward: 9.59, average _reward: 11.469484404354533 \n",
      "episode: 398, reward: 11.24, average _reward: 11.5312075608502 \n",
      "episode: 399, reward: 15.04, average _reward: 11.620573025900319 \n",
      "episode: 400, reward: 10.96, average _reward: 11.321943269466136 \n",
      "episode: 401, reward: 11.28, average _reward: 11.1924651127534 \n",
      "episode: 402, reward: 12.87, average _reward: 11.320313516133025 \n",
      "episode: 403, reward: 13.31, average _reward: 11.720847682080134 \n",
      "episode: 404, reward: 11.82, average _reward: 11.78005564521629 \n",
      "episode: 405, reward: 13.37, average _reward: 12.190376508204874 \n",
      "episode: 406, reward: 11.71, average _reward: 12.119573732826337 \n",
      "episode: 407, reward: 11.01, average _reward: 12.262199018900692 \n",
      "episode: 408, reward: 9.58, average _reward: 12.095899292825582 \n",
      "episode: 409, reward: 12.0, average _reward: 11.792623907547767 \n",
      "episode: 410, reward: 16.25, average _reward: 12.321855331851248 \n",
      "episode: 411, reward: 13.43, average _reward: 12.537174293585794 \n",
      "episode: 412, reward: 10.71, average _reward: 12.320632177124446 \n",
      "episode: 413, reward: 11.98, average _reward: 12.187788614322594 \n",
      "episode: 414, reward: 11.14, average _reward: 12.119174194051292 \n",
      "episode: 415, reward: 11.86, average _reward: 11.968047865585712 \n",
      "episode: 416, reward: 12.3, average _reward: 12.026183521559634 \n",
      "episode: 417, reward: 13.87, average _reward: 12.311949282979011 \n",
      "episode: 418, reward: 13.31, average _reward: 12.685183904776824 \n",
      "episode: 419, reward: 11.04, average _reward: 12.588692796290598 \n",
      "episode: 420, reward: 12.2, average _reward: 12.18336495225102 \n",
      "episode: 421, reward: 11.37, average _reward: 11.976466187069141 \n",
      "episode: 422, reward: 14.01, average _reward: 12.307403108483033 \n",
      "episode: 423, reward: 9.45, average _reward: 12.054257541584343 \n",
      "episode: 424, reward: 12.37, average _reward: 12.177220199837173 \n",
      "episode: 425, reward: 10.81, average _reward: 12.071935725924329 \n",
      "episode: 426, reward: 12.85, average _reward: 12.126886783224542 \n",
      "episode: 427, reward: 13.16, average _reward: 12.055433247647775 \n",
      "episode: 428, reward: 10.51, average _reward: 11.775370830046635 \n",
      "episode: 429, reward: 12.68, average _reward: 11.939906131468694 \n",
      "episode: 430, reward: 8.32, average _reward: 11.552772825857533 \n",
      "episode: 431, reward: 8.58, average _reward: 11.27391508879904 \n",
      "episode: 432, reward: 10.46, average _reward: 10.918854754141277 \n",
      "episode: 433, reward: 10.78, average _reward: 11.051333416215806 \n",
      "episode: 434, reward: 9.06, average _reward: 10.720773999003999 \n",
      "episode: 435, reward: 9.36, average _reward: 10.57606237960081 \n",
      "episode: 436, reward: 8.13, average _reward: 10.104978801323437 \n",
      "episode: 437, reward: 8.46, average _reward: 9.63521708009138 \n",
      "episode: 438, reward: 9.34, average _reward: 9.517899997516526 \n",
      "episode: 439, reward: 8.51, average _reward: 9.100863785722133 \n",
      "episode: 440, reward: 16.02, average _reward: 9.870468387378038 \n",
      "episode: 441, reward: 13.92, average _reward: 10.405258258045517 \n",
      "episode: 442, reward: 9.61, average _reward: 10.319486010633714 \n",
      "episode: 443, reward: 11.45, average _reward: 10.386699703563492 \n",
      "episode: 444, reward: 9.82, average _reward: 10.462630001076864 \n",
      "episode: 445, reward: 16.71, average _reward: 11.197337883538495 \n",
      "episode: 446, reward: 11.25, average _reward: 11.509029936229421 \n",
      "episode: 447, reward: 11.24, average _reward: 11.787030451208972 \n",
      "episode: 448, reward: 13.76, average _reward: 12.229102813273425 \n",
      "episode: 449, reward: 10.53, average _reward: 12.43091882963125 \n",
      "episode: 450, reward: 11.45, average _reward: 11.97429361079949 \n",
      "episode: 451, reward: 11.88, average _reward: 11.76951897412527 \n",
      "episode: 452, reward: 11.94, average _reward: 12.00278421083066 \n",
      "episode: 453, reward: 9.97, average _reward: 11.855429836692144 \n",
      "episode: 454, reward: 11.87, average _reward: 12.060509176337925 \n",
      "episode: 455, reward: 19.64, average _reward: 12.353418440471863 \n",
      "episode: 456, reward: 12.62, average _reward: 12.490511880819508 \n",
      "episode: 457, reward: 8.65, average _reward: 12.231479384476154 \n",
      "episode: 458, reward: 12.58, average _reward: 12.113273060080493 \n",
      "episode: 459, reward: 14.82, average _reward: 12.541660749595561 \n",
      "episode: 460, reward: 11.89, average _reward: 12.58516008702748 \n",
      "episode: 461, reward: 12.52, average _reward: 12.649737677631883 \n",
      "episode: 462, reward: 14.02, average _reward: 12.85800039632341 \n",
      "episode: 463, reward: 12.7, average _reward: 13.130540178845076 \n",
      "episode: 464, reward: 11.32, average _reward: 13.075530610670683 \n",
      "episode: 465, reward: 12.51, average _reward: 12.362379067872295 \n",
      "episode: 466, reward: 15.36, average _reward: 12.636442628859223 \n",
      "episode: 467, reward: 13.46, average _reward: 13.117948479572874 \n",
      "episode: 468, reward: 11.79, average _reward: 13.039134011368017 \n",
      "episode: 469, reward: 13.84, average _reward: 12.941977615065605 \n",
      "episode: 470, reward: 12.73, average _reward: 13.025947271508425 \n",
      "episode: 471, reward: 14.41, average _reward: 13.214546909308865 \n",
      "episode: 472, reward: 12.01, average _reward: 13.013670014542893 \n",
      "episode: 473, reward: 11.43, average _reward: 12.887054725617414 \n",
      "episode: 474, reward: 13.87, average _reward: 13.141500927367957 \n",
      "episode: 475, reward: 14.19, average _reward: 13.31001404571047 \n",
      "episode: 476, reward: 13.37, average _reward: 13.11047904329657 \n",
      "episode: 477, reward: 15.38, average _reward: 13.302086608681492 \n",
      "episode: 478, reward: 16.46, average _reward: 13.76976084192998 \n",
      "episode: 479, reward: 12.32, average _reward: 13.617545804129136 \n",
      "episode: 480, reward: 25.3, average _reward: 14.87457483437097 \n",
      "episode: 481, reward: 11.0, average _reward: 14.533286866804218 \n",
      "episode: 482, reward: 14.51, average _reward: 14.782505755396368 \n",
      "episode: 483, reward: 13.75, average _reward: 15.014179861789572 \n",
      "episode: 484, reward: 11.45, average _reward: 14.772291089865979 \n",
      "episode: 485, reward: 14.2, average _reward: 14.773093781862759 \n",
      "episode: 486, reward: 14.85, average _reward: 14.921295349166837 \n",
      "episode: 487, reward: 12.07, average _reward: 14.590125036324974 \n",
      "episode: 488, reward: 13.39, average _reward: 14.282992726903638 \n",
      "episode: 489, reward: 12.31, average _reward: 14.281814587309603 \n",
      "episode: 490, reward: 19.24, average _reward: 13.67626079301564 \n",
      "episode: 491, reward: 9.98, average _reward: 13.574938931057215 \n",
      "episode: 492, reward: 10.18, average _reward: 13.1423454595039 \n",
      "episode: 493, reward: 12.0, average _reward: 12.967458812236373 \n",
      "episode: 494, reward: 12.37, average _reward: 13.05980324216335 \n",
      "episode: 495, reward: 13.81, average _reward: 13.020688216179291 \n",
      "episode: 496, reward: 11.56, average _reward: 12.692137015822214 \n",
      "episode: 497, reward: 13.04, average _reward: 12.789155069135058 \n",
      "episode: 498, reward: 11.1, average _reward: 12.56010119834825 \n",
      "episode: 499, reward: 10.84, average _reward: 12.412722769148738 \n",
      "episode: 500, reward: 12.67, average _reward: 11.755826591926162 \n",
      "episode: 501, reward: 10.8, average _reward: 11.837874704102283 \n",
      "episode: 502, reward: 12.61, average _reward: 12.080471114666008 \n",
      "episode: 503, reward: 13.71, average _reward: 12.251390701969942 \n",
      "episode: 504, reward: 11.0, average _reward: 12.113988363367998 \n",
      "episode: 505, reward: 12.11, average _reward: 11.94352887399699 \n",
      "episode: 506, reward: 13.78, average _reward: 12.165161256700259 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 507, reward: 10.79, average _reward: 11.940785435766083 \n",
      "episode: 508, reward: 10.57, average _reward: 11.887999223249086 \n",
      "episode: 509, reward: 13.01, average _reward: 12.104847483078212 \n",
      "episode: 510, reward: 12.38, average _reward: 12.075128813631416 \n",
      "episode: 511, reward: 11.21, average _reward: 12.115601918653226 \n",
      "episode: 512, reward: 13.55, average _reward: 12.21047819753895 \n",
      "episode: 513, reward: 11.7, average _reward: 12.009593044399201 \n",
      "episode: 514, reward: 11.07, average _reward: 12.017221742629072 \n",
      "episode: 515, reward: 12.03, average _reward: 12.009970744044882 \n",
      "episode: 516, reward: 11.13, average _reward: 11.745276587523541 \n",
      "episode: 517, reward: 11.21, average _reward: 11.78716546813358 \n",
      "episode: 518, reward: 13.21, average _reward: 12.050927031042919 \n",
      "episode: 519, reward: 15.19, average _reward: 12.269585346059932 \n",
      "episode: 520, reward: 12.76, average _reward: 12.307968359538574 \n",
      "episode: 521, reward: 12.23, average _reward: 12.409707262179035 \n",
      "episode: 522, reward: 12.09, average _reward: 12.263163793764248 \n",
      "episode: 523, reward: 12.85, average _reward: 12.37836487034913 \n",
      "episode: 524, reward: 11.49, average _reward: 12.419869078566139 \n",
      "episode: 525, reward: 12.11, average _reward: 12.427861499808207 \n",
      "episode: 526, reward: 15.81, average _reward: 12.895872659027805 \n",
      "episode: 527, reward: 12.23, average _reward: 12.997849564364765 \n",
      "episode: 528, reward: 10.69, average _reward: 12.745659292267334 \n",
      "episode: 529, reward: 12.93, average _reward: 12.519538899820313 \n",
      "episode: 530, reward: 15.18, average _reward: 12.761848765900544 \n",
      "episode: 531, reward: 9.75, average _reward: 12.51415936095285 \n",
      "episode: 532, reward: 12.14, average _reward: 12.51888892213126 \n",
      "episode: 533, reward: 12.02, average _reward: 12.435466369399473 \n",
      "episode: 534, reward: 10.53, average _reward: 12.33964061222287 \n",
      "episode: 535, reward: 10.96, average _reward: 12.224630708251274 \n",
      "episode: 536, reward: 10.84, average _reward: 11.72718218408169 \n",
      "episode: 537, reward: 10.98, average _reward: 11.602146402521965 \n",
      "episode: 538, reward: 13.48, average _reward: 11.880811050427313 \n",
      "episode: 539, reward: 12.38, average _reward: 11.82588454317354 \n",
      "episode: 540, reward: 11.94, average _reward: 11.5011549765719 \n",
      "episode: 541, reward: 10.69, average _reward: 11.595677409265098 \n",
      "episode: 542, reward: 9.01, average _reward: 11.283441354180676 \n",
      "episode: 543, reward: 15.48, average _reward: 11.62945091006885 \n",
      "episode: 544, reward: 11.81, average _reward: 11.757787094226915 \n",
      "episode: 545, reward: 13.11, average _reward: 11.97231969460138 \n",
      "episode: 546, reward: 10.89, average _reward: 11.977777558409503 \n",
      "episode: 547, reward: 12.05, average _reward: 12.084133963962921 \n",
      "episode: 548, reward: 12.84, average _reward: 12.020444203047239 \n",
      "episode: 549, reward: 11.77, average _reward: 11.959254050696364 \n",
      "episode: 550, reward: 11.0, average _reward: 11.86572720214634 \n",
      "episode: 551, reward: 10.61, average _reward: 11.857783774815799 \n",
      "episode: 552, reward: 8.42, average _reward: 11.79837775754252 \n",
      "episode: 553, reward: 10.6, average _reward: 11.310291653410964 \n",
      "episode: 554, reward: 12.98, average _reward: 11.427470309752016 \n",
      "episode: 555, reward: 16.05, average _reward: 11.721397649274438 \n",
      "episode: 556, reward: 13.88, average _reward: 12.019861091125463 \n",
      "episode: 557, reward: 10.81, average _reward: 11.896265238511456 \n",
      "episode: 558, reward: 12.95, average _reward: 11.90763164952256 \n",
      "episode: 559, reward: 10.53, average _reward: 11.783277951869655 \n",
      "episode: 560, reward: 11.14, average _reward: 11.797208702917942 \n",
      "episode: 561, reward: 10.62, average _reward: 11.79748104298814 \n",
      "episode: 562, reward: 10.65, average _reward: 12.020852641080753 \n",
      "episode: 563, reward: 13.98, average _reward: 12.359170196516413 \n",
      "episode: 564, reward: 10.54, average _reward: 12.11437162039215 \n",
      "episode: 565, reward: 12.73, average _reward: 11.782940194818746 \n",
      "episode: 566, reward: 11.24, average _reward: 11.51957916278936 \n",
      "episode: 567, reward: 11.02, average _reward: 11.540835272156636 \n",
      "episode: 568, reward: 16.22, average _reward: 11.867950078585336 \n",
      "episode: 569, reward: 13.23, average _reward: 12.138500881176197 \n",
      "episode: 570, reward: 13.0, average _reward: 12.32459124280959 \n",
      "episode: 571, reward: 11.56, average _reward: 12.41846000789157 \n",
      "episode: 572, reward: 9.62, average _reward: 12.315310140670118 \n",
      "episode: 573, reward: 11.05, average _reward: 12.022353755353196 \n",
      "episode: 574, reward: 11.9, average _reward: 12.158698962447016 \n",
      "episode: 575, reward: 12.59, average _reward: 12.14483787280794 \n",
      "episode: 576, reward: 14.25, average _reward: 12.445146719726937 \n",
      "episode: 577, reward: 14.51, average _reward: 12.793914755302833 \n",
      "episode: 578, reward: 12.21, average _reward: 12.392705462106699 \n",
      "episode: 579, reward: 14.03, average _reward: 12.472282552779971 \n",
      "episode: 580, reward: 14.07, average _reward: 12.578629744078066 \n",
      "episode: 581, reward: 10.24, average _reward: 12.446980079933203 \n",
      "episode: 582, reward: 15.02, average _reward: 12.987090451212746 \n",
      "episode: 583, reward: 12.32, average _reward: 13.11342853058513 \n",
      "episode: 584, reward: 10.63, average _reward: 12.986519113743071 \n",
      "episode: 585, reward: 9.77, average _reward: 12.703927524855265 \n",
      "episode: 586, reward: 12.62, average _reward: 12.541173135436951 \n",
      "episode: 587, reward: 11.67, average _reward: 12.257512537930818 \n",
      "episode: 588, reward: 11.49, average _reward: 12.185327143787193 \n",
      "episode: 589, reward: 14.75, average _reward: 12.257440636100736 \n",
      "episode: 590, reward: 14.59, average _reward: 12.309733466021116 \n",
      "episode: 591, reward: 11.78, average _reward: 12.464265104450849 \n",
      "episode: 592, reward: 10.63, average _reward: 12.025164647642432 \n",
      "episode: 593, reward: 12.9, average _reward: 12.083737502541021 \n",
      "episode: 594, reward: 11.97, average _reward: 12.217308786023327 \n",
      "episode: 595, reward: 12.95, average _reward: 12.53497737863194 \n",
      "episode: 596, reward: 11.03, average _reward: 12.376011307848858 \n",
      "episode: 597, reward: 12.59, average _reward: 12.467387409035714 \n",
      "episode: 598, reward: 9.47, average _reward: 12.265123870587528 \n",
      "episode: 599, reward: 13.15, average _reward: 12.105180052373267 \n",
      "episode: 600, reward: 11.55, average _reward: 11.80095544811385 \n",
      "episode: 601, reward: 10.71, average _reward: 11.693744555567383 \n",
      "episode: 602, reward: 12.27, average _reward: 11.857448985678328 \n",
      "episode: 603, reward: 11.99, average _reward: 11.766298011326693 \n",
      "episode: 604, reward: 14.47, average _reward: 12.016858863643833 \n",
      "episode: 605, reward: 10.55, average _reward: 11.777701313392251 \n",
      "episode: 606, reward: 11.2, average _reward: 11.795104946735982 \n",
      "episode: 607, reward: 12.07, average _reward: 11.743122433283906 \n",
      "episode: 608, reward: 12.65, average _reward: 12.061056422887084 \n",
      "episode: 609, reward: 12.89, average _reward: 12.034760424398419 \n",
      "episode: 610, reward: 11.09, average _reward: 11.988699799682879 \n",
      "episode: 611, reward: 13.85, average _reward: 12.302369179375779 \n",
      "episode: 612, reward: 15.6, average _reward: 12.635405978985222 \n",
      "episode: 613, reward: 13.58, average _reward: 12.793938245742266 \n",
      "episode: 614, reward: 15.36, average _reward: 12.883129816916318 \n",
      "episode: 615, reward: 12.38, average _reward: 13.066143029664033 \n",
      "episode: 616, reward: 11.1, average _reward: 13.055958091757129 \n",
      "episode: 617, reward: 11.24, average _reward: 12.973532607371283 \n",
      "episode: 618, reward: 12.31, average _reward: 12.940130395481518 \n",
      "episode: 619, reward: 8.98, average _reward: 12.549767623145502 \n",
      "episode: 620, reward: 12.22, average _reward: 12.66281466990956 \n",
      "episode: 621, reward: 13.22, average _reward: 12.599911796587726 \n",
      "episode: 622, reward: 11.58, average _reward: 12.197872011440122 \n",
      "episode: 623, reward: 13.26, average _reward: 12.166235087581539 \n",
      "episode: 624, reward: 11.64, average _reward: 11.793622691090064 \n",
      "episode: 625, reward: 11.16, average _reward: 11.670938387802927 \n",
      "episode: 626, reward: 12.25, average _reward: 11.785681163342478 \n",
      "episode: 627, reward: 11.87, average _reward: 11.848005201351977 \n",
      "episode: 628, reward: 11.19, average _reward: 11.735316730189515 \n",
      "episode: 629, reward: 12.27, average _reward: 12.064591247128329 \n",
      "episode: 630, reward: 11.67, average _reward: 12.00966292854736 \n",
      "episode: 631, reward: 11.19, average _reward: 11.80655504496921 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 632, reward: 11.51, average _reward: 11.799925193858577 \n",
      "episode: 633, reward: 12.6, average _reward: 11.734174238564163 \n",
      "episode: 634, reward: 12.05, average _reward: 11.775100656838607 \n",
      "episode: 635, reward: 12.6, average _reward: 11.919148638684394 \n",
      "episode: 636, reward: 11.35, average _reward: 11.829124029315 \n",
      "episode: 637, reward: 9.9, average _reward: 11.632977546212036 \n",
      "episode: 638, reward: 13.62, average _reward: 11.87639306194089 \n",
      "episode: 639, reward: 14.62, average _reward: 12.110493907754543 \n",
      "episode: 640, reward: 13.17, average _reward: 12.261174934312399 \n",
      "episode: 641, reward: 13.32, average _reward: 12.474222242417845 \n",
      "episode: 642, reward: 12.26, average _reward: 12.549272794978 \n",
      "episode: 643, reward: 12.14, average _reward: 12.503441597486152 \n",
      "episode: 644, reward: 12.55, average _reward: 12.554150007777372 \n",
      "episode: 645, reward: 12.1, average _reward: 12.504717062050767 \n",
      "episode: 646, reward: 11.92, average _reward: 12.561895198123972 \n",
      "episode: 647, reward: 15.26, average _reward: 13.09771261565098 \n",
      "episode: 648, reward: 10.42, average _reward: 12.77777576360578 \n",
      "episode: 649, reward: 12.1, average _reward: 12.526110571926704 \n",
      "episode: 650, reward: 12.92, average _reward: 12.501229077085911 \n",
      "episode: 651, reward: 12.27, average _reward: 12.395790420158454 \n",
      "episode: 652, reward: 13.94, average _reward: 12.563265286423006 \n",
      "episode: 653, reward: 11.48, average _reward: 12.497342534040122 \n",
      "episode: 654, reward: 9.63, average _reward: 12.205135120333813 \n",
      "episode: 655, reward: 10.8, average _reward: 12.074543381786963 \n",
      "episode: 656, reward: 13.12, average _reward: 12.193937955362978 \n",
      "episode: 657, reward: 15.73, average _reward: 12.2406714808608 \n",
      "episode: 658, reward: 14.04, average _reward: 12.602568097886524 \n",
      "episode: 659, reward: 13.39, average _reward: 12.731194561231046 \n",
      "episode: 660, reward: 11.06, average _reward: 12.544759583802751 \n",
      "episode: 661, reward: 13.42, average _reward: 12.660719904821837 \n",
      "episode: 662, reward: 9.49, average _reward: 12.215992516426166 \n",
      "episode: 663, reward: 12.07, average _reward: 12.274853892871196 \n",
      "episode: 664, reward: 11.85, average _reward: 12.496490956162392 \n",
      "episode: 665, reward: 10.3, average _reward: 12.446569862340628 \n",
      "episode: 666, reward: 16.99, average _reward: 12.834267083245345 \n",
      "episode: 667, reward: 11.01, average _reward: 12.362605945033515 \n",
      "episode: 668, reward: 10.98, average _reward: 12.056234100138369 \n",
      "episode: 669, reward: 11.79, average _reward: 11.896540793653829 \n",
      "episode: 670, reward: 12.38, average _reward: 12.029003621497528 \n",
      "episode: 671, reward: 13.58, average _reward: 12.044601926083255 \n",
      "episode: 672, reward: 10.75, average _reward: 12.17062673419359 \n",
      "episode: 673, reward: 18.71, average _reward: 12.834250106387808 \n",
      "episode: 674, reward: 12.88, average _reward: 12.937216250970398 \n",
      "episode: 675, reward: 11.58, average _reward: 13.065701029932423 \n",
      "episode: 676, reward: 13.64, average _reward: 12.73075477533008 \n",
      "episode: 677, reward: 12.14, average _reward: 12.843873222807042 \n",
      "episode: 678, reward: 12.17, average _reward: 12.96361697700525 \n",
      "episode: 679, reward: 12.16, average _reward: 13.00119566445404 \n",
      "episode: 680, reward: 13.42, average _reward: 13.104336493037957 \n",
      "episode: 681, reward: 13.6, average _reward: 13.106080272926476 \n",
      "episode: 682, reward: 12.71, average _reward: 13.301718342582145 \n",
      "episode: 683, reward: 12.23, average _reward: 12.653787367841488 \n",
      "episode: 684, reward: 13.19, average _reward: 12.684571594861625 \n",
      "episode: 685, reward: 13.69, average _reward: 12.894881774553776 \n",
      "episode: 686, reward: 14.91, average _reward: 13.021410802911983 \n",
      "episode: 687, reward: 11.77, average _reward: 12.983777832733603 \n",
      "episode: 688, reward: 10.58, average _reward: 12.823948500860405 \n",
      "episode: 689, reward: 14.45, average _reward: 13.052638980795257 \n",
      "episode: 690, reward: 13.81, average _reward: 13.092050336698525 \n",
      "episode: 691, reward: 11.32, average _reward: 12.86395189614385 \n",
      "episode: 692, reward: 13.4, average _reward: 12.933751152944643 \n",
      "episode: 693, reward: 11.21, average _reward: 12.832233475446028 \n",
      "episode: 694, reward: 11.91, average _reward: 12.704419732264045 \n",
      "episode: 695, reward: 11.66, average _reward: 12.502116772188241 \n",
      "episode: 696, reward: 10.96, average _reward: 12.106888012059501 \n",
      "episode: 697, reward: 11.9, average _reward: 12.12025467764959 \n",
      "episode: 698, reward: 15.73, average _reward: 12.635888913678412 \n",
      "episode: 699, reward: 13.1, average _reward: 12.50086553981995 \n",
      "episode: 700, reward: 15.82, average _reward: 12.702322976240424 \n",
      "episode: 701, reward: 12.43, average _reward: 12.813229449523442 \n",
      "episode: 702, reward: 13.33, average _reward: 12.806206757740805 \n",
      "episode: 703, reward: 10.86, average _reward: 12.77075715861663 \n",
      "episode: 704, reward: 11.05, average _reward: 12.685410581488872 \n",
      "episode: 705, reward: 13.65, average _reward: 12.884351575066788 \n",
      "episode: 706, reward: 15.7, average _reward: 13.358674394242357 \n",
      "episode: 707, reward: 14.69, average _reward: 13.637558928283275 \n",
      "episode: 708, reward: 13.65, average _reward: 13.429745941088447 \n",
      "episode: 709, reward: 11.87, average _reward: 13.306362671688003 \n",
      "episode: 710, reward: 10.74, average _reward: 12.797956026589128 \n",
      "episode: 711, reward: 12.14, average _reward: 12.769752905901765 \n",
      "episode: 712, reward: 10.97, average _reward: 12.533471628339502 \n",
      "episode: 713, reward: 9.7, average _reward: 12.417798126281815 \n",
      "episode: 714, reward: 16.14, average _reward: 12.926126832467764 \n",
      "episode: 715, reward: 14.0, average _reward: 12.960496693100193 \n",
      "episode: 716, reward: 10.7, average _reward: 12.460667354133928 \n",
      "episode: 717, reward: 10.09, average _reward: 12.000687182465082 \n",
      "episode: 718, reward: 9.67, average _reward: 11.60276761116681 \n",
      "episode: 719, reward: 12.28, average _reward: 11.644289658872122 \n",
      "episode: 720, reward: 15.73, average _reward: 12.143438209834127 \n",
      "episode: 721, reward: 12.7, average _reward: 12.199049021598942 \n",
      "episode: 722, reward: 10.95, average _reward: 12.196656758288537 \n",
      "episode: 723, reward: 12.46, average _reward: 12.472226404012595 \n",
      "episode: 724, reward: 10.68, average _reward: 11.926919708197383 \n",
      "episode: 725, reward: 10.83, average _reward: 11.610689658525573 \n",
      "episode: 726, reward: 18.62, average _reward: 12.403021172053155 \n",
      "episode: 727, reward: 12.96, average _reward: 12.690334437134286 \n",
      "episode: 728, reward: 14.9, average _reward: 13.212961278278062 \n",
      "episode: 729, reward: 9.89, average _reward: 12.973497581907186 \n",
      "episode: 730, reward: 13.95, average _reward: 12.79503517499934 \n",
      "episode: 731, reward: 11.73, average _reward: 12.697641977963976 \n",
      "episode: 732, reward: 16.07, average _reward: 13.20936133589856 \n",
      "episode: 733, reward: 13.08, average _reward: 13.271834971472313 \n",
      "episode: 734, reward: 11.63, average _reward: 13.3660782012097 \n",
      "episode: 735, reward: 12.97, average _reward: 13.579534277523162 \n",
      "episode: 736, reward: 12.23, average _reward: 12.940379595188782 \n",
      "episode: 737, reward: 13.32, average _reward: 12.976477949256022 \n",
      "episode: 738, reward: 12.79, average _reward: 12.765736498747009 \n",
      "episode: 739, reward: 14.63, average _reward: 13.239975817892358 \n",
      "episode: 740, reward: 15.46, average _reward: 13.39092947584414 \n",
      "episode: 741, reward: 15.38, average _reward: 13.756595219107151 \n",
      "episode: 742, reward: 10.64, average _reward: 13.213753223749752 \n",
      "episode: 743, reward: 12.24, average _reward: 13.12892953366035 \n",
      "episode: 744, reward: 11.54, average _reward: 13.119753850380846 \n",
      "episode: 745, reward: 10.41, average _reward: 12.863621826190954 \n",
      "episode: 746, reward: 11.87, average _reward: 12.826845614046714 \n",
      "episode: 747, reward: 12.93, average _reward: 12.787123958870927 \n",
      "episode: 748, reward: 13.2, average _reward: 12.827340378986268 \n",
      "episode: 749, reward: 12.22, average _reward: 12.586558264234473 \n",
      "episode: 750, reward: 12.56, average _reward: 12.2966260076265 \n",
      "episode: 751, reward: 12.51, average _reward: 12.009507785983434 \n",
      "episode: 752, reward: 10.65, average _reward: 12.010763119585251 \n",
      "episode: 753, reward: 11.5, average _reward: 11.937279957892681 \n",
      "episode: 754, reward: 12.13, average _reward: 11.99697904224951 \n",
      "episode: 755, reward: 12.06, average _reward: 12.162749280234653 \n",
      "episode: 756, reward: 10.76, average _reward: 12.052526434620157 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 757, reward: 11.01, average _reward: 11.860832715328005 \n",
      "episode: 758, reward: 21.62, average _reward: 12.703730949552273 \n",
      "episode: 759, reward: 13.0, average _reward: 12.781918789403559 \n",
      "episode: 760, reward: 12.04, average _reward: 12.730076708928019 \n",
      "episode: 761, reward: 14.56, average _reward: 12.935397124696683 \n",
      "episode: 762, reward: 12.24, average _reward: 13.094791242718552 \n",
      "episode: 763, reward: 11.98, average _reward: 13.14255816159367 \n",
      "episode: 764, reward: 12.42, average _reward: 13.171453146465273 \n",
      "episode: 765, reward: 12.84, average _reward: 13.248724107595782 \n",
      "episode: 766, reward: 9.33, average _reward: 13.105540481795236 \n",
      "episode: 767, reward: 12.24, average _reward: 13.22822455564192 \n",
      "episode: 768, reward: 12.4, average _reward: 12.306081891215355 \n",
      "episode: 769, reward: 17.67, average _reward: 12.77215208288288 \n",
      "episode: 770, reward: 11.12, average _reward: 12.679993935008055 \n",
      "episode: 771, reward: 14.24, average _reward: 12.647392995735762 \n",
      "episode: 772, reward: 10.7, average _reward: 12.49295813284039 \n",
      "episode: 773, reward: 17.18, average _reward: 13.013289661944787 \n",
      "episode: 774, reward: 12.81, average _reward: 13.051951939868605 \n",
      "episode: 775, reward: 13.23, average _reward: 13.09169813836803 \n",
      "episode: 776, reward: 16.04, average _reward: 13.762795805716545 \n",
      "episode: 777, reward: 13.08, average _reward: 13.8467645799056 \n",
      "episode: 778, reward: 10.86, average _reward: 13.69229238026098 \n",
      "episode: 779, reward: 12.8, average _reward: 13.20546817240593 \n",
      "episode: 780, reward: 12.4, average _reward: 13.33346199387058 \n",
      "episode: 781, reward: 12.64, average _reward: 13.173165446598182 \n",
      "episode: 782, reward: 11.25, average _reward: 13.227836324864615 \n",
      "episode: 783, reward: 12.73, average _reward: 12.782358135584698 \n",
      "episode: 784, reward: 10.59, average _reward: 12.56078350114737 \n",
      "episode: 785, reward: 13.74, average _reward: 12.611235015799842 \n",
      "episode: 786, reward: 13.03, average _reward: 12.30996444759479 \n",
      "episode: 787, reward: 14.5, average _reward: 12.451871448988424 \n",
      "episode: 788, reward: 12.42, average _reward: 12.608166538755663 \n",
      "episode: 789, reward: 10.51, average _reward: 12.37945077536607 \n",
      "episode: 790, reward: 10.85, average _reward: 12.224278404773496 \n",
      "episode: 791, reward: 12.29, average _reward: 12.189548413445777 \n",
      "episode: 792, reward: 11.99, average _reward: 12.26400923308828 \n",
      "episode: 793, reward: 16.92, average _reward: 12.683601325627071 \n",
      "episode: 794, reward: 12.35, average _reward: 12.859367482150805 \n",
      "episode: 795, reward: 12.96, average _reward: 12.781107329722243 \n",
      "episode: 796, reward: 11.78, average _reward: 12.656075890206681 \n",
      "episode: 797, reward: 12.06, average _reward: 12.412449714882598 \n",
      "episode: 798, reward: 15.23, average _reward: 12.693022686849561 \n",
      "episode: 799, reward: 11.87, average _reward: 12.829381987433413 \n",
      "episode: 800, reward: 14.52, average _reward: 13.196877988296825 \n",
      "episode: 801, reward: 13.18, average _reward: 13.285813910967898 \n",
      "episode: 802, reward: 10.98, average _reward: 13.184880206088419 \n",
      "episode: 803, reward: 14.52, average _reward: 12.944723719862946 \n",
      "episode: 804, reward: 12.48, average _reward: 12.958035792507062 \n",
      "episode: 805, reward: 10.91, average _reward: 12.753799713817063 \n",
      "episode: 806, reward: 14.29, average _reward: 13.004559718586233 \n",
      "episode: 807, reward: 13.35, average _reward: 13.133328740661332 \n",
      "episode: 808, reward: 10.97, average _reward: 12.707345410636155 \n",
      "episode: 809, reward: 12.07, average _reward: 12.726577543788173 \n",
      "episode: 810, reward: 13.64, average _reward: 12.638531179635 \n",
      "episode: 811, reward: 15.61, average _reward: 12.881991625968727 \n",
      "episode: 812, reward: 11.95, average _reward: 12.979311700472755 \n",
      "episode: 813, reward: 11.11, average _reward: 12.637941186757688 \n",
      "episode: 814, reward: 11.75, average _reward: 12.564176920709766 \n",
      "episode: 815, reward: 10.66, average _reward: 12.538574201043314 \n",
      "episode: 816, reward: 10.73, average _reward: 12.182763429487379 \n",
      "episode: 817, reward: 12.39, average _reward: 12.087137725161682 \n",
      "episode: 818, reward: 11.06, average _reward: 12.096246577615116 \n",
      "episode: 819, reward: 11.83, average _reward: 12.072595087606583 \n",
      "episode: 820, reward: 14.42, average _reward: 12.150813015794707 \n",
      "episode: 821, reward: 12.2, average _reward: 11.810036505557616 \n",
      "episode: 822, reward: 10.96, average _reward: 11.710774575092946 \n",
      "episode: 823, reward: 14.39, average _reward: 12.038525343738462 \n",
      "episode: 824, reward: 13.53, average _reward: 12.216683786482145 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-6ce6fe7a5346>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-087828ce65c8>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mnext_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;31m# Critic loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(3000):\n",
    "    state = env.reset()\n",
    "    noise.reset()\n",
    "    episode_reward = 0\n",
    "    env.sock.sendto(env.resume_cmd_b,env.remote_address)\n",
    "    for step in range(2000):\n",
    "        action = agent.get_action(state)\n",
    "        action = noise.get_action(action, step)\n",
    "        new_state, reward, done, _ = env.step(action) \n",
    "        agent.memory.push(state, action, reward, new_state, done)\n",
    "        \n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.update(batch_size)        \n",
    "        \n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    rewards.append(episode_reward)\n",
    "    avg_rewards.append(np.mean(rewards[-10:]))\n",
    "    sys.stdout.write(\"episode: {}, reward: {}, average _reward: {} \\n\".format(episode, np.round(episode_reward, decimals=2), np.mean(rewards[-10:])))\n",
    "\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.plot(avg_rewards)\n",
    "plt.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T17:56:30.010981Z",
     "start_time": "2021-02-10T17:56:29.352038Z"
    }
   },
   "outputs": [],
   "source": [
    "for episode in range(1):\n",
    "    state = env.reset()\n",
    "    noise.reset()\n",
    "    episode_reward = 0\n",
    "    env.sock.sendto(env.resume_cmd_b,env.remote_address)\n",
    "    for step in range(2000):\n",
    "        action = agent.get_action(state)\n",
    "        new_state, reward, done, _ = env.step(action)   \n",
    "        \n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
